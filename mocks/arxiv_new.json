[{"id": "http://arxiv.org/abs/1604.03265v2", "updated": "2016-04-29T06:21:09Z", "published": "2016-04-12T07:10:43Z", "title": "Volumetric and Multi-View CNNs for Object Classification on 3D Data", "summary": "3D shape models are becoming widely available and easier to capture, making available 3D information crucial for progress in object classification. Current state-of-the-art methods rely on CNNs to address this problem. Recently, we witness two types of CNNs being developed: CNNs based upon volumetric representations versus CNNs based upon multi-view representations. Empirical results from these two types of CNNs exhibit a large gap, indicating that existing volumetric CNN architectures and approaches are unable to fully exploit the power of 3D representations. In this paper, we aim to improve both volumetric CNNs and multi-view CNNs according to extensive analysis of existing approaches. To this end, we introduce two distinct network architectures of volumetric CNNs. In addition, we examine multi-view CNNs, where we introduce multi-resolution filtering in 3D. Overall, we are able to outperform current state-of-the-art methods for both volumetric CNNs and multi-view CNNs. We provide extensive experiments designed to evaluate underlying design choices, thus providing a better understanding of the space of methods available for object classification on 3D data.", "author": "Charles R. Qi;Hao Su;Matthias Niessner;Angela Dai;Mengyuan Yan;Leonidas J. Guibas", "link": "http://arxiv.org/pdf/1604.03265v2", "arxiv:primary_category": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "@term": "cs.CV", "@scheme": "http://arxiv.org/schemas/atom"}, "category": [{"@term": "cs.CV", "@scheme": "http://arxiv.org/schemas/atom"}, {"@term": "cs.AI", "@scheme": "http://arxiv.org/schemas/atom"}], "citationCount": -1, "openaccess": 1}, {"id": "http://arxiv.org/abs/1810.11875v2", "updated": "2019-03-10T23:34:51Z", "published": "2018-10-28T20:21:27Z", "title": "Automatically Evolving CNN Architectures Based on Blocks", "summary": "The performance of Convolutional Neural Networks (CNNs) highly relies on their architectures. In order to design a CNN with promising performance, extended expertise in both CNNs and the investigated problem is required, which is not necessarily held by every user interested in CNNs or the problem domain. In this paper, we propose to automatically evolve CNN architectures by using a genetic algorithm based on ResNet blocks and DenseNet blocks. The proposed algorithm is  textbf{completely} automatic in designing CNN architectures, particularly, neither pre-processing before it starts nor post-processing on the designed CNN is needed. Furthermore, the proposed algorithm does not require users with domain knowledge on CNNs, the investigated problem or even genetic algorithms. The proposed algorithm is evaluated on CIFAR10 and CIFAR100 against 18 state-of-the-art peer competitors. Experimental results show that it outperforms state-of-the-art CNNs hand-crafted and CNNs designed by automatic peer competitors in terms of the classification accuracy, and achieves the competitive classification accuracy against semi-automatic peer competitors. In addition, the proposed algorithm consumes much less time than most peer competitors in finding the best CNN architectures.", "author": "Yanan Sun;Bing Xue;Mengjie Zhang;Gary G. Yen", "link": "http://arxiv.org/pdf/1810.11875v2", "arxiv:primary_category": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "@term": "cs.NE", "@scheme": "http://arxiv.org/schemas/atom"}, "category": {"@term": "cs.NE", "@scheme": "http://arxiv.org/schemas/atom"}, "citationCount": -1, "openaccess": 1}, {"id": "http://arxiv.org/abs/1805.08620v1", "updated": "2018-05-20T07:18:54Z", "published": "2018-05-20T07:18:54Z", "title": "Wavelet Convolutional Neural Networks", "summary": "Spatial and spectral approaches are two major approaches for image processing tasks such as image classification and object recognition. Among many such algorithms, convolutional neural networks (CNNs) have recently achieved significant performance improvement in many challenging tasks. Since CNNs process images directly in the spatial domain, they are essentially spatial approaches. Given that spatial and spectral approaches are known to have different characteristics, it will be interesting to incorporate a spectral approach into CNNs. We propose a novel CNN architecture, wavelet CNNs, which combines a multiresolution analysis and CNNs into one model. Our insight is that a CNN can be viewed as a limited form of a multiresolution analysis. Based on this insight, we supplement missing parts of the multiresolution analysis via wavelet transform and integrate them as additional components in the entire architecture. Wavelet CNNs allow us to utilize spectral information which is mostly lost in conventional CNNs but useful in most image processing tasks. We evaluate the practical performance of wavelet CNNs on texture classification and image annotation. The experiments show that wavelet CNNs can achieve better accuracy in both tasks than existing models while having significantly fewer parameters than conventional CNNs.", "author": "Shin Fujieda;Kohei Takayama;Toshiya Hachisuka", "arxiv:comment": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "#text": "10 pages, 7 figures, 5 tables. arXiv admin note: substantial text\n  overlap with arXiv:1707.07394"}, "link": "http://arxiv.org/pdf/1805.08620v1", "arxiv:primary_category": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "@term": "cs.CV", "@scheme": "http://arxiv.org/schemas/atom"}, "category": [{"@term": "cs.CV", "@scheme": "http://arxiv.org/schemas/atom"}, {"@term": "cs.LG", "@scheme": "http://arxiv.org/schemas/atom"}], "citationCount": -1, "openaccess": 1}, {"id": "http://arxiv.org/abs/1511.09128v1", "updated": "2015-11-30T01:46:15Z", "published": "2015-11-30T01:46:15Z", "title": "Aspect-based Opinion Summarization with Convolutional Neural Networks", "summary": "This paper considers Aspect-based Opinion Summarization (AOS) of reviews on particular products. To enable real applications, an AOS system needs to address two core subtasks, aspect extraction and sentiment classification. Most existing approaches to aspect extraction, which use linguistic analysis or topic modeling, are general across different products but not precise enough or suitable for particular products. Instead we take a less general but more precise scheme, directly mapping each review sentence into pre-defined aspects. To tackle aspect mapping and sentiment classification, we propose two Convolutional Neural Network (CNN) based methods, cascaded CNN and multitask CNN. Cascaded CNN contains two levels of convolutional networks. Multiple CNNs at level 1 deal with aspect mapping task, and a single CNN at level 2 deals with sentiment classification. Multitask CNN also contains multiple aspect CNNs and a sentiment CNN, but different networks share the same word embeddings. Experimental results indicate that both cascaded and multitask CNNs outperform SVM-based methods by large margins. Multitask CNN generally performs better than cascaded CNN.", "author": "Haibing Wu;Yiwei Gu;Shangdi Sun;Xiaodong Gu", "link": "http://arxiv.org/pdf/1511.09128v1", "arxiv:primary_category": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "@term": "cs.CL", "@scheme": "http://arxiv.org/schemas/atom"}, "category": [{"@term": "cs.CL", "@scheme": "http://arxiv.org/schemas/atom"}, {"@term": "cs.IR", "@scheme": "http://arxiv.org/schemas/atom"}, {"@term": "cs.LG", "@scheme": "http://arxiv.org/schemas/atom"}], "citationCount": -1, "openaccess": 1}, {"id": "http://arxiv.org/abs/2112.06571v1", "updated": "2021-12-13T11:26:12Z", "published": "2021-12-13T11:26:12Z", "title": "Extension of Convolutional Neural Network along Temporal and Vertical   Directions for Precipitation Downscaling", "summary": "Deep learning has been utilized for the statistical downscaling of climate data. Specifically, a two-dimensional (2D) convolutional neural network (CNN) has been successfully applied to precipitation estimation. This study implements a three-dimensional (3D) CNN to estimate watershed-scale daily precipitation from 3D atmospheric data and compares the results with those for a 2D CNN. The 2D CNN is extended along the time direction (3D-CNN-Time) and the vertical direction (3D-CNN-Vert). The precipitation estimates of these extended CNNs are compared with those of the 2D CNN in terms of the root-mean-square error (RMSE), Nash-Sutcliffe efficiency (NSE), and 99th percentile RMSE. It is found that both 3D-CNN-Time and 3D-CNN-Vert improve the model accuracy for precipitation estimation compared to the 2D CNN. 3D-CNN-Vert provided the best estimates during the training and test periods in terms of RMSE and NSE.", "author": "Takeyoshi Nagasato;Kei Ishida;Ali Ercan;Tongbi Tu;Masato Kiyama;Motoki Amagasaki;Kazuki Yokoo", "link": "http://arxiv.org/pdf/2112.06571v1", "arxiv:primary_category": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "@term": "cs.LG", "@scheme": "http://arxiv.org/schemas/atom"}, "category": [{"@term": "cs.LG", "@scheme": "http://arxiv.org/schemas/atom"}, {"@term": "physics.ao-ph", "@scheme": "http://arxiv.org/schemas/atom"}], "citationCount": -1, "openaccess": 1}, {"id": "http://arxiv.org/abs/2101.11081v1", "updated": "2021-01-26T20:59:37Z", "published": "2021-01-26T20:59:37Z", "title": "The Effect of Class Definitions on the Transferability of Adversarial   Attacks Against Forensic CNNs", "summary": "In recent years, convolutional neural networks (CNNs) have been widely used by researchers to perform forensic tasks such as image tampering detection. At the same time, adversarial attacks have been developed that are capable of fooling CNN-based classifiers. Understanding the transferability of adversarial attacks, i.e. an attacks ability to attack a different CNN than the one it was trained against, has important implications for designing CNNs that are resistant to attacks. While attacks on object recognition CNNs are believed to be transferrable, recent work by Barni et al. has shown that attacks on forensic CNNs have difficulty transferring to other CNN architectures or CNNs trained using different datasets. In this paper, we demonstrate that adversarial attacks on forensic CNNs are even less transferrable than previously thought even between virtually identical CNN architectures! We show that several common adversarial attacks against CNNs trained to identify image manipulation fail to transfer to CNNs whose only difference is in the class definitions (i.e. the same CNN architectures trained using the same data). We note that all formulations of class definitions contain the unaltered class. This has important implications for the future design of forensic CNNs that are robust to adversarial and anti-forensic attacks.", "author": "Xinwei Zhao;Matthew C. Stamm", "arxiv:journal_ref": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "#text": "Published at Electronic Imaging, Media Watermarking, Security, and\n  Forensics 2020, pp. 119-1-119-7(7)"}, "link": "http://arxiv.org/pdf/2101.11081v1", "arxiv:primary_category": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "@term": "cs.CV", "@scheme": "http://arxiv.org/schemas/atom"}, "category": [{"@term": "cs.CV", "@scheme": "http://arxiv.org/schemas/atom"}, {"@term": "cs.CR", "@scheme": "http://arxiv.org/schemas/atom"}, {"@term": "cs.LG", "@scheme": "http://arxiv.org/schemas/atom"}], "citationCount": -1, "openaccess": 1}, {"id": "http://arxiv.org/abs/1612.06519v1", "updated": "2016-12-20T06:20:43Z", "published": "2016-12-20T06:20:43Z", "title": "Exploring the Design Space of Deep Convolutional Neural Networks at   Large Scale", "summary": "In recent years, the research community has discovered that deep neural networks (DNNs) and convolutional neural networks (CNNs) can yield higher accuracy than all previous solutions to a broad array of machine learning problems. To our knowledge, there is no single CNN/DNN architecture that solves all problems optimally. Instead, the \"right\" CNN/DNN architecture varies depending on the application at hand. CNN/DNNs comprise an enormous design space. Quantitatively, we find that a small region of the CNN design space contains 30 billion different CNN architectures.   In this dissertation, we develop a methodology that enables systematic exploration of the design space of CNNs. Our methodology is comprised of the following four themes.   1. Judiciously choosing benchmarks and metrics.   2. Rapidly training CNN models.   3. Defining and describing the CNN design space.   4. Exploring the design space of CNN architectures.   Taken together, these four themes comprise an effective methodology for discovering the \"right\" CNN architectures to meet the needs of practical applications.", "author": "Forrest Iandola", "arxiv:comment": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "#text": "thesis, UC Berkeley (2016)"}, "link": "http://arxiv.org/pdf/1612.06519v1", "arxiv:primary_category": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "@term": "cs.CV", "@scheme": "http://arxiv.org/schemas/atom"}, "category": [{"@term": "cs.CV", "@scheme": "http://arxiv.org/schemas/atom"}, {"@term": "cs.LG", "@scheme": "http://arxiv.org/schemas/atom"}, {"@term": "cs.NE", "@scheme": "http://arxiv.org/schemas/atom"}], "citationCount": -1, "openaccess": 1}, {"id": "http://arxiv.org/abs/1905.09797v1", "updated": "2019-05-23T17:40:41Z", "published": "2019-05-23T17:40:41Z", "title": "Interpreting Adversarially Trained Convolutional Neural Networks", "summary": "We attempt to interpret how adversarially trained convolutional neural networks (AT-CNNs) recognize objects. We design systematic approaches to interpret AT-CNNs in both qualitative and quantitative ways and compare them with normally trained models. Surprisingly, we find that adversarial training alleviates the texture bias of standard CNNs when trained on object recognition tasks, and helps CNNs learn a more shape-biased representation. We validate our hypothesis from two aspects. First, we compare the salience maps of AT-CNNs and standard CNNs on clean images and images under different transformations. The comparison could visually show that the prediction of the two types of CNNs is sensitive to dramatically different types of features. Second, to achieve quantitative verification, we construct additional test datasets that destroy either textures or shapes, such as style-transferred version of clean data, saturated images and patch-shuffled ones, and then evaluate the classification accuracy of AT-CNNs and normal CNNs on these datasets. Our findings shed some light on why AT-CNNs are more robust than those normally trained ones and contribute to a better understanding of adversarial training over CNNs from an interpretation perspective.", "author": "Tianyuan Zhang;Zhanxing Zhu", "arxiv:comment": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "#text": "To apper in ICML19"}, "link": "http://arxiv.org/pdf/1905.09797v1", "arxiv:primary_category": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "@term": "cs.LG", "@scheme": "http://arxiv.org/schemas/atom"}, "category": [{"@term": "cs.LG", "@scheme": "http://arxiv.org/schemas/atom"}, {"@term": "cs.CV", "@scheme": "http://arxiv.org/schemas/atom"}, {"@term": "stat.ML", "@scheme": "http://arxiv.org/schemas/atom"}], "citationCount": -1, "openaccess": 1}, {"id": "10.1109/TCYB.2020.2983860", "updated": "2020-03-27T02:31:49Z", "published": "2018-08-11T15:32:06Z", "title": "Automatically designing CNN architectures using genetic algorithm for   image classification", "summary": "Convolutional Neural Networks (CNNs) have gained a remarkable success on many image classification tasks in recent years. However, the performance of CNNs highly relies upon their architectures. For most state-of-the-art CNNs, their architectures are often manually-designed with expertise in both CNNs and the investigated problems. Therefore, it is difficult for users, who have no extended expertise in CNNs, to design optimal CNN architectures for their own image classification problems of interest. In this paper, we propose an automatic CNN architecture design method by using genetic algorithms, to effectively address the image classification tasks. The most merit of the proposed algorithm remains in its \"automatic\" characteristic that users do not need domain knowledge of CNNs when using the proposed algorithm, while they can still obtain a promising CNN architecture for the given images. The proposed algorithm is validated on widely used benchmark image classification datasets, by comparing to the state-of-the-art peer competitors covering eight manually-designed CNNs, seven automatic+manually tuning and five automatic CNN architecture design algorithms. The experimental results indicate the proposed algorithm outperforms the existing automatic CNN architecture design algorithms in terms of classification accuracy, parameter numbers and consumed computational resources. The proposed algorithm also shows the very comparable classification accuracy to the best one from manually-designed and automatic+manually tuning CNNs, while consumes much less of computational resource.", "author": "Yanan Sun;Bing Xue;Mengjie Zhang;Gary G. Yen", "arxiv:doi": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "#text": "10.1109/TCYB.2020.2983860"}, "link": "http://arxiv.org/pdf/1808.03818v3", "arxiv:journal_ref": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "#text": "IEEE Transactions on Cybernetics, 2020"}, "arxiv:primary_category": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "@term": "cs.NE", "@scheme": "http://arxiv.org/schemas/atom"}, "category": {"@term": "cs.NE", "@scheme": "http://arxiv.org/schemas/atom"}, "citationCount": -1, "openaccess": 1}, {"id": "http://arxiv.org/abs/2207.05756v2", "updated": "2022-07-26T13:28:20Z", "published": "2022-07-12T13:25:42Z", "title": "Exploring Adversarial Examples and Adversarial Robustness of   Convolutional Neural Networks by Mutual Information", "summary": "A counter-intuitive property of convolutional neural networks (CNNs) is their inherent susceptibility to adversarial examples, which severely hinders the application of CNNs in security-critical fields. Adversarial examples are similar to original examples but contain malicious perturbations. Adversarial training is a simple and effective defense method to improve the robustness of CNNs to adversarial examples. The mechanisms behind adversarial examples and adversarial training are worth exploring. Therefore, this work investigates similarities and differences between normally trained CNNs (NT-CNNs) and adversarially trained CNNs (AT-CNNs) in information extraction from the mutual information perspective. We show that 1) whether NT-CNNs or AT-CNNs, for original and adversarial examples, the trends towards mutual information are almost similar throughout training; 2) compared with normal training, adversarial training is more difficult and the amount of information that AT-CNNs extract from the input is less; 3) the CNNs trained with different methods have different preferences for certain types of information; NT-CNNs tend to extract texture-based information from the input, while AT-CNNs prefer to shape-based information. The reason why adversarial examples mislead CNNs may be that they contain more texture-based information about other classes. Furthermore, we also analyze the mutual information estimators used in this work and find that they outline the geometric properties of the middle layer's output.", "author": "Jiebao Zhang;Wenhua Qian;Rencan Nie;Jinde Cao;Dan Xu", "arxiv:comment": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "#text": "initial submit 3"}, "link": "http://arxiv.org/pdf/2207.05756v2", "arxiv:primary_category": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "@term": "cs.LG", "@scheme": "http://arxiv.org/schemas/atom"}, "category": [{"@term": "cs.LG", "@scheme": "http://arxiv.org/schemas/atom"}, {"@term": "cs.AI", "@scheme": "http://arxiv.org/schemas/atom"}], "citationCount": -1, "openaccess": 1}, {"id": "http://arxiv.org/abs/2207.12113v1", "updated": "2022-07-20T15:08:52Z", "published": "2022-07-20T15:08:52Z", "title": "AutoDiCE: Fully Automated Distributed CNN Inference at the Edge", "summary": "Deep Learning approaches based on Convolutional Neural Networks (CNNs) are extensively utilized and very successful in a wide range of application areas, including image classification and speech recognition. For the execution of trained CNNs, i.e. model inference, we nowadays witness a shift from the Cloud to the Edge. Unfortunately, deploying and inferring large, compute and memory intensive CNNs on edge devices is challenging because these devices typically have limited power budgets and compute/memory resources. One approach to address this challenge is to leverage all available resources across multiple edge devices to deploy and execute a large CNN by properly partitioning the CNN and running each CNN partition on a separate edge device. Although such distribution, deployment, and execution of large CNNs on multiple edge devices is a desirable and beneficial approach, there currently does not exist a design and programming framework that takes a trained CNN model, together with a CNN partitioning specification, and fully automates the CNN model splitting and deployment on multiple edge devices to facilitate distributed CNN inference at the Edge. Therefore, in this paper, we propose a novel framework, called AutoDiCE, for automated splitting of a CNN model into a set of sub-models and automated code generation for distributed and collaborative execution of these sub-models on multiple, possibly heterogeneous, edge devices, while supporting the exploitation of parallelism among and within the edge devices. Our experimental results show that AutoDiCE can deliver distributed CNN inference with reduced energy consumption and memory usage per edge device, and improved overall system throughput at the same time.", "author": "Xiaotian Guo;Andy D. Pimentel;Todor Stefanov", "link": "http://arxiv.org/pdf/2207.12113v1", "arxiv:primary_category": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "@term": "cs.DC", "@scheme": "http://arxiv.org/schemas/atom"}, "category": [{"@term": "cs.DC", "@scheme": "http://arxiv.org/schemas/atom"}, {"@term": "cs.AI", "@scheme": "http://arxiv.org/schemas/atom"}, {"@term": "cs.LG", "@scheme": "http://arxiv.org/schemas/atom"}], "citationCount": -1, "openaccess": 1}, {"id": "http://arxiv.org/abs/1901.04100v1", "updated": "2019-01-14T01:14:03Z", "published": "2019-01-14T01:14:03Z", "title": "LEP-CNN: A Lightweight Edge Device Assisted Privacy-preserving CNN   Inference Solution for IoT", "summary": "Supporting convolutional neural network (CNN) inference on resource-constrained IoT devices in a timely manner has been an outstanding challenge for emerging smart systems. To mitigate the burden on IoT devices, the prevailing solution is to offload the CNN inference task, which is usually composed of billions of operations, to public cloud. However, the \"offloading-to-cloud\" solution may cause privacy breach while moving sensitive data to cloud. For privacy protection, the research community has resorted to advanced cryptographic primitives and approximation techniques to support CNN inference on encrypted data. Consequently, these attempts cause impractical computational overhead on IoT devices and degrade the performance of CNNs. Moreover, relying on the remote cloud can cause additional network latency and even make the system dysfunction when network connection is off.   We proposes an extremely lightweight edge device assisted private CNN inference solution for IoT devices, namely LEP-CNN. The main design of LEP-CNN is based on a novel online/offline encryption scheme. The decryption of LEP-CNN is pre-computed offline via utilizing the linear property of the most time-consuming operations of CNNs. As a result, LEP-CNN allows IoT devices to securely offload over 99% CNN operations, and edge devices to execute CNN inference on encrypted data as efficient as on plaintext. LEP-CNN also provides an integrity check option to help IoT devices detect error results with a successful rate over 99%. Experiments on AlexNet show that LEP-CNN can speed up the CNN inference for more than 35 times for resource constrained IoT devices. A homomorphic encryption based AlexNet using CryptoNets is implemented to compare with LEP-CNN to demonstrate that LEP-CNN has a better performance than homomorphic encryption based privacy preserving neural networks under time-sensitive scenarios.", "author": "Yifan Tian;Jiawei Yuan;Shucheng Yu;Yantian Hou", "link": "http://arxiv.org/pdf/1901.04100v1", "arxiv:primary_category": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "@term": "cs.CR", "@scheme": "http://arxiv.org/schemas/atom"}, "category": {"@term": "cs.CR", "@scheme": "http://arxiv.org/schemas/atom"}, "citationCount": -1, "openaccess": 1}, {"id": "http://arxiv.org/abs/1707.05395v1", "updated": "2017-07-17T21:04:13Z", "published": "2017-07-17T21:04:13Z", "title": "Incremental Boosting Convolutional Neural Network for Facial Action Unit   Recognition", "summary": "Recognizing facial action units (AUs) from spontaneous facial expressions is still a challenging problem. Most recently, CNNs have shown promise on facial AU recognition. However, the learned CNNs are often overfitted and do not generalize well to unseen subjects due to limited AU-coded training images. We proposed a novel Incremental Boosting CNN (IB-CNN) to integrate boosting into the CNN via an incremental boosting layer that selects discriminative neurons from the lower layer and is incrementally updated on successive mini-batches. In addition, a novel loss function that accounts for errors from both the incremental boosted classifier and individual weak classifiers was proposed to fine-tune the IB-CNN. Experimental results on four benchmark AU databases have demonstrated that the IB-CNN yields significant improvement over the traditional CNN and the boosting CNN without incremental learning, as well as outperforming the state-of-the-art CNN-based methods in AU recognition. The improvement is more impressive for the AUs that have the lowest frequencies in the databases.", "author": "Shizhong Han;Zibo Meng;Ahmed Shehab Khan;Yan Tong", "arxiv:comment": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "#text": "NIPS2016"}, "link": "http://arxiv.org/pdf/1707.05395v1", "arxiv:primary_category": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "@term": "cs.CV", "@scheme": "http://arxiv.org/schemas/atom"}, "category": {"@term": "cs.CV", "@scheme": "http://arxiv.org/schemas/atom"}, "citationCount": -1, "openaccess": 1}, {"id": "http://arxiv.org/abs/1804.00021v2", "updated": "2018-05-09T19:38:24Z", "published": "2018-03-30T18:19:32Z", "title": "Hierarchical Transfer Convolutional Neural Networks for Image   Classification", "summary": "In this paper, we address the issue of how to enhance the generalization performance of convolutional neural networks (CNN) in the early learning stage for image classification. This is motivated by real-time applications that require the generalization performance of CNN to be satisfactory within limited training time. In order to achieve this, a novel hierarchical transfer CNN framework is proposed. It consists of a group of shallow CNNs and a cloud CNN, where the shallow CNNs are trained firstly and then the first layers of the trained shallow CNNs are used to initialize the first layer of the cloud CNN. This method will boost the generalization performance of the cloud CNN significantly, especially during the early stage of training. Experiments using CIFAR-10 and ImageNet datasets are performed to examine the proposed method. Results demonstrate the improvement of testing accuracy is 12% on average and as much as 20% for the CIFAR-10 case while 5% testing accuracy improvement for the ImageNet case during the early stage of learning. It is also shown that universal improvements of testing accuracy are obtained across different settings of dropout and number of shallow CNNs.", "author": "Xishuang Dong;Hsiang-Huang Wu;Yuzhong Yan;Lijun Qian", "link": "http://arxiv.org/pdf/1804.00021v2", "arxiv:primary_category": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "@term": "cs.CV", "@scheme": "http://arxiv.org/schemas/atom"}, "category": [{"@term": "cs.CV", "@scheme": "http://arxiv.org/schemas/atom"}, {"@term": "cs.LG", "@scheme": "http://arxiv.org/schemas/atom"}, {"@term": "stat.ML", "@scheme": "http://arxiv.org/schemas/atom"}], "citationCount": -1, "openaccess": 1}, {"id": "http://arxiv.org/abs/1903.11176v1", "updated": "2019-03-26T22:05:58Z", "published": "2019-03-26T22:05:58Z", "title": "On evaluating CNN representations for low resource medical image   classification", "summary": "Convolutional Neural Networks (CNNs) have revolutionized performances in several machine learning tasks such as image classification, object tracking, and keyword spotting. However, given that they contain a large number of parameters, their direct applicability into low resource tasks is not straightforward. In this work, we experiment with an application of CNN models to gastrointestinal landmark classification with only a few thousands of training samples through transfer learning. As in a standard transfer learning approach, we train CNNs on a large external corpus, followed by representation extraction for the medical images. Finally, a classifier is trained on these CNN representations. However, given that several variants of CNNs exist, the choice of CNN is not obvious. To address this, we develop a novel metric that can be used to predict test performances, given CNN representations on the training set. Not only we demonstrate the superiority of the CNN based transfer learning approach against an assembly of knowledge driven features, but the proposed metric also carries an 87% correlation with the test set performances as obtained using various CNN representations.", "author": "Taruna Agrawal;Rahul Gupta;Shrikanth Narayanan", "arxiv:comment": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "#text": "Accepted to ICASSP 2019"}, "link": "http://arxiv.org/pdf/1903.11176v1", "arxiv:primary_category": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "@term": "eess.IV", "@scheme": "http://arxiv.org/schemas/atom"}, "category": [{"@term": "eess.IV", "@scheme": "http://arxiv.org/schemas/atom"}, {"@term": "cs.LG", "@scheme": "http://arxiv.org/schemas/atom"}, {"@term": "stat.ML", "@scheme": "http://arxiv.org/schemas/atom"}], "citationCount": -1, "openaccess": 1}, {"id": "http://arxiv.org/abs/2204.00944v2", "updated": "2022-04-05T07:48:38Z", "published": "2022-04-02T22:00:27Z", "title": "Progressive Minimal Path Method with Embedded CNN", "summary": "We propose Path-CNN, a method for the segmentation of centerlines of tubular structures by embedding convolutional neural networks (CNNs) into the progressive minimal path method. Minimal path methods are widely used for topology-aware centerline segmentation, but usually these methods rely on weak, hand-tuned image features. In contrast, CNNs use strong image features which are learned automatically from images. But CNNs usually do not take the topology of the results into account, and often require a large amount of annotations for training. We integrate CNNs into the minimal path method, so that both techniques benefit from each other: CNNs employ learned image features to improve the determination of minimal paths, while the minimal path method ensures the correct topology of the segmented centerlines, provides strong geometric priors to increase the performance of CNNs, and reduces the amount of annotations for the training of CNNs significantly. Our method has lower hardware requirements than many recent methods. Qualitative and quantitative comparison with other methods shows that Path-CNN achieves better performance, especially when dealing with tubular structures with complex shapes in challenging environments.", "author": "Wei Liao", "arxiv:comment": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "#text": "Accepted to IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR), New Orleans, 2022"}, "link": "http://arxiv.org/pdf/2204.00944v2", "arxiv:primary_category": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "@term": "cs.CV", "@scheme": "http://arxiv.org/schemas/atom"}, "category": {"@term": "cs.CV", "@scheme": "http://arxiv.org/schemas/atom"}, "citationCount": -1, "openaccess": 1}, {"id": "http://arxiv.org/abs/2210.01727v1", "updated": "2022-10-04T16:31:21Z", "published": "2022-10-04T16:31:21Z", "title": "Enhanced CNN with Global Features for Fault Diagnosis of Complex   Chemical Processes", "summary": "Convolutional neural network (CNN) models have been widely used for fault diagnosis of complex systems. However, traditional CNN models rely on small kernel filters to obtain local features from images. Thus, an excessively deep CNN is required to capture global features, which are critical for fault diagnosis of dynamical systems. In this work, we present an improved CNN that embeds global features (GF-CNN). Our method uses a multi-layer perceptron (MLP) for dimension reduction to directly extract global features and integrate them into the CNN. The advantage of this method is that both local and global patterns in images can be captured by a simple model architecture instead of establishing deep CNN models. The proposed method is applied to the fault diagnosis of the Tennessee Eastman process. Simulation results show that the GF-CNN can significantly improve the fault diagnosis performance compared to traditional CNN. The proposed method can also be applied to other areas such as computer vision and image processing.", "author": "Qiugang Lu;Saif S. S. Al-Wahaibi", "arxiv:comment": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "#text": "6 pages, 5 figures"}, "link": "http://arxiv.org/pdf/2210.01727v1", "arxiv:primary_category": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "@term": "eess.SY", "@scheme": "http://arxiv.org/schemas/atom"}, "category": [{"@term": "eess.SY", "@scheme": "http://arxiv.org/schemas/atom"}, {"@term": "cs.SY", "@scheme": "http://arxiv.org/schemas/atom"}], "citationCount": -1, "openaccess": 1}, {"id": "http://arxiv.org/abs/1504.02351v1", "updated": "2015-04-09T15:27:49Z", "published": "2015-04-09T15:27:49Z", "title": "When Face Recognition Meets with Deep Learning: an Evaluation of   Convolutional Neural Networks for Face Recognition", "summary": "Deep learning, in particular Convolutional Neural Network (CNN), has achieved promising results in face recognition recently. However, it remains an open question: why CNNs work well and how to design a 'good' architecture. The existing works tend to focus on reporting CNN architectures that work well for face recognition rather than investigate the reason. In this work, we conduct an extensive evaluation of CNN-based face recognition systems (CNN-FRS) on a common ground to make our work easily reproducible. Specifically, we use public database LFW (Labeled Faces in the Wild) to train CNNs, unlike most existing CNNs trained on private databases. We propose three CNN architectures which are the first reported architectures trained using LFW data. This paper quantitatively compares the architectures of CNNs and evaluate the effect of different implementation choices. We identify several useful properties of CNN-FRS. For instance, the dimensionality of the learned features can be significantly reduced without adverse effect on face recognition accuracy. In addition, traditional metric learning method exploiting CNN-learned features is evaluated. Experiments show two crucial factors to good CNN-FRS performance are the fusion of multiple CNNs and metric learning. To make our work reproducible, source code and models will be made publicly available.", "author": "Guosheng Hu;Yongxin Yang;Dong Yi;Josef Kittler;William Christmas;Stan Z. Li;Timothy Hospedales", "arxiv:comment": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "#text": "7 pages, 4 figures, 7 tables"}, "link": "http://arxiv.org/pdf/1504.02351v1", "arxiv:primary_category": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "@term": "cs.CV", "@scheme": "http://arxiv.org/schemas/atom"}, "category": [{"@term": "cs.CV", "@scheme": "http://arxiv.org/schemas/atom"}, {"@term": "cs.LG", "@scheme": "http://arxiv.org/schemas/atom"}, {"@term": "cs.NE", "@scheme": "http://arxiv.org/schemas/atom"}], "citationCount": -1, "openaccess": 1}, {"id": "http://arxiv.org/abs/2005.06892v1", "updated": "2020-05-14T11:54:04Z", "published": "2020-05-14T11:54:04Z", "title": "ZynqNet: An FPGA-Accelerated Embedded Convolutional Neural Network", "summary": "Image Understanding is becoming a vital feature in ever more applications ranging from medical diagnostics to autonomous vehicles. Many applications demand for embedded solutions that integrate into existing systems with tight real-time and power constraints. Convolutional Neural Networks (CNNs) presently achieve record-breaking accuracies in all image understanding benchmarks, but have a very high computational complexity. Embedded CNNs thus call for small and efficient, yet very powerful computing platforms. This master thesis explores the potential of FPGA-based CNN acceleration and demonstrates a fully functional proof-of-concept CNN implementation on a Zynq System-on-Chip. The ZynqNet Embedded CNN is designed for image classification on ImageNet and consists of ZynqNet CNN, an optimized and customized CNN topology, and the ZynqNet FPGA Accelerator, an FPGA-based architecture for its evaluation. ZynqNet CNN is a highly efficient CNN topology. Detailed analysis and optimization of prior topologies using the custom-designed Netscope CNN Analyzer have enabled a CNN with 84.5% top-5 accuracy at a computational complexity of only 530 million multiplyaccumulate operations. The topology is highly regular and consists exclusively of convolutional layers, ReLU nonlinearities and one global pooling layer. The CNN fits ideally onto the FPGA accelerator. The ZynqNet FPGA Accelerator allows an efficient evaluation of ZynqNet CNN. It accelerates the full network based on a nested-loop algorithm which minimizes the number of arithmetic operations and memory accesses. The FPGA accelerator has been synthesized using High-Level Synthesis for the Xilinx Zynq XC-7Z045, and reaches a clock frequency of 200MHz with a device utilization of 80% to 90 %.", "author": "David Gschwend", "arxiv:comment": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "#text": "85 pages, 26 figures. Code available at\n  http://github.com/dgschwend/zynqnet"}, "link": "http://arxiv.org/pdf/2005.06892v1", "arxiv:primary_category": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "@term": "cs.CV", "@scheme": "http://arxiv.org/schemas/atom"}, "category": [{"@term": "cs.CV", "@scheme": "http://arxiv.org/schemas/atom"}, {"@term": "cs.LG", "@scheme": "http://arxiv.org/schemas/atom"}], "citationCount": -1, "openaccess": 1}, {"id": "http://arxiv.org/abs/2104.00341v1", "updated": "2021-04-01T08:45:15Z", "published": "2021-04-01T08:45:15Z", "title": "SpectralNET: Exploring Spatial-Spectral WaveletCNN for Hyperspectral   Image Classification", "summary": "Hyperspectral Image (HSI) classification using Convolutional Neural Networks (CNN) is widely found in the current literature. Approaches vary from using SVMs to 2D CNNs, 3D CNNs, 3D-2D CNNs. Besides 3D-2D CNNs and FuSENet, the other approaches do not consider both the spectral and spatial features together for HSI classification task, thereby resulting in poor performances. 3D CNNs are computationally heavy and are not widely used, while 2D CNNs do not consider multi-resolution processing of images, and only limits itself to the spatial features. Even though 3D-2D CNNs try to model the spectral and spatial features their performance seems limited when applied over multiple dataset. In this article, we propose SpectralNET, a wavelet CNN, which is a variation of 2D CNN for multi-resolution HSI classification. A wavelet CNN uses layers of wavelet transform to bring out spectral features. Computing a wavelet transform is lighter than computing 3D CNN. The spectral features extracted are then connected to the 2D CNN which bring out the spatial features, thereby creating a spatial-spectral feature vector for classification. Overall a better model is achieved that can classify multi-resolution HSI data with high accuracy. Experiments performed with SpectralNET on benchmark dataset, i.e. Indian Pines, University of Pavia, and Salinas Scenes confirm the superiority of proposed SpectralNET with respect to the state-of-the-art methods. The code is publicly available in https://github.com/tanmay-ty/SpectralNET.", "author": "Tanmay Chakraborty;Utkarsh Trehan", "link": "http://arxiv.org/pdf/2104.00341v1", "arxiv:primary_category": {"@xmlns:arxiv": "http://arxiv.org/schemas/atom", "@term": "eess.IV", "@scheme": "http://arxiv.org/schemas/atom"}, "category": [{"@term": "eess.IV", "@scheme": "http://arxiv.org/schemas/atom"}, {"@term": "cs.CV", "@scheme": "http://arxiv.org/schemas/atom"}, {"@term": "cs.LG", "@scheme": "http://arxiv.org/schemas/atom"}], "citationCount": -1, "openaccess": 1}]