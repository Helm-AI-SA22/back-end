{
    "documents": [
        {
            "abstract": "In recent years, deep learning poses a deep technical revolution in almost every field and attracts great attentions from industry and academia. Especially, the convolutional neural network (CNN), one representative model of deep learning, achieves great successes in computer vision and natural language processing. However, simply or blindly applying CNN to the other fields results in lower training effects or makes it quite difficult to adjust the model parameters. In this poster, we propose a general methodology named V-CNN by introducing data visualizing for CNN. V-CNN introduces a data visualization model prior to CNN modeling to make sure the data after processing is fit for the features of images as well as CNN modeling. We apply V-CNN to the network intrusion detection problem based on a famous practical dataset: AWID. Simulation results confirm V-CNN significantly outperforms other studies and the recall rate of each invasion category is more than 99.8%.",
            "authors": "Mao Yang;Bo Li;Guanxiong Feng;Zhongjiang Yan",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1807.02164v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1807.02164v1",
            "publicationDate": "2018-06-12T10:57:57Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.23256704435701123,
            "title": "V-CNN: When Convolutional Neural Network encounters Data Visualization",
            "topics": [
                {
                    "affinity": 0.17752324044704437,
                    "id": 0
                },
                {
                    "affinity": 0.8182066082954407,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Hyperspectral remote sensing plays role in the field of earth observation research because of rich spatial, radiation and spectral information. With the rapid development of deep learning, deep neural networks are widely used in hyperspectral remote sensing image classification tasks, but at the same time, a series of difficulties have arisen, such as high demand for training samples, time-consuming model training. Convolutional neural network (CNN) is well known for its capability of feature learning and has demonstrated excellent performance in hyperspectral image classification. In this paper, the one-dimension CNN (1D-CNN) modules was added after the two-dimension CNN (2D-CNN), so the complete network structure contains two different dimension CNN, called multi-dimension CNN (MD-CNN). The proposed 1D-CNN blocks can continue to learn contextual features and is expected to have more discriminative power. Experimental results with hyperspectral image benchmark datasets demonstrate that the proposed method can outperform the state-of-the-art CNN-based classification methods.",
            "authors": "Haojie Cai;Tao Chen",
            "citationCount": 1,
            "id": "10.1109/IGARSS39084.2020.9323561",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9323561",
            "publicationDate": "26 Sept.-2 Oct. 2020",
            "source": [
                "ieee"
            ],
            "tfidf": 0.2249825317521736,
            "title": "Multi-Dimension CNN for Hyperspectral Image Classificaton",
            "topics": [
                {
                    "affinity": 0.4710131883621216,
                    "id": 0
                },
                {
                    "affinity": 0.5251523852348328,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Deep Convolutional Neural Networks (CNN) have exhibited superior performance in many visual recognition tasks including image classification, object detection, and scene label- ing, due to their large learning capacity and resistance to overfit. For the image classification task, most of the current deep CNN- based approaches take the whole size-normalized image as input and have achieved quite promising results. Compared with the previously dominating approaches based on feature extraction, pooling, and classification, the deep CNN-based approaches mainly rely on the learning capability of deep CNN to achieve superior results: the burden of minimizing intra-class variation while maximizing inter-class difference is entirely dependent on the implicit feature learning component of deep CNN; we rely upon the implicitly learned filters and pooling component to select the discriminative regions, which correspond to the activated neurons. However, if the irrelevant regions constitute a large portion of the image of interest, the classification performance of the deep CNN, which takes the whole image as input, can be heavily affected. To solve this issue, we propose a novel latent CNN framework, which treats the most discriminate region as a latent variable. We can jointly learn the global CNN with the latent CNN to avoid the aforementioned big irrelevant region issue, and our experimental results show the evident advantage of the proposed latent CNN over traditional deep CNN: latent CNN outperforms the state-of-the-art performance of deep CNN on standard benchmark datasets including the CIFAR-10, CIFAR- 100, MNIST and PASCAL VOC 2007 Classification dataset.",
            "authors": "Miao Sun;Tony X. Han;Xun Xu;Ming-Chang Liu;Ahmad Khodayari-Rostamabad",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1604.04333v2",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1604.04333v2",
            "publicationDate": "2016-04-15T02:07:42Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.20687995550141433,
            "title": "Latent Model Ensemble with Auto-localization",
            "topics": [
                {
                    "affinity": 0.5589706301689148,
                    "id": 0
                },
                {
                    "affinity": 0.43851685523986816,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "In this paper, we present the result of sparsity increase by CNN compression on 6 representative CNN networks including a famous localization CNN network VGG16-SSD-300. We will also show activation analysis result by applying the compressed CNN networks to a CNN HW accelerator model. Finally, this paper will be ended up with processing time estimation result of the CNN HW accelerator model which reflects the reduced transmission time of sparse weights.",
            "authors": "Mi-Young Lee;Joo-Hyun Lee;Jin-Kyu Kim;Byung-Jo Kim;Ju-Yeob Kim",
            "citationCount": 5,
            "id": "10.1109/ISOCC47750.2019.9027643",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9027643",
            "publicationDate": "6-9 Oct. 2019",
            "source": [
                "ieee"
            ],
            "tfidf": 0.20624370631916503,
            "title": "The Sparsity and Activation Analysis of Compressed CNN Networks in a HW CNN Accelerator Model",
            "topics": [
                {
                    "affinity": 0.9803771376609802,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "In recent years, convolutional neural network (CNN) has made a breakthrough development and been widely used in various fields, such as image recognition, target classification and natural language processing. However, with the continuous development of CNN, the complexity of CNN is gradually increasing. The ordinary hardware processors cannot meet the speed requirements of CNN. The hardware platform about CNN based on FPGA has gradually become the focus of research because of its parallel computing advantages. However, it is difficult and not friendly to implement CNN on FPGA for software developers. In this paper, we propose a FPGA-Cloud architecture for CNN. We implement CNN platform based on FPGA, and deploy the platform in the cloud. CNN resources based on FPGA can be utilized by local users through the network, which is language-friendly for software developers in hardware development and satisfies the user's requirement for CNN processing task.",
            "authors": "Guangju Wei;Yanzhao Hou;Zhao Zhao;Qimei Cui;Gang Deng;Xiaofeng Tao",
            "citationCount": 3,
            "id": "10.1109/APCC.2018.8633447",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8633447",
            "publicationDate": "12-14 Nov. 2018",
            "source": [
                "ieee"
            ],
            "tfidf": 0.19211028951644238,
            "title": "Demo: FPGA-Cloud Architecture For CNN",
            "topics": [
                {
                    "affinity": 0.2717796862125397,
                    "id": 0
                },
                {
                    "affinity": 0.7233000993728638,
                    "id": 1
                }
            ]
        },
        {
            "abstract": "With the rapid development of deep learning technology, CNN and LSTM have become two of the most popular neural networks. This paper combines CNN and LSTM or its variant and makes a slight change. It proposes a text classification model named NA-CNN-LSTM or NA-CNN-COIF-LSTM, which has no activation function in CNN. The experimental results on the subjective and objective text categorization dataset [1] show that the proposed model has better performance than the standard CNN or LSTM.",
            "authors": "Yuandong Luan;Shaofu Lin",
            "citationCount": 27,
            "id": "10.1109/ICAICA.2019.8873454",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8873454",
            "publicationDate": "29-31 March 2019",
            "source": [
                "ieee"
            ],
            "tfidf": 0.18899070687828898,
            "title": "Research on Text Classification Based on CNN and LSTM",
            "topics": [
                {
                    "affinity": 0.9838643670082092,
                    "id": 0
                }
            ]
        },
        {
            "abstract": "We present O-CNN, an Octree-based Convolutional Neural Network (CNN) for 3D shape analysis. Built upon the octree representation of 3D shapes, our method takes the average normal vectors of a 3D model sampled in the finest leaf octants as input and performs 3D CNN operations on the octants occupied by the 3D shape surface. We design a novel octree data structure to efficiently store the octant information and CNN features into the graphics memory and execute the entire O-CNN training and evaluation on the GPU. O-CNN supports various CNN structures and works for 3D shapes in different representations. By restraining the computations on the octants occupied by 3D surfaces, the memory and computational costs of the O-CNN grow quadratically as the depth of the octree increases, which makes the 3D CNN feasible for high-resolution 3D models. We compare the performance of the O-CNN with other existing 3D CNN solutions and demonstrate the efficiency and efficacy of O-CNN in three shape analysis tasks, including object classification, shape retrieval, and shape segmentation.",
            "authors": "Peng-Shuai Wang;Yang Liu;Yu-Xiao Guo;Chun-Yu Sun;Xin Tong",
            "citationCount": -1,
            "id": "10.1145/3072959.3073608",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1712.01537v1",
            "publicationDate": "2017-12-05T09:25:19Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.18183423190987502,
            "title": "O-CNN: Octree-based Convolutional Neural Networks for 3D Shape Analysis",
            "topics": [
                {
                    "affinity": 0.9781863689422607,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Convolutional Neural Network (CNN) give an unmatched performance in image classification, object detection and object tracking. As many of the modern embedded systems for portable devices deals with similar tasks, they often deploy CNN based algorithms. The intensive computational workload associated with CNN inference demands powerful computing platforms like Graphics Processing Units. However, deploying CNN on mobile devices demands low power, application specific computing platforms like Field-Programmable Gate Array (FPGA) and Application-Specific Integrated Circuit (ASIC) which can work as computation accelerator units. Moreover, using certain algorithmic optimizations like using Depthwise Separable Convolution instead of standard convolution, significantly reduces the computational burden of CNN inference. This paper discusses a pipelined architecture of Depthwise Separable Convolution followed by activation and pooling operations for a single layer of CNN. The architecture is implemented on Xilinx 7 series FPGA and works at a clock period of 40ns. It can be used as a building block for an integrated system of CNN accelerator for implementation on FPGAs of different sizes. This work focuses on speeding up the convolution process, instead of implementing large design of an integrated system of CNN accelerator which makes it difficult to focus on performance of the subsystems. To the best of the knowledge of the authors, earlier works have implemented an integrated system of CNN accelerator but the blueprint for architecture of a single layer of CNN is not discussed individually, which can be a great support for the beginners in understanding FPGA based computing accelerators for CNN.",
            "authors": "Harsh Srivastava;Kishor Sarawadekar",
            "citationCount": 3,
            "id": "10.1109/ASPCON49795.2020.9276672",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9276672",
            "publicationDate": "7-9 Oct. 2020",
            "source": [
                "ieee"
            ],
            "tfidf": 0.18145568481012792,
            "title": "A Depthwise Separable Convolution Architecture for CNN Accelerator",
            "topics": [
                {
                    "affinity": 0.10285790264606476,
                    "id": 0
                },
                {
                    "affinity": 0.22056636214256287,
                    "id": 1
                },
                {
                    "affinity": 0.6765757203102112,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Convolutional neural network (CNN) models have been widely used for fault diagnosis of complex systems. However, traditional CNN models rely on small kernel filters to obtain local features from images. Thus, an excessively deep CNN is required to capture global features, which are critical for fault diagnosis of dynamical systems. In this work, we present an improved CNN that embeds global features (GF-CNN). Our method uses a multi-layer perceptron (MLP) for dimension reduction to directly extract global features and integrate them into the CNN. The advantage of this method is that both local and global patterns in images can be captured by a simple model architecture instead of establishing deep CNN models. The proposed method is applied to the fault diagnosis of the Tennessee Eastman process. Simulation results show that the GF-CNN can significantly improve the fault diagnosis performance compared to traditional CNN. The proposed method can also be applied to other areas such as computer vision and image processing.",
            "authors": "Qiugang Lu;Saif S. S. Al-Wahaibi",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/2210.01727v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/2210.01727v1",
            "publicationDate": "2022-10-04T16:31:21Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.18083222574200727,
            "title": "Enhanced CNN with Global Features for Fault Diagnosis of Complex   Chemical Processes",
            "topics": [
                {
                    "affinity": 0.6079731583595276,
                    "id": 0
                },
                {
                    "affinity": 0.38794636726379395,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "For millimeter wave (mmWave) massive multiple-input multiple-output (MIMO) systems, hybrid processing architecture is usually used to reduce the complexity and cost, which poses a very challenging issue in channel estimation. In this paper, deep convolutional neural network (CNN) is employed to address this problem. We first propose a spatial-frequency CNN (SF-CNN) based channel estimation exploiting both the spatial and frequency correlation, where the corrupted channel matrices at adjacent subcarriers are input into the CNN simultaneously. Then, exploiting the temporal correlation in time-varying channels, a spatial-frequency-temporal CNN (SFT-CNN) based approach is developed to further improve the accuracy. Moreover, we design a spatial pilot-reduced CNN (SPR-CNN) to save spatial pilot overhead for channel estimation, where channels in several successive coherence intervals are grouped and estimated by a channel estimation unit with memory. Numerical results show that the proposed SF-CNN and SFT-CNN based approaches outperform the non-ideal minimum mean-squared error (MMSE) estimator but with reduced complexity, and achieve the performance close to the ideal MMSE estimator that is very difficult to be implemented in practical situations. They are also robust to different propagation scenarios. The SPR-CNN based approach achieves comparable performance to SF-CNN and SFT-CNN based approaches while only requires about one-third of spatial pilot overhead at the cost of complexity. The results in this paper clearly demonstrate that deep CNN can efficiently exploit channel correlation to improve the estimation performance for mmWave massive MIMO systems.",
            "authors": "Peihao Dong;Hua Zhang;Geoffrey Ye Li;Ivan Sim\u00f5es Gaspar;Navid NaderiAlizadeh",
            "citationCount": 112,
            "id": "10.1109/JSTSP.2019.2925975",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8752012",
            "publicationDate": "Sept. 2019",
            "source": [
                "ieee",
                "arxiv"
            ],
            "tfidf": 0.18053579629091496,
            "title": "Deep CNN-Based Channel Estimation for mmWave Massive MIMO Systems",
            "topics": [
                {
                    "affinity": 0.9949624538421631,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Faster R-CNN (R corresponds to \u201cRegion\u201d) which combined the RPN network and the Fast R-CNN network is one of the best ways to object detection of R-CNN series based on deep learning. The proposal obtained by RPN is directly connected to the ROI Pooling layer, which is a framework for CNN to achieve end-to-end object detection. The feasibility of Faster R-CNN implementation of ResNet101 network and PVANET network is discussed based on the implementation of Faster R-CNN in VGG16 network. Different Faster R-CNN models can be obtained by training with deep learning framework of Caffe. A better model can be obtained by comparing the experimental results using mean average precision (mAP) as an evaluation index. Numerical results show that Faster R-CNN trained by PVANET network obtained the highest mAP.",
            "authors": "Bin Liu;Wencang Zhao;Qiaoqiao Sun",
            "citationCount": 19,
            "id": "10.1109/CAC.2017.8243900",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8243900",
            "publicationDate": "20-22 Oct. 2017",
            "source": [
                "ieee"
            ],
            "tfidf": 0.1792899782807232,
            "title": "Study of object detection based on Faster R-CNN",
            "topics": [
                {
                    "affinity": 0.9716699123382568,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Deep learning is a powerful means to recognize remote sensing image scene categories. In this study, a deep convolutional neural network (CNN) based ensemble method is proposed. Firstly, a CNN architecture composed of the feature layer and the classifier layer is designed. Then the classifier layer of CNN is treated as base-learner and integrated with the AdaBoost technique to construct a CNN-AdaBoost ensemble framework. The proposed method is compared with the CNN-SVM and fine-tuned VGG16. The experiment results on UC Merced land-use dataset show that the CNN-AdaBoost achieves an improved overall accuracy by 4.46% against the sole CNN. Also, our method outperforms another two paradigms. Therefore, the proposed CNN based ensemble method is promising for image representations regarding remote sensing image scene classification.",
            "authors": "Xudong Hu;Penglin Zhang;Qi Zhang",
            "citationCount": 4,
            "id": "10.1109/IGARSS39084.2020.9324261",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9324261",
            "publicationDate": "26 Sept.-2 Oct. 2020",
            "source": [
                "ieee"
            ],
            "tfidf": 0.17885280098180822,
            "title": "A Novel Framework of CNN Integrated with Adaboost for Remote Sensing Scene Classification",
            "topics": [
                {
                    "affinity": 0.9903144836425781,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "The convolutional neural network (ConvNet or CNN) has proven to be very successful in many tasks such as those in computer vision. In this conceptual paper, we study the generative perspective of the discriminative CNN. In particular, we propose to learn the generative FRAME (Filters, Random field, And Maximum Entropy) model using the highly expressive filters pre-learned by the CNN at the convolutional layers. We show that the learning algorithm can generate realistic and rich object and texture patterns in natural scenes. We explain that each learned model corresponds to a new CNN unit at a layer above the layer of filters employed by the model. We further show that it is possible to learn a new layer of CNN units using a generative CNN model, which is a product of experts model, and the learning algorithm admits an EM interpretation with binary latent variables.",
            "authors": "Yang Lu;Song-Chun Zhu;Ying Nian Wu",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1509.08379v3",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1509.08379v3",
            "publicationDate": "2015-09-28T16:17:09Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.17267906322657514,
            "title": "Learning FRAME Models Using CNN Filters",
            "topics": [
                {
                    "affinity": 0.7777186036109924,
                    "id": 0
                },
                {
                    "affinity": 0.217106431722641,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Script identification in natural scene images is a key pre-step for text recognition and is also an indispensable condition for automatic text understanding systems that are designed for multi-language environments. In this paper, we present a novel framework integrating Local CNN and Global CNN both of which are based on ResNet-20 for script identification. We first obtain a lot of patches and segmented images based on the aspect ratios of the images. Subsequently, these patches and segmented images are used as inputs to Local CNN and Global CNN for training, respectively. Finally, to get the final results, the Adaboost algorithm is used to combine the results of Local CNN and Global CNN for decision-level fusion. Benefiting from such a strategy, Local CNN fully exploits the local features of the image, effectively revealing subtle differences among the scripts that are difficult to distinguish such as English, Greek, and Russian. Moreover, Global CNN mines the global features of the image to improve the accuracy of script identification. The experimental results demonstrate that our approach has a good performance on four public datasets.",
            "authors": "Liqiong Lu;Yaohua Yi;Faliang Huang;Kaili Wang;Qi Wang",
            "citationCount": 28,
            "id": "10.1109/ACCESS.2019.2911964",
            "openaccess": 1,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8693781",
            "publicationDate": "2019",
            "source": [
                "ieee"
            ],
            "tfidf": 0.17225565420159966,
            "title": "Integrating Local CNN and Global CNN for Script Identification in Natural Scene Images",
            "topics": [
                {
                    "affinity": 0.6394063830375671,
                    "id": 0
                },
                {
                    "affinity": 0.3567033112049103,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "In this paper, we proposed Transferable Ranking Convolutional Neural Network (TRk-CNN) that can be effectively applied when the classes of images to be classified show a high correlation with each other. The multi-class classification method based on the softmax function, which is generally used, is not effective in this case because the inter-class relationship is ignored. Although there is a Ranking-CNN that takes into account the ordinal classes, it cannot reflect the inter-class relationship to the final prediction. TRk-CNN, on the other hand, combines the weights of the primitive classification model to reflect the inter-class information to the final classification phase. We evaluated TRk-CNN in glaucoma image dataset that was labeled into three classes: normal, glaucoma suspect, and glaucoma eyes. Based on the literature we surveyed, this study is the first to classify three status of glaucoma fundus image dataset into three different classes. We compared the evaluation results of TRk-CNN with Ranking-CNN (Rk-CNN) and multi-class CNN (MC-CNN) using the DenseNet as the backbone CNN model. As a result, TRk-CNN achieved an average accuracy of 92.96%, specificity of 93.33%, sensitivity for glaucoma suspect of 95.12% and sensitivity for glaucoma of 93.98%. Based on average accuracy, TRk-CNN is 8.04% and 9.54% higher than Rk-CNN and MC-CNN and surprisingly 26.83% higher for sensitivity for suspicious than multi-class CNN. Our TRk-CNN is expected to be effectively applied to the medical image classification problem where the disease state is continuous and increases in the positive class direction.",
            "authors": "Tae Joon Jun;Youngsub Eom;Dohyeun Kim;Cherry Kim;Ji-Hye Park;Hoang Minh Nguyen;Daeyoung Kim",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1905.06509v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1905.06509v1",
            "publicationDate": "2019-05-16T02:45:04Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.1710070857207677,
            "title": "TRk-CNN: Transferable Ranking-CNN for image classification of glaucoma,   glaucoma suspect, and normal eyes",
            "topics": [
                {
                    "affinity": 0.9946632385253906,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Nowadays the CNN is widely used in practical applications for image classification task. However the design of the CNN model is very professional work and which is very difficult for ordinary users. Besides, even for experts of CNN, to select an optimal model for specific task may still need a lot of time (to train many different models). In order to solve this problem, we proposed an automated CNN recommendation system for image classification task. Our system is able to evaluate the complexity of the classification task and the classification ability of the CNN model precisely. By using the evaluation results, the system can recommend the optimal CNN model and which can match the task perfectly. The recommendation process of the system is very fast since we don't need any model training. The experiment results proved that the evaluation methods are very accurate and reliable.",
            "authors": "Song Wang;Li Sun;Wei Fan;Jun Sun;Satoshi Naoi;Koichi Shirahata;Takuya Fukagai;Yasumoto Tomita;Atsushi Ike",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1612.08484v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1612.08484v1",
            "publicationDate": "2016-12-27T03:18:28Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.17032394219972555,
            "title": "An Automated CNN Recommendation System for Image Classification Tasks",
            "topics": [
                {
                    "affinity": 0.5166758298873901,
                    "id": 0
                },
                {
                    "affinity": 0.4775470197200775,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "The recent explosive growth in convolutional neural network (CNN) research has produced a variety of new architectures for deep learning. One intriguing new architecture is the bilinear CNN (B-CNN), which has shown dramatic performance gains on certain fine-grained recognition problems [15]. We apply this new CNN to the challenging new face recognition benchmark, the IARPA Janus Benchmark A (IJB-A) [12]. It features faces from a large number of identities in challenging real-world conditions. Because the face images were not identified automatically using a computerized face detection system, it does not have the bias inherent in such a database. We demonstrate the performance of the B-CNN model beginning from an AlexNet-style network pre-trained on ImageNet. We then show results for fine-tuning using a moderate-sized and public external database, FaceScrub [17]. We also present results with additional fine-tuning on the limited training data provided by the protocol. In each case, the fine-tuned bilinear model shows substantial improvements over the standard CNN. Finally, we demonstrate how a standard CNN pre-trained on a large face database, the recently released VGG-Face model [20], can be converted into a B-CNN without any additional feature training. This B-CNN improves upon the CNN performance on the IJB-A benchmark, achieving 89.5% rank-1 recall.",
            "authors": "Aruni RoyChowdhury;Tsung-Yu Lin;Subhransu Maji;Erik Learned-Miller",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1506.01342v5",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1506.01342v5",
            "publicationDate": "2015-06-03T18:34:41Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.16770274225103232,
            "title": "One-to-many face recognition with bilinear CNNs",
            "topics": [
                {
                    "affinity": 0.9402886033058167,
                    "id": 1
                }
            ]
        },
        {
            "abstract": "State-of-the-art Convolutional Neural Network (CNN) based loop restoration generally involves a CNN structure with a large number of parameters and applies the CNN model to those degraded frames uniformly to generate their restored version, even though the contents within these frames are different. By contrast, in this paper, we propose a Guided CNN Restoration (GNR) scheme, where a CNN is used in conjunction with explicitly signaled guide parameters, with an aim to adapt the CNN model to different input contents. Specifically, the CNN architecture is designed such that the final restoration is constrained within the subspace generated by various output channels of the CNN, and meanwhile the weighting parameters for a linear combination of the output channels to obtain the final restoration are explicitly signaled by the encoders. The proposed GNR is incorporated into an AV1 encoder to replace the anchor in-loop filters and the weighting parameters are written into the encoded bitstream. Experimental results show that given a small CNN with 3,312 parameters, the proposed approach achieves a BD-rate reduction of 3.06% over the AV1 anchor, while the traditional CNN-based method only achieves 1.39%.",
            "authors": "Lingyi Kong;Dandan Ding;Fuchang Liu;Debargha Mukherjee;Urvang Joshi;Yue Chen",
            "citationCount": 2,
            "id": "10.1109/ICIP40778.2020.9190807",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9190807",
            "publicationDate": "25-28 Oct. 2020",
            "source": [
                "ieee"
            ],
            "tfidf": 0.16734395339643474,
            "title": "Guided CNN Restoration with Explicitly Signaled Linear Combination",
            "topics": [
                {
                    "affinity": 0.9924061298370361,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "The adoption of Convolutional Neural Network (CNN) models in high-stake domains is hindered by their inability to meet society's demand for transparency in decision-making. So far, a growing number of methodologies have emerged for developing CNN models that are interpretable by design. However, such models are not capable of providing interpretations in accordance with human perception, while maintaining competent performance. In this paper, we tackle these challenges with a novel, general framework for instantiating inherently interpretable CNN models, named E Pluribus Unum Interpretable CNN (EPU-CNN). An EPU-CNN model consists of CNN sub-networks, each of which receives a different representation of an input image expressing a perceptual feature, such as color or texture. The output of an EPU-CNN model consists of the classification prediction and its interpretation, in terms of relative contributions of perceptual features in different regions of the input image. EPU-CNN models have been extensively evaluated on various publicly available datasets, as well as a contributed benchmark dataset. Medical datasets are used to demonstrate the applicability of EPU-CNN for risk-sensitive decisions in medicine. The experimental results indicate that EPU-CNN models can achieve a comparable or better classification performance than other CNN architectures while providing humanly perceivable interpretations.",
            "authors": "George Dimas;Eirini Cholopoulou;Dimitris K. Iakovidis",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/2208.05369v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/2208.05369v1",
            "publicationDate": "2022-08-10T14:37:03Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.16716811617171035,
            "title": "E Pluribus Unum Interpretable Convolutional Neural Networks",
            "topics": [
                {
                    "affinity": 0.9937368035316467,
                    "id": 0
                }
            ]
        },
        {
            "abstract": "The objective of this paper is the effective transfer of the Convolutional Neural Network (CNN) feature in image search and classification. Systematically, we study three facts in CNN transfer. 1) We demonstrate the advantage of using images with a properly large size as input to CNN instead of the conventionally resized one. 2) We benchmark the performance of different CNN layers improved by average/max pooling on the feature maps. Our observation suggests that the Conv5 feature yields very competitive accuracy under such pooling step. 3) We find that the simple combination of pooled features extracted across various CNN layers is effective in collecting evidences from both low and high level descriptors. Following these good practices, we are capable of improving the state of the art on a number of benchmarks to a large margin.",
            "authors": "Liang Zheng;Yali Zhao;Shengjin Wang;Jingdong Wang;Qi Tian",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1604.00133v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1604.00133v1",
            "publicationDate": "2016-04-01T05:31:57Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.16650497572424702,
            "title": "Good Practice in CNN Feature Transfer",
            "topics": [
                {
                    "affinity": 0.7281310558319092,
                    "id": 0
                },
                {
                    "affinity": 0.2645530700683594,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Deep learning has been utilized for the statistical downscaling of climate data. Specifically, a two-dimensional (2D) convolutional neural network (CNN) has been successfully applied to precipitation estimation. This study implements a three-dimensional (3D) CNN to estimate watershed-scale daily precipitation from 3D atmospheric data and compares the results with those for a 2D CNN. The 2D CNN is extended along the time direction (3D-CNN-Time) and the vertical direction (3D-CNN-Vert). The precipitation estimates of these extended CNNs are compared with those of the 2D CNN in terms of the root-mean-square error (RMSE), Nash-Sutcliffe efficiency (NSE), and 99th percentile RMSE. It is found that both 3D-CNN-Time and 3D-CNN-Vert improve the model accuracy for precipitation estimation compared to the 2D CNN. 3D-CNN-Vert provided the best estimates during the training and test periods in terms of RMSE and NSE.",
            "authors": "Takeyoshi Nagasato;Kei Ishida;Ali Ercan;Tongbi Tu;Masato Kiyama;Motoki Amagasaki;Kazuki Yokoo",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/2112.06571v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/2112.06571v1",
            "publicationDate": "2021-12-13T11:26:12Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.1645728078957885,
            "title": "Extension of Convolutional Neural Network along Temporal and Vertical   Directions for Precipitation Downscaling",
            "topics": [
                {
                    "affinity": 0.9905437231063843,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Hyperspectral image (HSI) classification is widely used for the analysis of remotely sensed images. Hyperspectral imagery includes varying bands of images. Convolutional neural network (CNN) is one of the most frequently used deep learning-based methods for visual data processing. The use of CNN for HSI classification is also visible in recent works. These approaches are mostly based on 2-D CNN. On the other hand, the HSI classification performance is highly dependent on both spatial and spectral information. Very few methods have used the 3-D-CNN because of increased computational complexity. This letter proposes a hybrid spectral CNN (HybridSN) for HSI classification. In general, the HybridSN is a spectral-spatial 3-D-CNN followed by spatial 2-D-CNN. The 3-D-CNN facilitates the joint spatial-spectral feature representation from a stack of spectral bands. The 2-D-CNN on top of the 3-D-CNN further learns more abstract-level spatial representation. Moreover, the use of hybrid CNNs reduces the complexity of the model compared to the use of 3-D-CNN alone. To test the performance of this hybrid approach, very rigorous HSI classification experiments are performed over Indian Pines, University of Pavia, and Salinas Scene remote sensing data sets. The results are compared with the state-of-the-art hand-crafted as well as end-to-end deep learning-based methods. A very satisfactory performance is obtained using the proposed HybridSN for HSI classification. The source code can be found at https://github.com/gokriznastic/HybridSN.",
            "authors": "Swalpa Kumar Roy;Gopal Krishna;Shiv Ram Dubey;Bidyut B. Chaudhuri",
            "citationCount": 313,
            "id": "10.1109/LGRS.2019.2918719",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8736016",
            "publicationDate": "Feb. 2020",
            "source": [
                "ieee",
                "arxiv"
            ],
            "tfidf": 0.16446405127417388,
            "title": "HybridSN: Exploring 3-D\u20132-D CNN Feature Hierarchy for Hyperspectral Image Classification",
            "topics": [
                {
                    "affinity": 0.9841256737709045,
                    "id": 0
                }
            ]
        },
        {
            "abstract": "Convolutional neural networks (CNN) which is a feature-based machine learning algorithm is very popular in hyperspectral image (HSI) classification. CNN exploits the spatial relationship between HIS. However, HSI intrinsically have a sequence-based data structure called the spectral features. Combining spectral and spatial information offers a more comprehensive classification approach. 3D-CNN can exploit Spatial-spectral relationship but can be computationally expensive. LSTM, an important branch of the deep learning family, is mainly designed to handle Sequential data. In this paper we propose a model that uses the 1D CNN and 2D-CNN for extracting the spatial features and a LSTM for extracting the spectral features. Experimental results show that our method outperforms the accuracies reported in the existing CNN and LSTM based methods.",
            "authors": "Harshula Tulapurkar;Biplab Banerjee;B Krishna Mohan",
            "citationCount": 1,
            "id": "10.1109/InGARSS48198.2020.9358957",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9358957",
            "publicationDate": "1-4 Dec. 2020",
            "source": [
                "ieee"
            ],
            "tfidf": 0.16243265756897526,
            "title": "Effective and Efficient Dimensionality Reduction of Hyperspectral Image using CNN and LSTM network",
            "topics": [
                {
                    "affinity": 0.7746466398239136,
                    "id": 0
                },
                {
                    "affinity": 0.22026991844177246,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "In recent years, the research community has discovered that deep neural networks (DNNs) and convolutional neural networks (CNNs) can yield higher accuracy than all previous solutions to a broad array of machine learning problems. To our knowledge, there is no single CNN/DNN architecture that solves all problems optimally. Instead, the \"right\" CNN/DNN architecture varies depending on the application at hand. CNN/DNNs comprise an enormous design space. Quantitatively, we find that a small region of the CNN design space contains 30 billion different CNN architectures.   In this dissertation, we develop a methodology that enables systematic exploration of the design space of CNNs. Our methodology is comprised of the following four themes.   1. Judiciously choosing benchmarks and metrics.   2. Rapidly training CNN models.   3. Defining and describing the CNN design space.   4. Exploring the design space of CNN architectures.   Taken together, these four themes comprise an effective methodology for discovering the \"right\" CNN architectures to meet the needs of practical applications.",
            "authors": "Forrest Iandola",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1612.06519v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1612.06519v1",
            "publicationDate": "2016-12-20T06:20:43Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.16081352354652878,
            "title": "Exploring the Design Space of Deep Convolutional Neural Networks at   Large Scale",
            "topics": [
                {
                    "affinity": 0.992003858089447,
                    "id": 0
                }
            ]
        },
        {
            "abstract": "Convolutional neural network (CNN) has presented a great success in numerous areas and has sparked an increasing interest in accelerating CNN using hardware like FPGAs. However, efficient FPGA design for CNN applications requires a long development time and a strong background in hardware details. Consequently, an easy-to-use yet powerful auto CNN design optimization framework is required. In this work, we propose a collaborative framework to model and optimize the OpenCL based FPGA design for CNN applications according to the device resource limitation and the CNN specification. Our framework mainly consists of LoopTree, a novel data structure we propose to capture the structure of OpenCL based CNN design; a LoopTree based coarse-grained model, which will estimate the performance of the CNN design at the module level; and a source code based fine-grained model, which will estimate the CNN design performance in a cycle-accurate manner. Efficient designs can be achieved by collaborating the two models in a search and refined manner. A variety of OpenCL based designs have been implemented on board to verify our framework. The results show that our coarse-grained model and fine-grained model have an average estimation error of 10.2% and 4.7% which are much lower than prevalent operation statistics based estimation calculated by the predefined formula for specific loop schedules.",
            "authors": "Jiandong Mu;Wei Zhang;Hao Liang;Sharad Sinha",
            "citationCount": 5,
            "id": "10.1109/FPL.2018.00032",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8533483",
            "publicationDate": "27-31 Aug. 2018",
            "source": [
                "ieee"
            ],
            "tfidf": 0.15936218232255941,
            "title": "A Collaborative Framework for FPGA-based CNN Design Modeling and Optimization",
            "topics": [
                {
                    "affinity": 0.6004136800765991,
                    "id": 1
                },
                {
                    "affinity": 0.3928782343864441,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Convolutional Neural Network (CNN) image classifiers are traditionally designed to have sequential convolutional layers with a single output layer. This is based on the assumption that all target classes should be treated equally and exclusively. However, some classes can be more difficult to distinguish than others, and classes may be organized in a hierarchy of categories. At the same time, a CNN is designed to learn internal representations that abstract from the input data based on its hierarchical layered structure. So it is natural to ask if an inverse of this idea can be applied to learn a model that can predict over a classification hierarchy using multiple output layers in decreasing order of class abstraction. In this paper, we introduce a variant of the traditional CNN model named the Branch Convolutional Neural Network (B-CNN). A B-CNN model outputs multiple predictions ordered from coarse to fine along the concatenated convolutional layers corresponding to the hierarchical structure of the target classes, which can be regarded as a form of prior knowledge on the output. To learn with B-CNNs a novel training strategy, named the Branch Training strategy (BT-strategy), is introduced which balances the strictness of the prior with the freedom to adjust parameters on the output layers to minimize the loss. In this way we show that CNN based models can be forced to learn successively coarse to fine concepts in the internal layers at the output stage, and that hierarchical prior knowledge can be adopted to boost CNN models' classification performance. Our models are evaluated to show that the B-CNN extensions improve over the corresponding baseline CNN on the benchmark datasets MNIST, CIFAR-10 and CIFAR-100.",
            "authors": "Xinqi Zhu;Michael Bain",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1709.09890v2",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1709.09890v2",
            "publicationDate": "2017-09-28T11:02:43Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.15843510277723374,
            "title": "B-CNN: Branch Convolutional Neural Network for Hierarchical   Classification",
            "topics": [
                {
                    "affinity": 0.49395787715911865,
                    "id": 0
                },
                {
                    "affinity": 0.4311519265174866,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "For about 10 years, detecting the presence of a secret message hidden in an image was performed with an Ensemble Classifier trained with Rich features. In recent years, studies such as Xu et al. have indicated that well-designed convolutional Neural Networks (CNN) can achieve comparable performance to the two-step machine learning approaches.   In this paper, we propose a CNN that outperforms the state-ofthe-art in terms of error probability. The proposition is in the continuity of what has been recently proposed and it is a clever fusion of important bricks used in various papers. Among the essential parts of the CNN, one can cite the use of a pre-processing filterbank and a Truncation activation function, five convolutional layers with a Batch Normalization associated with a Scale Layer, as well as the use of a sufficiently sized fully connected section. An augmented database has also been used to improve the training of the CNN.   Our CNN was experimentally evaluated against S-UNIWARD and WOW embedding algorithms and its performances were compared with those of three other methods: an Ensemble Classifier plus a Rich Model, and two other CNN steganalyzers.",
            "authors": "Mehdi Yedroudj;Frederic Comby;Marc Chaumont",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1803.00407v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1803.00407v1",
            "publicationDate": "2018-02-26T14:09:14Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.15455245930318223,
            "title": "Yedrouj-Net: An efficient CNN for spatial steganalysis",
            "topics": [
                {
                    "affinity": 0.11810152977705002,
                    "id": 0
                },
                {
                    "affinity": 0.8776894807815552,
                    "id": 1
                }
            ]
        },
        {
            "abstract": "Deep learning-based methods based on convolutional neural networks (CNNs) have demonstrated remarkable performance in hyperspectral image (HSI) classification. Most of these approaches are only based on 2-D CNN or 3-D CNN. It is dramatic from the literature that using just 2-D CNN may result in missing channel relationship information, and using just 3-D CNN may make the model very complex. Moreover, the existing network models do not pay enough attention to extracting spectral-spatial correlation information. To address these issues, we propose a deep collaborative attention network for HSI classification by combining 2-D CNN, and 3-D CNN (CACNN). Specifically, we first extract spectral-spatial features by using 2-D CNN, and 3-D CNN, respectively, and then use a \u201cNonLocalBlock\u201d to combine these two kinds of features. This block serves as a typical spatial attention mechanism, and makes salient features be emphasized. Then, we propose a \u201cConv_Block\u201d that is similar to the lightweight dense block to extract correlation information contained in the feature maps. Finally, we consider a deep multilayer feature fusion strategy, and thereby combine the features of different hierarchical layers to extract the strong correlated spectral-spatial information among them. To test the performance of CACNN approach, several experiments are performed on four well-known HSIs. The results are compared with the state-of-the-art approaches, and satisfactory performance is obtained by our proposed method. The code of CACNN method is available on Dr. J. Liu's GitHub.",
            "authors": "Hao Guo;Jianjun Liu;Jinlong Yang;Zhiyong Xiao;Zebin Wu",
            "citationCount": 18,
            "id": "10.1109/JSTARS.2020.3016739",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9167434",
            "publicationDate": "2020",
            "source": [
                "ieee"
            ],
            "tfidf": 0.15345465570793743,
            "title": "Deep Collaborative Attention Network for Hyperspectral Image Classification by Combining 2-D CNN and 3-D CNN",
            "topics": [
                {
                    "affinity": 0.24312318861484528,
                    "id": 0
                },
                {
                    "affinity": 0.7539180517196655,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "we propose a method by applying Convolutional Neural Networks (CNN) to non-structured data. CNN has been successful in many fields such as image processing and speech recognition. On the other hand, it was difficult to adapt CNN to n non-structured data such as a csv file with multiple variables. The sequence of the data of the low dimensional grid structure such as the image has a meaning, and the CNN recognizes the order as the feature of the image and processes it. Due to this constraint, CNN could not perform feature recognition on non-structured data whose sequence can be reordered while leaving the meaning intact. In this work we developed a method to tackle this issue and make CNN applicable by endowing meaning to the sequence of non-structured data, and demonstrated its effectiveness by adding improvements.",
            "authors": "Kei Takahashi;Takumi Numajiri;Masaru Sogabe;Katsuyoshi Sakamoto;Koichi Yamaguchi;Tomah Sogabe",
            "citationCount": 0,
            "id": "10.1109/CANDARW.2018.00051",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8590905",
            "publicationDate": "27-30 Nov. 2018",
            "source": [
                "ieee"
            ],
            "tfidf": 0.15301067601867221,
            "title": "Development of Generic CNN Deep Learning Method Using Feature Graph",
            "topics": [
                {
                    "affinity": 0.9882938861846924,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Convolutional Neural Networks (CNNs) have revolutionized performances in several machine learning tasks such as image classification, object tracking, and keyword spotting. However, given that they contain a large number of parameters, their direct applicability into low resource tasks is not straightforward. In this work, we experiment with an application of CNN models to gastrointestinal landmark classification with only a few thousands of training samples through transfer learning. As in a standard transfer learning approach, we train CNNs on a large external corpus, followed by representation extraction for the medical images. Finally, a classifier is trained on these CNN representations. However, given that several variants of CNNs exist, the choice of CNN is not obvious. To address this, we develop a novel metric that can be used to predict test performances, given CNN representations on the training set. Not only we demonstrate the superiority of the CNN based transfer learning approach against an assembly of knowledge driven features, but the proposed metric also carries an 87% correlation with the test set performances as obtained using various CNN representations.",
            "authors": "Taruna Agrawal;Rahul Gupta;Shrikanth Narayanan",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1903.11176v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1903.11176v1",
            "publicationDate": "2019-03-26T22:05:58Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.1528812897557614,
            "title": "On evaluating CNN representations for low resource medical image   classification",
            "topics": [
                {
                    "affinity": 0.5852960348129272,
                    "id": 0
                },
                {
                    "affinity": 0.4105096459388733,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Convolutional Neural Network (CNN) is one of the most significant networks in the deep learning field. Since CNN made impressive achievements in many areas, including but not limited to computer vision and natural language processing, it attracted much attention both of industry and academia in the past few years. The existing reviews mainly focus on the applications of CNN in different scenarios without considering CNN from a general perspective, and some novel ideas proposed recently are not covered. In this review, we aim to provide novel ideas and prospects in this fast-growing field as much as possible. Besides, not only two-dimensional convolution but also one-dimensional and multi-dimensional ones are involved. First, this review starts with a brief introduction to the history of CNN. Second, we provide an overview of CNN. Third, classic and advanced CNN models are introduced, especially those key points making them reach state-of-the-art results. Fourth, through experimental analysis, we draw some conclusions and provide several rules of thumb for function selection. Fifth, the applications of one-dimensional, two-dimensional, and multi-dimensional convolution are covered. Finally, some open issues and promising directions for CNN are discussed to serve as guidelines for future work.",
            "authors": "Zewen Li;Wenjie Yang;Shouheng Peng;Fan Liu",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/2004.02806v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/2004.02806v1",
            "publicationDate": "2020-04-01T14:04:10Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.1528786391237396,
            "title": "A Survey of Convolutional Neural Networks: Analysis, Applications, and   Prospects",
            "topics": [
                {
                    "affinity": 0.9097514152526855,
                    "id": 0
                }
            ]
        },
        {
            "abstract": "Supporting convolutional neural network (CNN) inference on resource-constrained IoT devices in a timely manner has been an outstanding challenge for emerging smart systems. To mitigate the burden on IoT devices, the prevailing solution is to offload the CNN inference task, which is usually composed of billions of operations, to public cloud. However, the \"offloading-to-cloud\" solution may cause privacy breach while moving sensitive data to cloud. For privacy protection, the research community has resorted to advanced cryptographic primitives and approximation techniques to support CNN inference on encrypted data. Consequently, these attempts cause impractical computational overhead on IoT devices and degrade the performance of CNNs. Moreover, relying on the remote cloud can cause additional network latency and even make the system dysfunction when network connection is off.   We proposes an extremely lightweight edge device assisted private CNN inference solution for IoT devices, namely LEP-CNN. The main design of LEP-CNN is based on a novel online/offline encryption scheme. The decryption of LEP-CNN is pre-computed offline via utilizing the linear property of the most time-consuming operations of CNNs. As a result, LEP-CNN allows IoT devices to securely offload over 99% CNN operations, and edge devices to execute CNN inference on encrypted data as efficient as on plaintext. LEP-CNN also provides an integrity check option to help IoT devices detect error results with a successful rate over 99%. Experiments on AlexNet show that LEP-CNN can speed up the CNN inference for more than 35 times for resource constrained IoT devices. A homomorphic encryption based AlexNet using CryptoNets is implemented to compare with LEP-CNN to demonstrate that LEP-CNN has a better performance than homomorphic encryption based privacy preserving neural networks under time-sensitive scenarios.",
            "authors": "Yifan Tian;Jiawei Yuan;Shucheng Yu;Yantian Hou",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1901.04100v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1901.04100v1",
            "publicationDate": "2019-01-14T01:14:03Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.15176738128202572,
            "title": "LEP-CNN: A Lightweight Edge Device Assisted Privacy-preserving CNN   Inference Solution for IoT",
            "topics": [
                {
                    "affinity": 0.9956687092781067,
                    "id": 1
                }
            ]
        },
        {
            "abstract": "Recently, CNN is a popular choice to handle the hyperspectral image classification challenges. In spite of having such large spectral information in Hyper-Spectral Image(s) (HSI), it creates a curse of dimensionality. Also, large spatial variability of spectral signature adds more difficulty in classification problem. Additionally, training a CNN in the end to end fashion with scarced training examples is another challenging and interesting problem. In this paper, a novel target-patch-orientation method is proposed to train a CNN based network. Also, we have introduced a hybrid of 3D-CNN and 2D-CNN based network architecture to implement band reduction and feature extraction methods, respectively. Experimental results show that our method outperforms the accuracies reported in the existing state of the art methods.",
            "authors": "Jayasree Saha;Yuvraj Khanna;Jayanta Mukherjee",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/2001.11198v3",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/2001.11198v3",
            "publicationDate": "2020-01-30T07:45:07Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.15098654309272266,
            "title": "A CNN With Multi-scale Convolution for Hyperspectral Image   Classification using Target-Pixel-Orientation scheme",
            "topics": [
                {
                    "affinity": 0.9254250526428223,
                    "id": 0
                }
            ]
        },
        {
            "abstract": "Conventional 2D Convolutional Neural Networks (CNN) extract features from an input image by applying linear filters. These filters compute the spatial coherence by weighting the photometric information on a fixed neighborhood without taking into account the geometric information. We tackle the problem of improving the classical RGB CNN methods by using the depth information provided by the RGB-D cameras. State-of-the-art approaches use depth as an additional channel or image (HHA) or pass from 2D CNN to 3D CNN. This paper proposes a novel and generic procedure to articulate both photometric and geometric information in CNN architecture. The depth data is represented as a 2D offset to adapt spatial sampling locations. The new model presented is invariant to scale and rotation around the X and the Y axis of the camera coordinate system. Moreover, when depth data is constant, our model is equivalent to a regular CNN. Experiments of benchmarks validate the effectiveness of our model.",
            "authors": "Zongwei Wu;Guillaume Allibert;Christophe Stolz;Cedric Demonceaux",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/2009.09976v2",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/2009.09976v2",
            "publicationDate": "2020-09-21T15:58:32Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.150091381367113,
            "title": "Depth-Adapted CNN for RGB-D cameras",
            "topics": [
                {
                    "affinity": 0.9172346591949463,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Deep Convolutional Neural Network (CNN) is a special type of Neural Networks, which has shown exemplary performance on several competitions related to Computer Vision and Image Processing. Some of the exciting application areas of CNN include Image Classification and Segmentation, Object Detection, Video Processing, Natural Language Processing, and Speech Recognition. The powerful learning ability of deep CNN is primarily due to the use of multiple feature extraction stages that can automatically learn representations from the data. The availability of a large amount of data and improvement in the hardware technology has accelerated the research in CNNs, and recently interesting deep CNN architectures have been reported. Several inspiring ideas to bring advancements in CNNs have been explored, such as the use of different activation and loss functions, parameter optimization, regularization, and architectural innovations. However, the significant improvement in the representational capacity of the deep CNN is achieved through architectural innovations. Notably, the ideas of exploiting spatial and channel information, depth and width of architecture, and multi-path information processing have gained substantial attention. Similarly, the idea of using a block of layers as a structural unit is also gaining popularity. This survey thus focuses on the intrinsic taxonomy present in the recently reported deep CNN architectures and, consequently, classifies the recent innovations in CNN architectures into seven different categories. These seven categories are based on spatial exploitation, depth, multi-path, width, feature-map exploitation, channel boosting, and attention. Additionally, the elementary understanding of CNN components, current challenges, and applications of CNN are also provided.",
            "authors": "Asifullah Khan;Anabia Sohail;Umme Zahoora;Aqsa Saeed Qureshi",
            "citationCount": -1,
            "id": "10.1007/s10462-020-09825-6",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1901.06032v7",
            "publicationDate": "2019-01-17T23:20:23Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.14773553978685847,
            "title": "A Survey of the Recent Architectures of Deep Convolutional Neural   Networks",
            "topics": [
                {
                    "affinity": 0.2014528065919876,
                    "id": 0
                },
                {
                    "affinity": 0.7870802879333496,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Convolutional Neural Network (CNN) is the state-of-the-art for image classification task. Here we have briefly discussed different components of CNN. In this paper, We have explained different CNN architectures for image classification. Through this paper, we have shown advancements in CNN from LeNet-5 to latest SENet model. We have discussed the model description and training details of each model. We have also drawn a comparison among those models.",
            "authors": "Farhana Sultana;A. Sufian;Paramartha Dutta",
            "citationCount": -1,
            "id": "10.1109/ICRCICN.2018.8718718",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1905.03288v1",
            "publicationDate": "2019-05-08T18:34:19Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.14670209037263926,
            "title": "Advancements in Image Classification using Convolutional Neural Network",
            "topics": [
                {
                    "affinity": 0.35033297538757324,
                    "id": 0
                },
                {
                    "affinity": 0.639223575592041,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Recently, dynamic inference has emerged as a promising way to reduce the computational cost of deep convolutional neural network (CNN). In contrast to static methods (e.g. weight pruning), dynamic inference adaptively adjusts the inference process according to each input sample, which can considerably reduce the computational cost on \"easy\" samples while maintaining the overall model performance. In this paper, we introduce a general framework, S2DNAS, which can transform various static CNN models to support dynamic inference via neural architecture search. To this end, based on a given CNN model, we first generate a CNN architecture space in which each architecture is a multi-stage CNN generated from the given model using some predefined transformations. Then, we propose a reinforcement learning based approach to automatically search for the optimal CNN architecture in the generated space. At last, with the searched multi-stage network, we can perform dynamic inference by adaptively choosing a stage to evaluate for each sample. Unlike previous works that introduce irregular computations or complex controllers in the inference or re-design a CNN model from scratch, our method can generalize to most of the popular CNN architectures and the searched dynamic network can be directly deployed using existing deep learning frameworks in various hardware devices.",
            "authors": "Zhihang Yuan;Bingzhe Wu;Zheng Liang;Shiwan Zhao;Weichen Bi;Guangyu Sun",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1911.07033v2",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1911.07033v2",
            "publicationDate": "2019-11-16T13:49:44Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.14549220051325693,
            "title": "S2DNAS:Transforming Static CNN Model for Dynamic Inference via Neural   Architecture Search",
            "topics": [
                {
                    "affinity": 0.8844626545906067,
                    "id": 0
                },
                {
                    "affinity": 0.11227716505527496,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Despite the promise of Convolutional neural network (CNN) based classification models for histopathological images, it is infeasible to quantify its uncertainties. Moreover, CNNs may suffer from overfitting when the data is biased. We show that Bayesian-CNN can overcome these limitations by regularizing automatically and by quantifying the uncertainty. We have developed a novel technique to utilize the uncertainties provided by the Bayesian-CNN that significantly improves the performance on a large fraction of the test data (about 6% improvement in accuracy on 77% of test data). Further, we provide a novel explanation for the uncertainty by projecting the data into a low dimensional space through a nonlinear dimensionality reduction technique. This dimensionality reduction enables interpretation of the test data through visualization and reveals the structure of the data in a low dimensional feature space. We show that the Bayesian-CNN can perform much better than the state-of-the-art transfer learning CNN (TL-CNN) by reducing the false negative and false positive by 11% and 7.7% respectively for the present data set. It achieves this performance with only 1.86 million parameters as compared to 134.33 million for TL-CNN. Besides, we modify the Bayesian-CNN by introducing a stochastic adaptive activation function. The modified Bayesian-CNN performs slightly better than Bayesian-CNN on all performance metrics and significantly reduces the number of false negatives and false positives (3% reduction for both). We also show that these results are statistically significant by performing McNemar's statistical significance test. This work shows the advantages of Bayesian-CNN against the state-of-the-art, explains and utilizes the uncertainties for histopathological images. It should find applications in various medical image classifications.",
            "authors": "Ponkrshnan Thiagarajan;Pushkar Khairnar;Susanta Ghosh",
            "citationCount": -1,
            "id": "10.1109/TMI.2021.3123300",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/2010.12575v3",
            "publicationDate": "2020-10-07T23:20:26Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.1446488856588742,
            "title": "Explanation and Use of Uncertainty Quantified by Bayesian Neural Network   Classifiers for Breast Histopathology Images",
            "topics": [
                {
                    "affinity": 0.12658946216106415,
                    "id": 0
                },
                {
                    "affinity": 0.8697218298912048,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Recently Convolutional Neural Networks (CNNs) are widely applied into novel intelligent applications and systems. However, the CNN computation performance is significantly hindered by its computation flow, which computes the model structure sequentially by layers with massive convolution operations. Such a layer-wise sequential computation flow can cause certain performance issues, such as resource under-utilization, huge memory overhead, etc. To solve these problems, we propose a novel CNN structural decoupling method, which could decouple CNN models into \"critical paths\" and eliminate the inter-layer data dependency. Based on this method, we redefine the CNN computation flow into parallel and cascade computing paradigms, which can significantly enhance the CNN computation performance with both multi-core and single-core CPU processors. Experiments show that, our DC-CNN framework could reduce 24% to 33% latency on multi-core CPUs for CIFAR and ImageNet. On small-capacity mobile platforms, cascade computing could reduce the latency by average 24% on ImageNet and 42% on CIFAR10. Meanwhile, the memory reduction could also reach average 21% and 64%, respectively.",
            "authors": "Fuxun Yu;Zhuwei Qin;Di Wang;Ping Xu;Chenchen Liu;Zhi Tian;Xiang Chen",
            "citationCount": 2,
            "id": "10.23919/DATE48585.2020.9116429",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9116429",
            "publicationDate": "9-13 March 2020",
            "source": [
                "ieee"
            ],
            "tfidf": 0.14429549879029296,
            "title": "DC-CNN: Computational Flow Redefinition for Efficient CNN through Structural Decoupling",
            "topics": [
                {
                    "affinity": 0.37151777744293213,
                    "id": 0
                },
                {
                    "affinity": 0.554452657699585,
                    "id": 1
                }
            ]
        },
        {
            "abstract": "Weaving is a particular type of cloth made specifically with distinctive motifs which is a traditional handicraft of Nusantara archipelago. It is necessary to have a motive recognition technology innovation that can identify weaving motifs in the Nusantara archipelago woven fabrics. Recognition of pattern in the woven fabric is very difficult because the patterns and types of Nusantara weaving are very diverse. This research will design the classification of woven fabric patterns using Faster R-CNN and compare it with The Convolutional Neural Network method. This research aims to compare the performance of Faster R-CNN and CNN in classifying weaving patterns by measuring the accuracy, precision, and recall levels of the recognition of Malay woven motifs with both Faster R-CNN and CNN. After collecting datasets, analyzing system requirements, designing preprocessing, and training processes for both CNN and Faster R-CNN, it is found that from the training data in the form of images of the Malay woven fabric and the gringsing woven cloth, it is found that through the K-Fold Cross Validation with a value of k = 5, the classification using Faster R-CNN obtained 82.14% accuracy, 91.38% precision and 91.36% recall. Meanwhile, the CNN method obtained 76% accuracy, 74.1% precision, and 72.3 recall. The faster R-CNN was superior in all parameter tests compared to CNN with a difference of 6.14% for accuracy, 17.28% more precision, and 19.06% for recall value. It found that the choice of Feature extractor architecture impact detection accuracy.",
            "authors": "Yoze Rizki;Reny Medikawati Taufiq;Harun Mukhtar;Febby Apri Wenando;Januar Al Amien",
            "citationCount": 1,
            "id": "10.1109/ICIMCIS51567.2020.9354324",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9354324",
            "publicationDate": "19-20 Nov. 2020",
            "source": [
                "ieee"
            ],
            "tfidf": 0.14371879241439528,
            "title": "Comparison Between Faster R-CNN and CNN in Recognizing Weaving Patterns",
            "topics": [
                {
                    "affinity": 0.9938415288925171,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "In recent years, Convolutional Neural Network (CNN) based methods have achieved great success in a large number of applications and have been among the most powerful and widely used techniques in computer vision. However, CNN-based methods are computational-intensive and resource-consuming, and thus are hard to be integrated into embedded systems such as smart phones, smart glasses, and robots. FPGA is one of the most promising platforms for accelerating CNN, but the limited on-chip memory size limit the performance of FPGA accelerator for CNN. In this paper, we propose a framework for designing CNN accelerator on embedded FPGA for image classification. The proposed framework provides a tool for FPGA resource-aware design space exploration of CNNs and automatically generates the hardware description of the CNN to be programmed on a target FPGA. The framework consists of three main backends; software, hardware generation, and simulation/precision adjustment. The software backend serves as an API to the designer to design the CNN and train it according to the hardware resources that are available. Using the CNN model, hardware backend generates the necessary hardware components and integrates them to generate the hardware description of the CNN. Finaly, Simulation/precision adjustment backend adjusts the inter-layer precision units to minimize the classification error. We used 16-bit fixed-point data in a CNN accelerator (FPGA) and compared it to the exactly similar software version running on an ARM processor (32-bit floating point data). We encounter about 3% accuracy loss in classification of the accelerated (FPGA) version. In return, we got up to 15.75x speedup by classifying with the accelerated version on the FPGA.",
            "authors": "Ali Jahanshahi",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1911.06777v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1911.06777v1",
            "publicationDate": "2019-11-15T17:42:52Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.14237639711980982,
            "title": "TinyCNN: A Tiny Modular CNN Accelerator for Embedded FPGA",
            "topics": [
                {
                    "affinity": 0.31367945671081543,
                    "id": 1
                },
                {
                    "affinity": 0.6615567207336426,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Deep neural networks are representation learning techniques. During training, a deep net is capable of generating a descriptive language of unprecedented size and detail in machine learning. Extracting the descriptive language coded within a trained CNN model (in the case of image data), and reusing it for other purposes is a field of interest, as it provides access to the visual descriptors previously learnt by the CNN after processing millions of images, without requiring an expensive training phase. Contributions to this field (commonly known as feature representation transfer or transfer learning) have been purely empirical so far, extracting all CNN features from a single layer close to the output and testing their performance by feeding them to a classifier. This approach has provided consistent results, although its relevance is limited to classification tasks. In a completely different approach, in this paper we statistically measure the discriminative power of every single feature found within a deep CNN, when used for characterizing every class of 11 datasets. We seek to provide new insights into the behavior of CNN features, particularly the ones from convolutional layers, as this can be relevant for their application to knowledge representation and reasoning. Our results confirm that low and middle level features may behave differently to high level features, but only under certain conditions. We find that all CNN features can be used for knowledge representation purposes both by their presence or by their absence, doubling the information a single CNN feature may provide. We also study how much noise these features may include, and propose a thresholding approach to discard most of it. All these insights have a direct application to the generation of CNN embedding spaces.",
            "authors": "Dario Garcia-Gasulla;Ferran Par\u00e9s;Armand Vilalta;Jonatan Moreno;Eduard Ayguad\u00e9;Jes\u00fas Labarta;Ulises Cort\u00e9s;Toyotaro Suzumura",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1703.01127v4",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1703.01127v4",
            "publicationDate": "2017-03-03T12:23:13Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.14223733317150888,
            "title": "On the Behavior of Convolutional Nets for Feature Extraction",
            "topics": [
                {
                    "affinity": 0.4426949620246887,
                    "id": 0
                },
                {
                    "affinity": 0.554458498954773,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "We focus on a convolutional neural network (CNN), which has recently been utilized for fluid flow analyses, from the perspective on the influence of various operations inside it by considering some canonical regression problems with fluid flow data. We consider two types of CNN-based fluid flow analyses; 1. CNN metamodeling and 2. CNN autoencoder. For the first type of CNN with additional scalar inputs, which is one of the common forms of CNN for fluid flow analysis, we investigate the influence of input placements in the CNN training pipeline. As an example, estimation of force coefficients of an inclined flat plate and two side-by-side cylinders in laminar flows is considered. We find that care should be taken for the placement of additional scalar inputs depending on the problem setting and the complexity of flows that users handle. We then discuss the influence of various parameters and operations on the CNN performance, with the utilization of autoencoder (AE). A two-dimensional turbulence is considered for the demonstration of AE. The results of AE highly rely on the decaying nature. Investigation on the influence of padding operation at a convolutional layer is also performed. The zero padding shows reasonable ability compared to other methods which account for the boundary conditions assumed in the numerical data. Moreover, the effect of the dimensional reduction/extension methods inside CNN is also examined. The CNN model is robust against the difference in dimension reduction operations, while it is sensitive to the dimensional extension methods. The findings of this paper will help us better design a CNN architecture for practical fluid flow analysis",
            "authors": "Masaki Morimoto;Kai Fukami;Kai Zhang;Aditya G. Nair;Koji Fukagata",
            "citationCount": -1,
            "id": "10.1007/s00162-021-00580-0",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/2101.02535v2",
            "publicationDate": "2021-01-07T13:40:51Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.1417280166113168,
            "title": "Convolutional neural networks for fluid flow analysis: toward effective   metamodeling and low-dimensionalization",
            "topics": [
                {
                    "affinity": 0.9933570623397827,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Recently Convolution Neural Network (CNN) has become very popular in AI applications. CNN requires a lot of computational resources that has significant impact on chip size. In this paper we proposed a folding architecture to improve area efficiency of CNN chip implementation. Using the folding architecture, we can significantly reduce the chip size depending on resource availability. The proposed method exploits the resource sharing within CNN algorithm. The scalability of the design is also maintained. Therefore, similar approach can be easily applied into many different CNN image and filter size. The architecture is also easily implemented using RTL design. The result shows the comparison between speed and size using many different combinations of architecture.",
            "authors": "Mukhtar Amin;Trio Adiono",
            "citationCount": 1,
            "id": "10.1109/ICEEI47359.2019.8988879",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8988879",
            "publicationDate": "9-10 July 2019",
            "source": [
                "ieee"
            ],
            "tfidf": 0.14160334225109866,
            "title": "Area Optimized CNN Architecture Using Folding Approach",
            "topics": [
                {
                    "affinity": 0.20666438341140747,
                    "id": 0
                },
                {
                    "affinity": 0.5944034457206726,
                    "id": 1
                },
                {
                    "affinity": 0.19893214106559753,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Vision Transformers (ViT) have recently emerged as a powerful alternative to convolutional networks (CNNs). Although hybrid models attempt to bridge the gap between these two architectures, the self-attention layers they rely on induce a strong computational bottleneck, especially at large spatial resolutions. In this work, we explore the idea of reducing the time spent training these layers by initializing them as convolutional layers. This enables us to transition smoothly from any pre-trained CNN to its functionally identical hybrid model, called Transformed CNN (T-CNN). With only 50 epochs of fine-tuning, the resulting T-CNNs demonstrate significant performance gains over the CNN (+2.2% top-1 on ImageNet-1k for a ResNet50-RS) as well as substantially improved robustness (+11% top-1 on ImageNet-C). We analyze the representations learnt by the T-CNN, providing deeper insights into the fruitful interplay between convolutions and self-attention. Finally, we experiment initializing the T-CNN from a partially trained CNN, and find that it reaches better performance than the corresponding hybrid model trained from scratch, while reducing training time.",
            "authors": "St\u00e9phane d'Ascoli;Levent Sagun;Giulio Biroli;Ari Morcos",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/2106.05795v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/2106.05795v1",
            "publicationDate": "2021-06-10T14:56:10Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.14118195976151035,
            "title": "Transformed CNNs: recasting pre-trained convolutional layers with   self-attention",
            "topics": [
                {
                    "affinity": 0.6362799406051636,
                    "id": 0
                },
                {
                    "affinity": 0.3596336543560028,
                    "id": 1
                }
            ]
        },
        {
            "abstract": "Specialized accelerators have become prevalent in many mobile computing platforms for their ability to perform certain tasks, such as image processing, at a lower power cost than a generalized CPU or GPU. In this article, the authors focus on using cellular neural networks (CNNs) as a specialized accelerator. CNN is a neural computing paradigm that is well suited for image processing applications. However, hardware implementations were originally developed to handle only relatively small image sizes. The authors propose SP-CNN, an architecture and a multiplexing algorithm that provides scalability to CNN applications. The authors demonstrate the proposed multiplexing algorithms over a set of six image processing benchmarks and present a performance analysis of SP-CNN.",
            "authors": "Dilan Manatunga;Hyesoon Kim;Saibal Mukhopadhyay",
            "citationCount": 13,
            "id": "10.1109/MM.2015.121",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7310929",
            "publicationDate": "Sept.-Oct. 2015",
            "source": [
                "ieee"
            ],
            "tfidf": 0.14056754779962463,
            "title": "SP-CNN: A Scalable and Programmable CNN-Based Accelerator",
            "topics": [
                {
                    "affinity": 0.9125584959983826,
                    "id": 0
                }
            ]
        },
        {
            "abstract": "Deep learning techniques have become prominent in modern fault diagnosis for complex processes. In particular, convolutional neural networks (CNNs) have shown an appealing capacity to deal with multivariate time-series data by converting them into images. However, existing CNN techniques mainly focus on capturing local or multi-scale features from input images. A deep CNN is often required to indirectly extract global features, which are critical to describe the images converted from multivariate dynamical data. This paper proposes a novel local-global CNN (LG-CNN) architecture that directly accounts for both local and global features for fault diagnosis. Specifically, the local features are acquired by traditional local kernels whereas global features are extracted by using 1D tall and fat kernels that span the entire height and width of the image. Both local and global features are then merged for classification using fully-connected layers. The proposed LG-CNN is validated on the benchmark Tennessee Eastman process (TEP) dataset. Comparison with traditional CNN shows that the proposed LG-CNN can greatly improve the fault diagnosis performance without significantly increasing the model complexity. This is attributed to the much wider local receptive field created by the LG-CNN than that by CNN. The proposed LG-CNN architecture can be easily extended to other image processing and computer vision tasks.",
            "authors": "Saif S. S. Al-Wahaibi;Qiugang Lu",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/2210.01077v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/2210.01077v1",
            "publicationDate": "2022-10-03T16:49:16Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.14047829229477093,
            "title": "Improving Convolutional Neural Networks for Fault Diagnosis by   Assimilating Global Features",
            "topics": [
                {
                    "affinity": 0.5121843218803406,
                    "id": 0
                },
                {
                    "affinity": 0.4848674237728119,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Convolutional neural networks (CNN) have achieved great success in the optical image processing field. Hence, methods based on CNN are introduced into PolSAR image classification. Usually CNN needs a lot of training samples, but the cost of collecting ground truth data and making labels is very high. Our goal is to increase training samples by repeating learning processes with small sample learning technique. The proposed method used in this study is CNN and conditional random fields(CRF), which combines the structured modeling ability of CRF and the feature extraction advantage of CNN. On base of CNN and CRF, the framework of small sample learning is developed. The experimental data are two AIRSAR datasets. The paper will analyze the appropriate ratio of samples for small sample learning in the whole dataset. The results show that for these two data sets, when the ratio is 0.5%, small sample learning can achieve very high classification accuracy. It is similar to the accuracy of other methods which need at least 3% samples for training.",
            "authors": "Shuai Zhang;Qiang Yin;Jun Ni;Fan Zhang",
            "citationCount": 1,
            "id": "10.1109/APSAR46974.2019.9048266",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9048266",
            "publicationDate": "26-29 Nov. 2019",
            "source": [
                "ieee"
            ],
            "tfidf": 0.13949990766354115,
            "title": "PolSAR image classification with small sample learning based on CNN and CRF",
            "topics": [
                {
                    "affinity": 0.9915387630462646,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Software requirement analysis plays a vital role in Software Development Life Cycle (SDLC). Users requests are transformed into structured software requirements. It is required to know the class of requirements that each request belongs to. Manual classification of these requirements is time consuming . In this work, Convolutional Neural Network (CNN) model is proposed to classify software requirements into functional and non functional. The performance of CNN is affected by model architecture, embedding input word vectors, filter region size and number of filters. In this work, Binary particle swarm optimization (BPSO) is used to optimize the above parameters of CNN (CNN-BPSO) to improve the performance of CNN for software requirements classification. The proposed model is evaluated on PROMISE corpus data set which contains a set of functional and non-functional requirements. The experimental results of proposed CNN-BPSO model is able to provide better prediction accuracy than CNN model.",
            "authors": "Manjubala Bisi;Kirti Keskar",
            "citationCount": 0,
            "id": "10.1109/INDICON49873.2020.9342381",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9342381",
            "publicationDate": "10-13 Dec. 2020",
            "source": [
                "ieee"
            ],
            "tfidf": 0.13935475794124857,
            "title": "CNN-BPSO approach to Select Optimal Values of CNN Parameters for Software Requirements Classification",
            "topics": [
                {
                    "affinity": 0.9918084740638733,
                    "id": 0
                }
            ]
        },
        {
            "abstract": "This paper investigates different trade-offs between the number of model parameters and enhanced speech qualities by employing several deep tensor-to-vector regression models for speech enhancement. We find that a hybrid architecture, namely CNN-TT, is capable of maintaining a good quality performance with a reduced model parameter size. CNN-TT is composed of several convolutional layers at the bottom for feature extraction to improve speech quality and a tensor-train (TT) output layer on the top to reduce model parameters. We first derive a new upper bound on the generalization power of the convolutional neural network (CNN) based vector-to-vector regression models. Then, we provide experimental evidence on the Edinburgh noisy speech corpus to demonstrate that, in single-channel speech enhancement, CNN outperforms DNN at the expense of a small increment of model sizes. Besides, CNN-TT slightly outperforms the CNN counterpart by utilizing only 32 % of the CNN model parameters. Besides, further performance improvement can be attained if the number of CNN-TT parameters is increased to 44 % of the CNN model size. Finally, our experiments of multi-channel speech enhancement on a simulated noisy WSJ0 corpus demonstrate that our proposed hybrid CNN-TT architecture achieves better results than both DNN and CNN models in terms of better-enhanced speech qualities and smaller parameter sizes.",
            "authors": "Jun Qi;Hu Hu;Yannan Wang;Chao-Han Huck Yang;Sabato Marco Siniscalchi;Chin-Hui Lee",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/2007.13024v2",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/2007.13024v2",
            "publicationDate": "2020-07-25T22:21:05Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.13825687672378373,
            "title": "Exploring Deep Hybrid Tensor-to-Vector Network Architectures for   Regression Based Speech Enhancement",
            "topics": [
                {
                    "affinity": 0.7524347901344299,
                    "id": 0
                },
                {
                    "affinity": 0.24460572004318237,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "With the great advancements of convolution neural networks(CNN), CNN accelerators are increasingly developed and deployed in the major computing systems.To make use of the CNN accelerators, CNN models are trained via the off-line training systems such as Caffe, Pytorch and Tensorflow on multi-core CPUs and GPUs first and then compiled to the target accelerators. Although the two-step process seems to be natural and has been widely applied, it assumes that the accelerators' behavior can be fully modeled on CPUs and GPUs. This does not hold true and the behavior of the CNN accelerators is un-deterministic when the circuit works at 'unstable' mode when it is overclocked or is affected by the environment like fault-prone aerospace. The exact behaviors of the accelerators are determined by both the chip fabrication and the working environment or status. In this case, applying the conventional off-line training result to the accelerators directly may lead to considerable accuracy loss.   To address this problem, we propose to train for the 'unstable' CNN accelerator and have the 'un-determined behavior' learned together with the data in the same framework. Basically, it starts from the off-line trained model and then integrates the uncertain circuit behaviors into the CNN models through additional accelerator-specific training. The fine-tuned training makes the CNN models less sensitive to the circuit uncertainty. We apply the design method to both an overclocked CNN accelerator and a faulty accelerator. According to our experiments on a subset of ImageNet, the accelerator-specific training can improve the top 5 accuracy up to 3.4% and 2.4% on average when the CNN accelerator is at extreme overclocking. When the accelerator is exposed to a faulty environment, the top 5 accuracy improves up to 6.8% and 4.28% on average under the most severe fault injection.",
            "authors": "KouZi Xing",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1812.01689v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1812.01689v1",
            "publicationDate": "2018-12-02T11:45:07Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.1381933439077886,
            "title": "Training for 'Unstable' CNN Accelerator:A Case Study on FPGA",
            "topics": [
                {
                    "affinity": 0.18412934243679047,
                    "id": 1
                },
                {
                    "affinity": 0.8120238780975342,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "The convolutional neural network (CNN), which is one of the deep learning models, has seen much success in a variety of computer vision tasks. However, designing CNN architectures still requires expert knowledge and a lot of trial and error. In this paper, we attempt to automatically construct CNN architectures for an image classification task based on Cartesian genetic programming (CGP). In our method, we adopt highly functional modules, such as convolutional blocks and tensor concatenation, as the node functions in CGP. The CNN structure and connectivity represented by the CGP encoding method are optimized to maximize the validation accuracy. To evaluate the proposed method, we constructed a CNN architecture for the image classification task with the CIFAR-10 dataset. The experimental result shows that the proposed method can be used to automatically find the competitive CNN architecture compared with state-of-the-art models.",
            "authors": "Masanori Suganuma;Shinichi Shirakawa;Tomoharu Nagao",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1704.00764v2",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1704.00764v2",
            "publicationDate": "2017-04-03T19:06:16Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.13750024785905576,
            "title": "A Genetic Programming Approach to Designing Convolutional Neural Network   Architectures",
            "topics": [
                {
                    "affinity": 0.9902464151382446,
                    "id": 0
                }
            ]
        },
        {
            "abstract": "Convolutional Neural Networks (CNNs) are powerful models that achieve impressive results for image classification. In addition, pre-trained CNNs are also useful for other computer vision tasks as generic feature extractors. This paper aims to gain insight into the feature aspect of CNN and demonstrate other uses of CNN features. Our results show that CNN feature maps can be used with Random Forests and SVM to yield classification results that outperforms the original CNN. A CNN that is less than optimal (e.g. not fully trained or overfitting) can also extract features for Random Forest/SVM that yield competitive classification accuracy. In contrast to the literature which uses the top-layer activations as feature representation of images for other tasks, using lower-layer features can yield better results for classification.",
            "authors": "Ben Athiwaratkun;Keegan Kang",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1507.02313v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1507.02313v1",
            "publicationDate": "2015-07-08T21:13:26Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.13692527320037523,
            "title": "Feature Representation in Convolutional Neural Networks",
            "topics": [
                {
                    "affinity": 0.9252849221229553,
                    "id": 0
                }
            ]
        },
        {
            "abstract": "Computer vision performances have been significantly improved in recent years by Convolutional Neural Networks(CNN). Currently, applications using CNN algorithms are deployed mainly on general purpose hardwares, such as CPUs, GPUs or FPGAs. However, power consumption, speed, accuracy, memory footprint, and die size should all be taken into consideration for mobile and embedded applications. Domain Specific Architecture (DSA) for CNN is the efficient and practical solution for CNN deployment and implementation. We designed and produced a 28nm Two-Dimensional CNN-DSA accelerator with an ultra power-efficient performance of 9.3TOPS/Watt and with all processing done in the internal memory instead of outside DRAM. It classifies 224x224 RGB image inputs at more than 140fps with peak power consumption at less than 300mW and an accuracy comparable to the VGG benchmark. The CNN-DSA accelerator is reconfigurable to support CNN model coefficients of various layer sizes and layer types, including convolution, depth-wise convolution, short-cut connections, max pooling, and ReLU. Furthermore, in order to better support real-world deployment for various application scenarios, especially with low-end mobile and embedded platforms and MCUs (Microcontroller Units), we also designed algorithms to fully utilize the CNN-DSA accelerator efficiently by reducing the dependency on external accelerator computation resources, including implementation of Fully-Connected (FC) layers within the accelerator and compression of extracted features from the CNN-DSA accelerator. Live demos with our CNN-DSA accelerator on mobile and embedded systems show its capabilities to be widely and practically applied in the real world.",
            "authors": "Baohua Sun;Lin Yang;Patrick Dong;Wenhan Zhang;Jason Dong;Charles Young",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1805.00361v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1805.00361v1",
            "publicationDate": "2018-04-30T17:36:14Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.13687683587631305,
            "title": "Ultra Power-Efficient CNN Domain Specific Accelerator with 9.3TOPS/Watt   for Mobile and Embedded Applications",
            "topics": [
                {
                    "affinity": 0.19430507719516754,
                    "id": 0
                },
                {
                    "affinity": 0.5661119818687439,
                    "id": 1
                },
                {
                    "affinity": 0.23958291113376617,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "In recent years, convolutional neural networks (CNNs) have shown great performance in various fields such as image classification, pattern recognition, and multi-media compression. Two of the feature properties, local connectivity and weight sharing, can reduce the number of parameters and increase processing speed during training and inference. However, as the dimension of data becomes higher and the CNN architecture becomes more complicated, the end-to-end approach or the combined manner of CNN is computationally intensive, which becomes limitation to CNN's further implementation. Therefore, it is necessary and urgent to implement CNN in a faster way. In this paper, we first summarize the acceleration methods that contribute to but not limited to CNN by reviewing a broad variety of research papers. We propose a taxonomy in terms of three levels, i.e.~structure level, algorithm level, and implementation level, for acceleration methods. We also analyze the acceleration methods in terms of CNN architecture compression, algorithm optimization, and hardware-based improvement. At last, we give a discussion on different perspectives of these acceleration and optimization methods within each level. The discussion shows that the methods in each level still have large exploration space. By incorporating such a wide range of disciplines, we expect to provide a comprehensive reference for researchers who are interested in CNN acceleration.",
            "authors": "Qianru Zhang;Meng Zhang;Tinghuan Chen;Zhifei Sun;Yuzhe Ma;Bei Yu",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1807.08596v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1807.08596v1",
            "publicationDate": "2018-07-23T13:25:46Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.13653448401582785,
            "title": "Recent Advances in Convolutional Neural Network Acceleration",
            "topics": [
                {
                    "affinity": 0.9927920699119568,
                    "id": 0
                }
            ]
        },
        {
            "abstract": "Although many studies suggest high performance hand detection methods, those methods are likely to be overfitting. Fortunately, the Convolution Neural Network (CNN) based approach provides a better way that is less sensitive to translation and hand poses. However the CNN approach is complex and can increase computational time, which at the end reduce its effectiveness on a system where the speed is essential.In this study we propose a shallow CNN network which is fast, and insensitive to translation and hand poses. It is tested on two different domains of hand datasets, and performs in relatively comparable performance and faster than the other state-of-the-art hand CNN-based hand detection method. Our evaluation shows that the proposed shallow CNN network performs at 93.9% accuracy and reaches much faster speed than its competitors.",
            "authors": "Richard Adiguna;Yustinus Eko Soelistio",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1809.10432v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1809.10432v1",
            "publicationDate": "2018-09-27T10:00:31Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.136492530247591,
            "title": "CNN Based Posture-Free Hand Detection",
            "topics": [
                {
                    "affinity": 0.9061797857284546,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Deep learning, in particular Convolutional Neural Network (CNN), has achieved promising results in face recognition recently. However, it remains an open question: why CNNs work well and how to design a 'good' architecture. The existing works tend to focus on reporting CNN architectures that work well for face recognition rather than investigate the reason. In this work, we conduct an extensive evaluation of CNN-based face recognition systems (CNN-FRS) on a common ground to make our work easily reproducible. Specifically, we use public database LFW (Labeled Faces in the Wild) to train CNNs, unlike most existing CNNs trained on private databases. We propose three CNN architectures which are the first reported architectures trained using LFW data. This paper quantitatively compares the architectures of CNNs and evaluate the effect of different implementation choices. We identify several useful properties of CNN-FRS. For instance, the dimensionality of the learned features can be significantly reduced without adverse effect on face recognition accuracy. In addition, traditional metric learning method exploiting CNN-learned features is evaluated. Experiments show two crucial factors to good CNN-FRS performance are the fusion of multiple CNNs and metric learning. To make our work reproducible, source code and models will be made publicly available.",
            "authors": "Guosheng Hu;Yongxin Yang;Dong Yi;Josef Kittler;William Christmas;Stan Z. Li;Timothy Hospedales",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1504.02351v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1504.02351v1",
            "publicationDate": "2015-04-09T15:27:49Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.13635883296667062,
            "title": "When Face Recognition Meets with Deep Learning: an Evaluation of   Convolutional Neural Networks for Face Recognition",
            "topics": [
                {
                    "affinity": 0.9660798907279968,
                    "id": 1
                }
            ]
        },
        {
            "abstract": "Deep convolutional neural networks (CNNs) have had a major impact in most areas of image understanding, including object category detection. In object detection, methods such as R-CNN have obtained excellent results by integrating CNNs with region proposal generation algorithms such as selective search. In this paper, we investigate the role of proposal generation in CNN-based detectors in order to determine whether it is a necessary modelling component, carrying essential geometric information not contained in the CNN, or whether it is merely a way of accelerating detection. We do so by designing and evaluating a detector that uses a trivial region generation scheme, constant for each image. Combined with SPP, this results in an excellent and fast detector that does not require to process an image with algorithms other than the CNN itself. We also streamline and simplify the training of CNN-based detectors by integrating several learning steps in a single algorithm, as well as by proposing a number of improvements that accelerate detection.",
            "authors": "Karel Lenc;Andrea Vedaldi",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1506.06981v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1506.06981v1",
            "publicationDate": "2015-06-23T13:26:36Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.1351479059267039,
            "title": "R-CNN minus R",
            "topics": [
                {
                    "affinity": 0.9908978343009949,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "This paper investigates how working of Convolutional Neural Network (CNN) can be explained through visualization in the context of machine perception of autonomous vehicles. We visualize what type of features are extracted in different convolution layers of CNN that helps to understand how CNN gradually increases spatial information in every layer. Thus, it concentrates on region of interests in every transformation. Visualizing heat map of activation helps us to understand how CNN classifies and localizes different objects in image. This study also helps us to reason behind low accuracy of a model helps to increase trust on object detection module.",
            "authors": "Abhishek Mukhopadhyay;Imon Mukherjee;Pradipta Biswas",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/2007.07482v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/2007.07482v1",
            "publicationDate": "2020-07-15T05:01:27Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.13483653672426552,
            "title": "Decoding CNN based Object Classifier Using Visualization",
            "topics": [
                {
                    "affinity": 0.39948150515556335,
                    "id": 0
                },
                {
                    "affinity": 0.5936393737792969,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Applying Compressed Sensing to Magnetic Resonance Image Reconstruction using CNN (CNN-CS) has attracted much attention. CNN-CS enables us to reconstruct an image quickly with better quality than traditional mathematical iterative Compressed Sensing methods (CS-MRI). While many CNN-CS methods have been proposed, they have problems recovering high frequency components and removing aliasing artifacts. In this study, we proposed a novel Transformed Image Domain CNN-CS using multi-channel grouped CNN-based image reconstruction using the Fresnel transform (eFRBAS transform). Experimental results showed that the proposed method was able to predict an artifact-free image better than other methods, especially for a 20-30% low sampling rate.",
            "authors": "Shohei Ouchi;Satoshi Ito",
            "citationCount": 0,
            "id": "10.1109/ISBI48211.2021.9434166",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9434166",
            "publicationDate": "13-16 April 2021",
            "source": [
                "ieee"
            ],
            "tfidf": 0.1341189916184884,
            "title": "Multi-Channel Grouped CNN-Based Image Reconstruction For Reduced Sampled Complex MR Signal",
            "topics": [
                {
                    "affinity": 0.1582346111536026,
                    "id": 0
                },
                {
                    "affinity": 0.8366895914077759,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "This paper proposes a principled information theoretic analysis of classification for deep neural network structures, e.g. convolutional neural networks (CNN). The output of convolutional filters is modeled as a random variable Y conditioned on the object class C and network filter bank F. The conditional entropy (CENT) H(Y |C,F) is shown in theory and experiments to be a highly compact and class-informative code, that can be computed from the filter outputs throughout an existing CNN and used to obtain higher classification results than the original CNN itself. Experiments demonstrate the effectiveness of CENT feature analysis in two separate CNN classification contexts. 1) In the classification of neurodegeneration due to Alzheimer's disease (AD) and natural aging from 3D magnetic resonance image (MRI) volumes, 3 CENT features result in an AUC=94.6% for whole-brain AD classification, the highest reported accuracy on the public OASIS dataset used and 12% higher than the softmax output of the original CNN trained for the task. 2) In the context of visual object classification from 2D photographs, transfer learning based on a small set of CENT features identified throughout an existing CNN leads to AUC values comparable to the 1000-feature softmax output of the original network when classifying previously unseen object categories. The general information theoretical analysis explains various recent CNN design successes, e.g. densely connected CNN architectures, and provides insights for future research directions in deep learning.",
            "authors": "Ahmad Chaddad;Behnaz Naisiri;Marco Pedersoli;Eric Granger;Christian Desrosiers;Matthew Toews",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1712.00003v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1712.00003v1",
            "publicationDate": "2017-11-29T23:09:58Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.13397353717415011,
            "title": "Modeling Information Flow Through Deep Neural Networks",
            "topics": [
                {
                    "affinity": 0.901515781879425,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "At the moment, the world lives in a pandemic situation of COVID -19 and related variants, driving urgent needs for expanded assessments. A complementary support of related healthcare can be based on an intelligent system that can diagnose early onset of respiratory disorders. The convolutional neural networks (CNN) were implemented utilizing image data, reflecting bidimensional signals. Specifically, CNN has shown to be powerful tool in the context of cardiopulmonary sounds evaluation. The configurations of CNN contain convolutional layers to extract feature maps and fully connected layers to classify indicators of interest. Even though, learning algorithms use parameters like learning rate which can determine and attain CNN configuration less complex, with excellent results as reflected in the experiments we carried out, and which focused on achieved configuration of CNN with excellent results classifying heart sounds (HS) and lung sounds (LS).",
            "authors": "P. Mayorga;J. A. Valdez;C. Druzgalski;V. Zeljkovi\u0107;L. A. Quintero L\u00f3pez",
            "citationCount": 0,
            "id": "10.1109/GMEPE/PAHCE55115.2022.9757779",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9757779",
            "publicationDate": "21-26 March 2022",
            "source": [
                "ieee"
            ],
            "tfidf": 0.13372953854212294,
            "title": "CNN Networks to Classify Cardiopulmonary Signals Redes CNN en Clasificaci\u00f3n de Se\u00f1ales Cardiopulmonares",
            "topics": [
                {
                    "affinity": 0.9907954931259155,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "We present an approach to utilize large amounts of web data for learning CNNs. Specifically inspired by curriculum learning, we present a two-step approach for CNN training. First, we use easy images to train an initial visual representation. We then use this initial CNN and adapt it to harder, more realistic images by leveraging the structure of data and categories. We demonstrate that our two-stage CNN outperforms a fine-tuned CNN trained on ImageNet on Pascal VOC 2012. We also demonstrate the strength of webly supervised learning by localizing objects in web images and training a R-CNN style detector. It achieves the best performance on VOC 2007 where no VOC training data is used. Finally, we show our approach is quite robust to noise and performs comparably even when we use image search results from March 2013 (pre-CNN image search era).",
            "authors": "Xinlei Chen;Abhinav Gupta",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1505.01554v2",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1505.01554v2",
            "publicationDate": "2015-05-07T00:56:15Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.13327669584264096,
            "title": "Webly Supervised Learning of Convolutional Networks",
            "topics": [
                {
                    "affinity": 0.9902191758155823,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Convolutional neural network (CNN) is one of the most frequently used deep learning techniques. Various forms of models have been proposed and improved for learning at CNN. When learning with CNN, it is necessary to determine the optimal hyperparameters. However, the number of hyperparameters is so large that it is difficult to do it manually, so much research has been done on automation. A method that uses metaheuristic algorithms is attracting attention in research on hyperparameter optimization. Metaheuristic algorithms are naturally inspired and include evolution strategies, genetic algorithms, antcolony optimization and particle swarm optimization. In particular, particle swarm optimization converges faster than genetic algorithms, and various models have been proposed. In this paper, we propose CNN hyperparameter optimization with linearly decreasing weight particle swarm optimization (LDWPSO). In the experiment, the MNIST data set and CIFAR-10 data set, which are often used as benchmark data sets, are used. By optimizing CNN hyperparameters with LDWPSO, learning the MNIST and CIFAR-10 datasets, we compare the accuracy with a standard CNN based on LeNet-5. As a result, when using the MNIST dataset, the baseline CNN is 94.02% at the 5th epoch, compared to 98.95% for LDWPSO CNN, which improves accuracy. When using the CIFAR-10 dataset, the Baseline CNN is 28.07% at the 10th epoch, compared to 69.37% for the LDWPSO CNN, which greatly improves accuracy.",
            "authors": "T. Serizawa;H. Fujita",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/2001.05670v2",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/2001.05670v2",
            "publicationDate": "2020-01-16T06:27:50Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.1330074780072886,
            "title": "Optimization of Convolutional Neural Network Using the Linearly   Decreasing Weight Particle Swarm Optimization",
            "topics": [
                {
                    "affinity": 0.9934828281402588,
                    "id": 0
                }
            ]
        },
        {
            "abstract": "In this paper, we designed a new hardware architecture that uses non-blocking network for accelerating the convolutional neural network (CNN). Unlike many other CNN accelerator which only capable of supporting a specific type of network model, by making use of the rearrangeability of non-blocking network, we can provide high flexibility and high parallelism. We successfully implemented our CNN accelerator on Xilinx Virtex UltraScale+ FPGA VCU128 Evaluation Kit and evaluated it by running CNN model, LeNet-5.",
            "authors": "Chun Yan Lo;Longyu Ma;Chiu-Wing Sham",
            "citationCount": 0,
            "id": "10.1109/GCCE53005.2021.9622107",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9622107",
            "publicationDate": "12-15 Oct. 2021",
            "source": [
                "ieee"
            ],
            "tfidf": 0.13291995058948128,
            "title": "CNN Accelerator with Non-Blocking Network Design",
            "topics": [
                {
                    "affinity": 0.8097413182258606,
                    "id": 0
                },
                {
                    "affinity": 0.1803518384695053,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Stock price prediction predicts the future trend of stocks using the previous data, which has been widely focused on. Previous works aim to use either CNN or LSTM to predict the price, and few works focus on discussing the strength and weaknesses of CNN and LSTM in stock prediction tasks. In this paper, we aim to compare both CNN and LSTM on the stock price prediction problem. We first exploit the advantages and disadvantages of CNN and LSTM. Then, we propose a combined LSTM-CNN model to achieve a better performance, which avoids the layback of LSTM and increase the robustness of CNN. Our LSTM-CNN model can provide an accurate prediction and reliable attempt to combine CNN and LSTM on the stock price prediction.",
            "authors": "Xinrong Zhou",
            "citationCount": 1,
            "id": "10.1109/MLBDBI54094.2021.00020",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9730948",
            "publicationDate": "3-5 Dec. 2021",
            "source": [
                "ieee"
            ],
            "tfidf": 0.13276533698027443,
            "title": "Stock Price Prediction using Combined LSTM-CNN Model",
            "topics": [
                {
                    "affinity": 0.9890117049217224,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Improving the performance of CNN-based mobile applications by offloading its computation from mobile devices to the cloud has attracted the attention of the community. Generally, there are three stages in the workflow, including local inference on the mobile device, data transmission of the intermediate result, and remote inference in the cloud. However, the time cost of local inference and data transmission are still the bottleneck in reaching the desirable inference performance. In this paper, we propose an image-aware inference framework called IF-CNN to enable fast inference based on computation offloading. In the framework, we first build a model pool consisting of CNN models with different complexities. The most efficient one from such candidate models is selected to process the corresponding image. During the selection process, we have designed an effective model to predict the confidence based on multi-task learning. After model selection, half-floating optimization and feature compression are applied to accelerate the process of distributed inference between mobile devices and cloud. Experimental results show that IF-CNN is credible to identify the most effective model for different images and the total inference performance could be significantly improved. Meanwhile, IF-CNN is complementary to other inference acceleration methods of CNN models.",
            "authors": "Guansheng Shu;Weiqing Liu;Xiaojie Zheng;Jing Li",
            "citationCount": 10,
            "id": "10.1109/ACCESS.2018.2880196",
            "openaccess": 1,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8529186",
            "publicationDate": "2018",
            "source": [
                "ieee"
            ],
            "tfidf": 0.13243360231226747,
            "title": "IF-CNN: Image-Aware Inference Framework for CNN With the Collaboration of Mobile Devices and Cloud",
            "topics": [
                {
                    "affinity": 0.6661834716796875,
                    "id": 0
                },
                {
                    "affinity": 0.20165805518627167,
                    "id": 1
                },
                {
                    "affinity": 0.13215845823287964,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "We present a novel method called Contextual Pyramid CNN (CP-CNN) for generating high-quality crowd density and count estimation by explicitly incorporating global and local contextual information of crowd images. The proposed CP-CNN consists of four modules: Global Context Estimator (GCE), Local Context Estimator (LCE), Density Map Estimator (DME) and a Fusion-CNN (F-CNN). GCE is a VGG-16 based CNN that encodes global context and it is trained to classify input images into different density classes, whereas LCE is another CNN that encodes local context information and it is trained to perform patch-wise classification of input images into different density classes. DME is a multi-column architecture-based CNN that aims to generate high-dimensional feature maps from the input image which are fused with the contextual information estimated by GCE and LCE using F-CNN. To generate high resolution and high-quality density maps, F-CNN uses a set of convolutional and fractionally-strided convolutional layers and it is trained along with the DME in an end-to-end fashion using a combination of adversarial loss and pixel-level Euclidean loss. Extensive experiments on highly challenging datasets show that the proposed method achieves significant improvements over the state-of-the-art methods.",
            "authors": "Vishwanath A. Sindagi;Vishal M. Patel",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1708.00953v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1708.00953v1",
            "publicationDate": "2017-08-02T22:54:21Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.13204722676403385,
            "title": "Generating High-Quality Crowd Density Maps using Contextual Pyramid CNNs",
            "topics": [
                {
                    "affinity": 0.9644386172294617,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "In this paper, we propose a convolutional neural network (CNN)-based rate-distortion (R-D) modeling method for H.265/HEVC. A fully convolutional neural network (CNN) is designed to learn end-to-end, pixels-to-pixels mappings from the original images to the structural similarity (SSIM) maps indicating distortion. The rate information is predicted through a CNN with fully connected layers as well. When compared to traditional CNN methods, the proposed mappings to the distortion or rate information. The experiments demonstrate the feasibility of our CNN-based framework for rate-distortion modeling.",
            "authors": "Bin Xu;Xiang Pan;Yan Zhou;Yiming Li;Daiqin Yang;Zhenzhong Chen",
            "citationCount": 14,
            "id": "10.1109/VCIP.2017.8305151",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8305151",
            "publicationDate": "10-13 Dec. 2017",
            "source": [
                "ieee"
            ],
            "tfidf": 0.13204003836876585,
            "title": "CNN-based rate-distortion modeling for H.265/HEVC",
            "topics": [
                {
                    "affinity": 0.8236345052719116,
                    "id": 0
                },
                {
                    "affinity": 0.1654413938522339,
                    "id": 1
                }
            ]
        },
        {
            "abstract": "In this paper we introduce a novel method for general semantic segmentation that can benefit from general semantics of Convolutional Neural Network (CNN). Our segmentation proposes visually and semantically coherent image segments. We use binary encoding of CNN features to overcome the difficulty of the clustering on the high-dimensional CNN feature space. These binary codes are very robust against noise and non-semantic changes in the image. These binary encoding can be embedded into the CNN as an extra layer at the end of the network. This results in real-time segmentation. To the best of our knowledge our method is the first attempt on general semantic image segmentation using CNN. All the previous papers were limited to few number of category of the images (e.g. PASCAL VOC). Experiments show that our segmentation algorithm outperform the state-of-the-art non-semantic segmentation methods by large margin.",
            "authors": "Mahdyar Ravanbakhsh;Hossein Mousavi;Moin Nabi;Mohammad Rastegari;Carlo Regazzoni",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1609.09220v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1609.09220v1",
            "publicationDate": "2016-09-29T06:46:27Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.1319076000435059,
            "title": "CNN-aware Binary Map for General Semantic Segmentation",
            "topics": [
                {
                    "affinity": 0.9729910492897034,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Recent advancement in neuromorphic devices has created new opportunities to develop and deploy the algorithms which mimic human-like information processing. Spiking neural networks (SNNs) are considered as third generation neural networks that produce sparse and asynchronous representation of spikes similar to the processing of the primate brain. SNNs offer advantages of low power requirement, event driven processing and fast inference compared to the state-of-the art second generation artificial neural networks. In order to understand the performance comparison of spiking Convolutional Neural Networks (Spiking-CNN), we adopted the population time encoding technique with at most one spike at a time on well-known CNN architecture. The network is trained using spike time dependent plasticity (STDP) and reward-modulated STDP learning rules with the use of SpykeTorch library in python. The simulation results considering image classification task on MNIST dataset indicate the comparable performance of the spiking CNN with traditional CNN architecture. The performance of the adopted spiking-CNN shows almost 97% accuracy on the selected dataset. The study suggests the need for further development of methodologies which can outperform the traditional CNN using spiking-CNN.",
            "authors": "Mandar Kalbande;Punitkumar Bhavsar",
            "citationCount": 0,
            "id": "10.1109/TENSYMP54529.2022.9864551",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9864551",
            "publicationDate": "1-3 July 2022",
            "source": [
                "ieee"
            ],
            "tfidf": 0.13188562893866315,
            "title": "Performance Comparison of Deep Spiking CNN with Artificial Deep CNN for Image Classification Tasks",
            "topics": [
                {
                    "affinity": 0.28252944350242615,
                    "id": 0
                },
                {
                    "affinity": 0.7141740322113037,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "In this paper we introduce a novel method for segmentation that can benefit from general semantics of Convolutional Neural Network (CNN). Our segmentation proposes visually and semantically coherent image segments. We use binary encoding of CNN features to overcome the difficulty of the clustering on the high-dimensional CNN feature space. These binary encoding can be embedded into the CNN as an extra layer at the end of the network. This results in real-time segmentation. To the best of our knowledge our method is the first attempt on general semantic image segmentation using CNN. All the previous papers were limited to few number of category of the images (e.g. PASCAL VOC). Experiments show that our segmentation algorithm outperform the state-of-the-art non-semantic segmentation methods by a large margin.",
            "authors": "Mahdyar Ravanbakhsh;Hossein Mousavi;Moin Nabi;Lucio Marcenaro;Carlo Regazzoni",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1611.06764v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1611.06764v1",
            "publicationDate": "2016-11-21T12:55:30Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.13142678214119954,
            "title": "Efficient Convolutional Neural Network with Binary Quantization Layer",
            "topics": [
                {
                    "affinity": 0.9679906368255615,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "In this paper we consider using memristors to implement the neighborhood connections of a CNN. First the benefits and drawbacks of using memristors as programmable CNN weights are described. Then, an existing memristor model is improved to allow full-scale simulation of the design. The new model is implemented in the SPICE simulation environment and is not restricted to CNN applications. Then, the CNN cell design is presented and simulations describing memristor programming are performed.",
            "authors": "Eero Lehtonen;Mika Laiho",
            "citationCount": 127,
            "id": "10.1109/CNNA.2010.5430304",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5430304",
            "publicationDate": "3-5 Feb. 2010",
            "source": [
                "ieee"
            ],
            "tfidf": 0.13122808908889635,
            "title": "CNN using memristors for neighborhood connections",
            "topics": [
                {
                    "affinity": 0.9792378544807434,
                    "id": 1
                }
            ]
        },
        {
            "abstract": "Convolutional Neural Networks (CNN) has been extensively studied for Hyperspectral Image Classification (HSIC) more specifically, 2D and 3D CNN models have proved highly efficient in exploiting the spatial and spectral information of Hyperspectral Images. However, 2D CNN only considers the spatial information and ignores the spectral information whereas 3D CNN jointly exploits spatial-spectral information at a high computational cost. Therefore, this work proposed a lightweight CNN (3D followed by 2D-CNN) model which significantly reduces the computational cost by distributing spatial-spectral feature extraction across a lighter model alongside a preprocessing that has been carried out to improve the classification results. Five benchmark Hyperspectral datasets (i.e., SalinasA, Salinas, Indian Pines, Pavia University, Pavia Center, and Botswana) are used for experimental evaluation. The experimental results show that the proposed pipeline outperformed in terms of generalization performance, statistical significance, and computational complexity, as compared to the state-of-the-art 2D/3D CNN models except commonly used computationally expensive design choices.",
            "authors": "Muhammad Ahmad;Sidrah Shabbir;Rana Aamir Raza;Manuel Mazzara;Salvatore Distefano;Adil Mehmood Khan",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/2101.10532v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/2101.10532v1",
            "publicationDate": "2021-01-25T18:43:57Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.13071002265403703,
            "title": "Hyperspectral Image Classification: Artifacts of Dimension Reduction on   Hybrid CNN",
            "topics": [
                {
                    "affinity": 0.9919548034667969,
                    "id": 0
                }
            ]
        },
        {
            "abstract": "State-of-the-art image recognition systems use sophisticated Convolutional Neural Networks (CNNs) that are designed and trained to identify numerous object classes. Such networks are fairly resource intensive to compute, prohibiting their deployment on resource-constrained embedded platforms. On one hand, the ability to classify an exhaustive list of categories is excessive for the demands of most IoT applications. On the other hand, designing a new custom-designed CNN for each new IoT application is impractical, due to the inherent difficulty in developing competitive models and time-to-market pressure. To address this problem, we investigate the question of: \"Can one utilize an existing optimized CNN model to automatically build a competitive CNN for an IoT application whose objects of interest are a fraction of categories that the original CNN was designed to classify, such that the resource requirement is proportionally scaled down?\" We use the term resource scalability to refer to this concept, and develop a methodology for automated synthesis of resource scalable CNNs from an existing optimized baseline CNN. The synthesized CNN has sufficient learning capacity for handling the given IoT application requirements, and yields competitive accuracy. The proposed approach is fast, and unlike the presently common practice of CNN design, does not require iterative rounds of training trial and error.",
            "authors": "Mohammad Motamedi;Felix Portillo;Mahya Saffarpour;Daniel Fong;Soheil Ghiasi",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1901.00738v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1901.00738v1",
            "publicationDate": "2018-12-16T01:21:57Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.13044398889236056,
            "title": "Resource-Scalable CNN Synthesis for IoT Applications",
            "topics": [
                {
                    "affinity": 0.9656304121017456,
                    "id": 1
                }
            ]
        },
        {
            "abstract": "The rapid development of Convolutional Neural Networks (CNNs) in recent years has triggered significant breakthroughs in many machine learning (ML) applications. The ability to understand and compare various CNN models available is thus essential. The conventional approach with visualizing each model's quantitative features, such as classification accuracy and computational complexity, is not sufficient for a deeper understanding and comparison of the behaviors of different models. Moreover, most of the existing tools for assessing CNN behaviors only support comparison between two models and lack the flexibility of customizing the analysis tasks according to user needs. This paper presents a visual analytics system, VAC-CNN (Visual Analytics for Comparing CNNs), that supports the in-depth inspection of a single CNN model as well as comparative studies of two or more models. The ability to compare a larger number of (e.g., tens of) models especially distinguishes our system from previous ones. With a carefully designed model visualization and explaining support, VAC-CNN facilitates a highly interactive workflow that promptly presents both quantitative and qualitative information at each analysis stage. We demonstrate VAC-CNN's effectiveness for assisting novice ML practitioners in evaluating and comparing multiple CNN models through two use cases and one preliminary evaluation study using the image classification tasks on the ImageNet dataset.",
            "authors": "Xiwei Xuan;Xiaoyu Zhang;Oh-Hyun Kwon;Kwan-Liu Ma",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/2110.13252v2",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/2110.13252v2",
            "publicationDate": "2021-10-25T20:36:14Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.13003826310174604,
            "title": "VAC-CNN: A Visual Analytics System for Comparative Studies of Deep   Convolutional Neural Networks",
            "topics": [
                {
                    "affinity": 0.8769427537918091,
                    "id": 0
                }
            ]
        },
        {
            "abstract": "Mathematically speaking, a transformationally invariant operator, such as a transformationally identical (TI) matrix kernel (i.e., K= T{K}), commutes with the transformation (T{.}) itself when they operate on the first operand matrix. We found that by consistently applying the same type of TI kernels in a convolutional neural networks (CNN) system, the commutative property holds throughout all layers of convolution processes with and without involving an activation function and/or a 1D convolution across channels within a layer. We further found that any CNN possessing the same TI kernel property for all convolution layers followed by a flatten layer with weight sharing among their transformation corresponding elements would output the same result for all transformation versions of the original input vector. In short, CNN[ Vi ] = CNN[ T{Vi} ] providing every K = T{K} in CNN, where Vi denotes input vector and CNN[.] represents the whole CNN process as a function of input vector that produces an output vector. With such a transformationally identical CNN (TI-CNN) system, each transformation, that is not associated with a predefined TI used in data augmentation, would inherently include all of its corresponding transformation versions of the input vector for the training. Hence the use of same TI property for every kernel in the CNN would serve as an orientation or a translation independent training guide in conjunction with the error-backpropagation during the training. This TI kernel property is desirable for applications requiring a highly consistent output result from corresponding transformation versions of an input. Several C programming routines are provided to facilitate interested parties of using the TI-CNN technique which is expected to produce a better generalization performance than its ordinary CNN counterpart.",
            "authors": "Shih Chung B. Lo;Matthew T. Freedman;Seong K. Mun;Shuo Gu",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1806.03636v3",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1806.03636v3",
            "publicationDate": "2018-06-10T11:16:36Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.12996418984486754,
            "title": "Transformationally Identical and Invariant Convolutional Neural Networks   through Symmetric Element Operators",
            "topics": [
                {
                    "affinity": 0.9175900220870972,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "The Convolutional Neural Network (CNN) has witnessed success and vast opportunities in the field of deep learning based video compression. Many deep learning models have either outperformed or performed on par with state-of-the-art compression standards like H.264 and HEVC. In this paper, we propose H.264 Inter-frame prediction based video compression approach using Temporal 3-D CNN based encoder and Y-style CNN based decoder. The proposed architecture includes three stages, Temporal 3-D CNN encoder for forward Predicted (P) frame computation, H.264 like Integer Discrete Cosine Transform and scalar quantization for entropy coding and Y-style CNN for P-frame decoding. The experiments are conducted with different training loss functions and different datasets. The results show that the proposed model outperforms the state-of-the-art compression standards with low computational complexity.",
            "authors": "Abhishek Kumar Sinha;Deepak Mishra",
            "citationCount": 1,
            "id": "10.1109/ICCCNT49239.2020.9225580",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9225580",
            "publicationDate": "1-3 July 2020",
            "source": [
                "ieee"
            ],
            "tfidf": 0.12972407813592732,
            "title": "T3D-Y Codec: A Video Compression Framework using Temporal 3-D CNN Encoder and Y-Style CNN Decoder",
            "topics": [
                {
                    "affinity": 0.556505024433136,
                    "id": 0
                },
                {
                    "affinity": 0.37643498182296753,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Remote sensing image scene classification is a fundamental but challenging task in understanding remote sensing images. Recently, deep learning-based methods, especially convolutional neural network-based (CNN-based) methods have shown enormous potential to understand remote sensing images. CNN-based methods meet with success by utilizing features learned from data rather than features designed manually. The feature-learning procedure of CNN largely depends on the architecture of CNN. However, most of the architectures of CNN used for remote sensing scene classification are still designed by hand which demands a considerable amount of architecture engineering skills and domain knowledge, and it may not play CNN's maximum potential on a special dataset. In this paper, we proposed an automatically architecture learning procedure for remote sensing scene classification. We designed a parameters space in which every set of parameters represents a certain architecture of CNN (i.e., some parameters represent the type of operators used in the architecture such as convolution, pooling, no connection or identity, and the others represent the way how these operators connect). To discover the optimal set of parameters for a given dataset, we introduced a learning strategy which can allow efficient search in the architecture space by means of gradient descent. An architecture generator finally maps the set of parameters into the CNN used in our experiments.",
            "authors": "Jie Chen;Haozhe Huang;Jian Peng;Jiawei Zhu;Li Chen;Wenbo Li;Binyu Sun;Haifeng Li",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/2001.09614v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/2001.09614v1",
            "publicationDate": "2020-01-27T07:42:46Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.12856620277217384,
            "title": "Convolution Neural Network Architecture Learning for Remote Sensing   Scene Classification",
            "topics": [
                {
                    "affinity": 0.6996707916259766,
                    "id": 0
                },
                {
                    "affinity": 0.2972842752933502,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "This paper proposes a method to modify traditional convolutional neural networks (CNNs) into interpretable CNNs, in order to clarify knowledge representations in high conv-layers of CNNs. In an interpretable CNN, each filter in a high conv-layer represents a certain object part. We do not need any annotations of object parts or textures to supervise the learning process. Instead, the interpretable CNN automatically assigns each filter in a high conv-layer with an object part during the learning process. Our method can be applied to different types of CNNs with different structures. The clear knowledge representation in an interpretable CNN can help people understand the logics inside a CNN, i.e., based on which patterns the CNN makes the decision. Experiments showed that filters in an interpretable CNN were more semantically meaningful than those in traditional CNNs.",
            "authors": "Quanshi Zhang;Ying Nian Wu;Song-Chun Zhu",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1710.00935v4",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1710.00935v4",
            "publicationDate": "2017-10-02T23:00:42Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.12844962138977722,
            "title": "Interpretable Convolutional Neural Networks",
            "topics": [
                {
                    "affinity": 0.25083884596824646,
                    "id": 0
                },
                {
                    "affinity": 0.7443150281906128,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Image Understanding is becoming a vital feature in ever more applications ranging from medical diagnostics to autonomous vehicles. Many applications demand for embedded solutions that integrate into existing systems with tight real-time and power constraints. Convolutional Neural Networks (CNNs) presently achieve record-breaking accuracies in all image understanding benchmarks, but have a very high computational complexity. Embedded CNNs thus call for small and efficient, yet very powerful computing platforms. This master thesis explores the potential of FPGA-based CNN acceleration and demonstrates a fully functional proof-of-concept CNN implementation on a Zynq System-on-Chip. The ZynqNet Embedded CNN is designed for image classification on ImageNet and consists of ZynqNet CNN, an optimized and customized CNN topology, and the ZynqNet FPGA Accelerator, an FPGA-based architecture for its evaluation. ZynqNet CNN is a highly efficient CNN topology. Detailed analysis and optimization of prior topologies using the custom-designed Netscope CNN Analyzer have enabled a CNN with 84.5% top-5 accuracy at a computational complexity of only 530 million multiplyaccumulate operations. The topology is highly regular and consists exclusively of convolutional layers, ReLU nonlinearities and one global pooling layer. The CNN fits ideally onto the FPGA accelerator. The ZynqNet FPGA Accelerator allows an efficient evaluation of ZynqNet CNN. It accelerates the full network based on a nested-loop algorithm which minimizes the number of arithmetic operations and memory accesses. The FPGA accelerator has been synthesized using High-Level Synthesis for the Xilinx Zynq XC-7Z045, and reaches a clock frequency of 200MHz with a device utilization of 80% to 90 %.",
            "authors": "David Gschwend",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/2005.06892v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/2005.06892v1",
            "publicationDate": "2020-05-14T11:54:04Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.128271420198924,
            "title": "ZynqNet: An FPGA-Accelerated Embedded Convolutional Neural Network",
            "topics": [
                {
                    "affinity": 0.9910062551498413,
                    "id": 0
                }
            ]
        },
        {
            "abstract": "We develop a unified model, known as MgNet, that simultaneously recovers some convolutional neural networks (CNN) for image classification and multigrid (MG) methods for solving discretized partial differential equations (PDEs). This model is based on close connections that we have observed and uncovered between the CNN and MG methodologies. For example, pooling operation and feature extraction in CNN correspond directly to restriction operation and iterative smoothers in MG, respectively. As the solution space is often the dual of the data space in PDEs, the analogous concept of feature space and data space (which are dual to each other) is introduced in CNN. With such connections and new concept in the unified model, the function of various convolution operations and pooling used in CNN can be better understood. As a result, modified CNN models (with fewer weights and hyper parameters) are developed that exhibit competitive and sometimes better performance in comparison with existing CNN models when applied to both CIFAR-10 and CIFAR-100 data sets.",
            "authors": "Juncai He;Jinchao Xu",
            "citationCount": -1,
            "id": "10.1007/s11425-019-9547-2",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1901.10415v2",
            "publicationDate": "2019-01-29T17:30:59Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.1273018183310368,
            "title": "MgNet: A Unified Framework of Multigrid and Convolutional Neural Network",
            "topics": [
                {
                    "affinity": 0.9910696148872375,
                    "id": 0
                }
            ]
        },
        {
            "abstract": "Recent work has shown that the end-to-end approach using convolutional neural network (CNN) is effective in various types of machine learning tasks. For audio signals, the approach takes raw waveforms as input using an 1-D convolution layer. In this paper, we improve the 1-D CNN architecture for music auto-tagging by adopting building blocks from state-of-the-art image classification models, ResNets and SENets, and adding multi-level feature aggregation to it. We compare different combinations of the modules in building CNN architectures. The results show that they achieve significant improvements over previous state-of-the-art models on the MagnaTagATune dataset and comparable results on Million Song Dataset. Furthermore, we analyze and visualize our model to show how the 1-D CNN operates.",
            "authors": "Taejun Kim;Jongpil Lee;Juhan Nam",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1710.10451v2",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1710.10451v2",
            "publicationDate": "2017-10-28T11:55:50Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.12715299715049322,
            "title": "Sample-level CNN Architectures for Music Auto-tagging Using Raw   Waveforms",
            "topics": [
                {
                    "affinity": 0.9743927717208862,
                    "id": 0
                }
            ]
        },
        {
            "abstract": "Given a pre-trained CNN without any testing samples, this paper proposes a simple yet effective method to diagnose feature representations of the CNN. We aim to discover representation flaws caused by potential dataset bias. More specifically, when the CNN is trained to estimate image attributes, we mine latent relationships between representations of different attributes inside the CNN. Then, we compare the mined attribute relationships with ground-truth attribute relationships to discover the CNN's blind spots and failure modes due to dataset bias. In fact, representation flaws caused by dataset bias cannot be examined by conventional evaluation strategies based on testing images, because testing images may also have a similar bias. Experiments have demonstrated the effectiveness of our method.",
            "authors": "Quanshi Zhang;Wenguan Wang;Song-Chun Zhu",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1710.10577v2",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1710.10577v2",
            "publicationDate": "2017-10-29T08:25:51Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.12711805761911052,
            "title": "Examining CNN Representations with respect to Dataset Bias",
            "topics": [
                {
                    "affinity": 0.9891456961631775,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Deep Learning approaches based on Convolutional Neural Networks (CNNs) are extensively utilized and very successful in a wide range of application areas, including image classification and speech recognition. For the execution of trained CNNs, i.e. model inference, we nowadays witness a shift from the Cloud to the Edge. Unfortunately, deploying and inferring large, compute and memory intensive CNNs on edge devices is challenging because these devices typically have limited power budgets and compute/memory resources. One approach to address this challenge is to leverage all available resources across multiple edge devices to deploy and execute a large CNN by properly partitioning the CNN and running each CNN partition on a separate edge device. Although such distribution, deployment, and execution of large CNNs on multiple edge devices is a desirable and beneficial approach, there currently does not exist a design and programming framework that takes a trained CNN model, together with a CNN partitioning specification, and fully automates the CNN model splitting and deployment on multiple edge devices to facilitate distributed CNN inference at the Edge. Therefore, in this paper, we propose a novel framework, called AutoDiCE, for automated splitting of a CNN model into a set of sub-models and automated code generation for distributed and collaborative execution of these sub-models on multiple, possibly heterogeneous, edge devices, while supporting the exploitation of parallelism among and within the edge devices. Our experimental results show that AutoDiCE can deliver distributed CNN inference with reduced energy consumption and memory usage per edge device, and improved overall system throughput at the same time.",
            "authors": "Xiaotian Guo;Andy D. Pimentel;Todor Stefanov",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/2207.12113v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/2207.12113v1",
            "publicationDate": "2022-07-20T15:08:52Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.12663186966793735,
            "title": "AutoDiCE: Fully Automated Distributed CNN Inference at the Edge",
            "topics": [
                {
                    "affinity": 0.5422956347465515,
                    "id": 1
                },
                {
                    "affinity": 0.40321245789527893,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Convolutional neural network (CNN)-based filters have achieved great success in video coding. However, in most previous works, individual models are needed for each quantization parameter (QP) band. This paper presents a generic method to help an arbitrary CNN-filter handle different quantization noise. We model the quantization noise problem and implement a feasible solution on CNN, which introduces the quantization step (Qstep) into the convolution. When the quantization noise increases, the ability of the CNN-filter to suppress noise improves accordingly. This method can be used directly to replace the (vanilla) convolution layer in any existing CNN-filters. By using only 25% of the parameters, the proposed method achieves better performance than using multiple models with VTM-6.3 anchor. Besides, an additional BD-rate reduction of 0.2% is achieved by our proposed method for chroma components.",
            "authors": "Chao Liu;Heming Sun;Jiro Katto;Xiaoyang Zeng;Yibo Fan",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/2010.13059v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/2010.13059v1",
            "publicationDate": "2020-10-25T08:02:38Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.12639572517230427,
            "title": "A QP-adaptive Mechanism for CNN-based Filter in Video Coding",
            "topics": [
                {
                    "affinity": 0.20036661624908447,
                    "id": 0
                },
                {
                    "affinity": 0.7948988676071167,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "This paper presents a study on the use of Convolutional Neural Networks for camera relocalisation and its application to map compression. We follow state of the art visual relocalisation results and evaluate response to different data inputs -- namely, depth, grayscale, RGB, spatial position and combinations of these. We use a CNN map representation and introduce the notion of CNN map compression by using a smaller CNN architecture. We evaluate our proposal in a series of publicly available datasets. This formulation allows us to improve relocalisation accuracy by increasing the number of training trajectories while maintaining a constant-size CNN.",
            "authors": "Luis Contreras;Walterio Mayol-Cuevas",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1703.00845v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1703.00845v1",
            "publicationDate": "2017-03-02T16:12:29Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.1263067546134883,
            "title": "Towards CNN Map Compression for camera relocalisation",
            "topics": [
                {
                    "affinity": 0.6922973394393921,
                    "id": 1
                },
                {
                    "affinity": 0.30087587237358093,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Deep convolutional neural networks (CNN) have shown their promise as a universal representation for recognition. However, global CNN activations lack geometric invariance, which limits their robustness for classification and matching of highly variable scenes. To improve the invariance of CNN activations without degrading their discriminative power, this paper presents a simple but effective scheme called multi-scale orderless pooling (MOP-CNN). This scheme extracts CNN activations for local patches at multiple scale levels, performs orderless VLAD pooling of these activations at each level separately, and concatenates the result. The resulting MOP-CNN representation can be used as a generic feature for either supervised or unsupervised recognition tasks, from image classification to instance-level retrieval; it consistently outperforms global CNN activations without requiring any joint training of prediction layers for a particular target dataset. In absolute terms, it achieves state-of-the-art results on the challenging SUN397 and MIT Indoor Scenes classification datasets, and competitive results on ILSVRC2012/2013 classification and INRIA Holidays retrieval datasets.",
            "authors": "Yunchao Gong;Liwei Wang;Ruiqi Guo;Svetlana Lazebnik",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1403.1840v3",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1403.1840v3",
            "publicationDate": "2014-03-07T19:03:15Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.12628265353282017,
            "title": "Multi-scale Orderless Pooling of Deep Convolutional Activation Features",
            "topics": [
                {
                    "affinity": 0.1549699306488037,
                    "id": 0
                },
                {
                    "affinity": 0.8413127660751343,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Convolutional Neural Network (CNN) based image recognition has shown significant progress in recent years. It has achieved state-of-art results in field of pattern recognition. The proposed work uses convolutional network for Hindi handwritten character recognition. In traditional CNN, the raw images are fed as input to the network, which along with relevant information also contain redundant data. This redundant data makes feature extraction complex and increases the training time of the network. To improve the feature learning capacity of CNN, this work integrates wavelets and convolutional network for recognizing Hindi characters. The wavelet coefficients in three directions i.e. horizontal, vertical and diagonal are extracted from raw images and fed as input to the network. These coefficients are trained independently on different networks and the extracted features are merged at dense layer. The proposed framework outperforms the traditional CNN as demonstrated by experiments. It achieves significant results in much lesser time as compared to traditional CNN.",
            "authors": "Madhuri Yadav;Ravindra Kr. Purwar",
            "citationCount": 2,
            "id": "10.1109/ICPEICES.2018.8897291",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8897291",
            "publicationDate": "22-24 Oct. 2018",
            "source": [
                "ieee"
            ],
            "tfidf": 0.1261511183231641,
            "title": "Integrating Wavelet Coefficients and CNN for Recognizing Handwritten Characters",
            "topics": [
                {
                    "affinity": 0.1483890414237976,
                    "id": 0
                },
                {
                    "affinity": 0.8007815480232239,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Traffic congestion is one of the major issues of urban cities. The conventional techniques used usually to control traffic via different types of sensors are less precise and expensive. Intelligent solutions using deep learning algorithms provide promising results in terms of better performance, prompt decision making and cost effectiveness. This article aims at providing an easy, more accurate and less expensive solution for the traffic control issues specifically at the traffic signals. Three deep neural network (DNN) frameworks i.e. Faster R-CNN, Mask R-CNN and ResNet-50 have been implemented and compared for vehicle detection, classification and counting. A dataset of 3200 images of different vehicles is used for the training of the models. The training is carried out on NVIDIA 1060TI 3GB GPU. Trained system is tested on indigenous recorded video data of 8 hours for two routes at a traffic signal. Results demonstrated that the overall detection accuracy of Faster R-CNN and Mask R-CNN is >80%, whereas detection accuracy of ResNet-50 is >75%. The counting accuracies of Faster R-CNN, Mask R-CNN and ResNet-50 are >75%, >70% and >62% respectively. Various error analysis have been carried out to validate the performance of the aforementioned frameworks. Furthermore, a prototype has also been developed by interconnecting the DNN results with Arduino via Serial communication.",
            "authors": "Hassam Tahir;Muhammad Shahbaz Khan;Muhammad Owais Tariq",
            "citationCount": 1,
            "id": "10.1109/ICCCIS51004.2021.9397079",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9397079",
            "publicationDate": "19-20 Feb. 2021",
            "source": [
                "ieee"
            ],
            "tfidf": 0.12548840673367642,
            "title": "Performance Analysis and Comparison of Faster R-CNN, Mask R-CNN and ResNet50 for the Detection and Counting of Vehicles",
            "topics": [
                {
                    "affinity": 0.9754145741462708,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "With the increase of the nonlinearity and dimension, it is difficult for the present popular metamodeling techniques to construct reliable metamodels. To address this problem, Convolutional Neural Network (CNN) is introduced to construct a highly accurate metamodel efficiently. Considering the inherent characteristics of the CNN, it is a potential modeling tool to handle highly nonlinear and dimensional problems (hundreds-dimensional problems) with the limited training samples. In order to evaluate the proposed CNN metamodel for hundreds-dimensional and strong nonlinear problems, CNN is compared with other metamodeling techniques. Furthermore, several high-dimensional analytical functions are also employed to test the CNN metamodel. Testing and comparisons confirm the efficiency and capability of the CNN metamodel for hundreds-dimensional and strong nonlinear problems. Moreover, the proposed CNN metamodel is also applied to IsoGeometric Analysis (IGA)-based optimization successfully.",
            "authors": "Yu Li;Hu Wang;Juanjuan Liu",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1712.01639v3",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1712.01639v3",
            "publicationDate": "2017-11-15T00:06:54Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.12539054214886472,
            "title": "Can CNN Construct Highly Accurate Models Efficiently for   High-Dimensional Problems in Complex Product Designs?",
            "topics": [
                {
                    "affinity": 0.9907909035682678,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "The proof of origin of logs is becoming increasingly important. In the context of Industry 4.0 and to combat illegal logging there is an increasing motivation to track each individual log. Our previous works in this field focused on log tracking using digital log end images based on methods inspired by fingerprint and iris-recognition. This work presents a convolutional neural network (CNN) based approach which comprises a CNN-based segmentation of the log end combined with a final CNN-based recognition of the segmented log end using the triplet loss function for CNN training. Results show that the proposed two-stage CNN-based approach outperforms traditional approaches.",
            "authors": "Georg Wimmer;Rudolf Schraml;Heinz Hofbauer;Alexander Petutschnigg;Andreas Uhl",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/2101.04450v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/2101.04450v1",
            "publicationDate": "2021-01-12T12:51:32Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.1253425095649813,
            "title": "Two-stage CNN-based wood log recognition",
            "topics": [
                {
                    "affinity": 0.2311401218175888,
                    "id": 1
                },
                {
                    "affinity": 0.6800209879875183,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "We conduct an in-depth exploration of different strategies for doing event detection in videos using convolutional neural networks (CNNs) trained for image classification. We study different ways of performing spatial and temporal pooling, feature normalization, choice of CNN layers as well as choice of classifiers. Making judicious choices along these dimensions led to a very significant increase in performance over more naive approaches that have been used till now. We evaluate our approach on the challenging TRECVID MED'14 dataset with two popular CNN architectures pretrained on ImageNet. On this MED'14 dataset, our methods, based entirely on image-trained CNN features, can outperform several state-of-the-art non-CNN models. Our proposed late fusion of CNN- and motion-based features can further increase the mean average precision (mAP) on MED'14 from 34.95% to 38.74%. The fusion approach achieves the state-of-the-art classification performance on the challenging UCF-101 dataset.",
            "authors": "Shengxin Zha;Florian Luisier;Walter Andrews;Nitish Srivastava;Ruslan Salakhutdinov",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1503.04144v3",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1503.04144v3",
            "publicationDate": "2015-03-13T17:00:53Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.12512272974469615,
            "title": "Exploiting Image-trained CNN Architectures for Unconstrained Video   Classification",
            "topics": [
                {
                    "affinity": 0.27743449807167053,
                    "id": 0
                },
                {
                    "affinity": 0.7179346680641174,
                    "id": 1
                }
            ]
        },
        {
            "abstract": "This paper proposes a framework based on deep convolutional neural networks (CNNs) for automatic heart sound classification using short-segments of individual heart beats. We design a 1D-CNN that directly learns features from raw heart-sound signals, and a 2D-CNN that takes inputs of two- dimensional time-frequency feature maps based on Mel-frequency cepstral coefficients (MFCC). We further develop a time-frequency CNN ensemble (TF-ECNN) combining the 1D-CNN and 2D-CNN based on score-level fusion of the class probabilities. On the large PhysioNet CinC challenge 2016 database, the proposed CNN models outperformed traditional classifiers based on support vector machine and hidden Markov models with various hand-crafted time- and frequency-domain features. Best classification scores with 89.22% accuracy and 89.94% sensitivity were achieved by the ECNN, and 91.55% specificity and 88.82% modified accuracy by the 2D-CNN alone on the test set.",
            "authors": "Fuad Noman;Chee-Ming Ting;Sh-Hussain Salleh;Hernando Ombao",
            "citationCount": -1,
            "id": "10.1109/ICASSP.2019.8682668",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1810.11573v1",
            "publicationDate": "2018-10-27T01:32:27Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.12503764821177962,
            "title": "Short-segment heart sound classification using an ensemble of deep   convolutional neural networks",
            "topics": [
                {
                    "affinity": 0.963306188583374,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Conventional Convolutional neural networks (CNN) are trained on large domain datasets and are hence typically over-represented and inefficient in limited class applications. An efficient way to convert such large many-class pre-trained networks into small few-class networks is through a hierarchical decomposition of its feature maps. To alleviate this issue, we propose an automated framework for such decomposition in Hierarchically Self Decomposing CNN (HSD-CNN), in four steps. HSD-CNN is derived automatically using a class-specific filter sensitivity analysis that quantifies the impact of specific features on a class prediction. The decomposed hierarchical network can be utilized and deployed directly to obtain sub-networks for a subset of classes, and it is shown to perform better without the requirement of retraining these sub-networks. Experimental results show that HSD-CNN generally does not degrade accuracy if the full set of classes are used. Interestingly, when operating on known subsets of classes, HSD-CNN has an improvement in accuracy with a much smaller model size, requiring much fewer operations. HSD-CNN flow is verified on the CIFAR10, CIFAR100 and CALTECH101 data sets. We report accuracies up to $85.6 %$ ( $94.75 %$ ) on scenarios with 13 ( 4 ) classes of CIFAR100, using a pre-trained VGG-16 network on the full data set. In this case, the proposed HSD-CNN requires $3.97  times$ fewer parameters and has $71.22 %$ savings in operations, in comparison to baseline VGG-16 containing features for all 100 classes.",
            "authors": "K. Sai Ram;Jayanta Mukherjee;Amit Patra;Partha Pratim Das",
            "citationCount": -1,
            "id": "10.1145/3293353.3293383",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1811.04406v2",
            "publicationDate": "2018-11-11T12:20:18Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.125034901170333,
            "title": "HSD-CNN: Hierarchically self decomposing CNN architecture using class   specific filter sensitivity analysis",
            "topics": [
                {
                    "affinity": 0.545603334903717,
                    "id": 0
                },
                {
                    "affinity": 0.4506528079509735,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "This paper presents an unsupervised method to learn a neural network, namely an explainer, to interpret a pre-trained convolutional neural network (CNN), i.e., explaining knowledge representations hidden in middle conv-layers of the CNN. Given feature maps of a certain conv-layer of the CNN, the explainer performs like an auto-encoder, which first disentangles the feature maps into object-part features and then inverts object-part features back to features of higher conv-layers of the CNN. More specifically, the explainer contains interpretable conv-layers, where each filter disentangles the representation of a specific object part from chaotic input feature maps. As a paraphrase of CNN features, the disentangled representations of object parts help people understand the logic inside the CNN. We also learn the explainer to use object-part features to reconstruct features of higher CNN layers, in order to minimize loss of information during the feature disentanglement. More crucially, we learn the explainer via network distillation without using any annotations of sample labels, object parts, or textures for supervision. We have applied our method to different types of CNNs for evaluation, and explainers have significantly boosted the interpretability of CNN features.",
            "authors": "Quanshi Zhang;Yu Yang;Yuchen Liu;Ying Nian Wu;Song-Chun Zhu",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1805.07468v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1805.07468v1",
            "publicationDate": "2018-05-18T23:02:14Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.12474896166987338,
            "title": "Unsupervised Learning of Neural Networks to Explain Neural Networks",
            "topics": [
                {
                    "affinity": 0.9930698275566101,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "We propose a new CNN-CRF end-to-end learning framework, which is based on joint stochastic optimization with respect to both Convolutional Neural Network (CNN) and Conditional Random Field (CRF) parameters. While stochastic gradient descent is a standard technique for CNN training, it was not used for joint models so far. We show that our learning method is (i) general, i.e. it applies to arbitrary CNN and CRF architectures and potential functions; (ii) scalable, i.e. it has a low memory footprint and straightforwardly parallelizes on GPUs; (iii) easy in implementation. Additionally, the unified CNN-CRF optimization approach simplifies a potential hardware implementation. We empirically evaluate our method on the task of semantic labeling of body parts in depth images and show that it compares favorably to competing techniques.",
            "authors": "Alexander Kirillov;Dmitrij Schlesinger;Shuai Zheng;Bogdan Savchynskyy;Philip H. S. Torr;Carsten Rother",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1511.05067v3",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1511.05067v3",
            "publicationDate": "2015-11-16T17:59:14Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.12423842559047281,
            "title": "Joint Training of Generic CNN-CRF Models with Stochastic Optimization",
            "topics": [
                {
                    "affinity": 0.5306110382080078,
                    "id": 0
                },
                {
                    "affinity": 0.4636867940425873,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "In image classification, Convolutional Neural Network(CNN) models have achieved high performance with the rapid development in deep learning. However, some categories in the image datasets are more difficult to distinguished than others. Improving the classification accuracy on these confused categories is benefit to the overall performance. In this paper, we build a Confusion Visual Tree(CVT) based on the confused semantic level information to identify the confused categories. With the information provided by the CVT, we can lead the CNN training procedure to pay more attention on these confused categories. Therefore, we propose Visual Tree Convolutional Neural Networks(VT-CNN) based on the original deep CNN embedded with our CVT. We evaluate our VT-CNN model on the benchmark datasets CIFAR-10 and CIFAR-100. In our experiments, we build up 3 different VT-CNN models and they obtain improvement over their based CNN models by 1.36%, 0.89% and 0.64%, respectively.",
            "authors": "Yuntao Liu;Yong Dou;Ruochun Jin;Peng Qiao",
            "citationCount": -1,
            "id": "10.1109/ICPR.2018.8546126",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1906.01536v1",
            "publicationDate": "2019-06-04T15:49:57Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.12420751205544593,
            "title": "Visual Tree Convolutional Neural Network in Image Classification",
            "topics": [
                {
                    "affinity": 0.9907183051109314,
                    "id": 0
                }
            ]
        },
        {
            "abstract": "In this paper, we address the issue of how to enhance the generalization performance of convolutional neural networks (CNN) in the early learning stage for image classification. This is motivated by real-time applications that require the generalization performance of CNN to be satisfactory within limited training time. In order to achieve this, a novel hierarchical transfer CNN framework is proposed. It consists of a group of shallow CNNs and a cloud CNN, where the shallow CNNs are trained firstly and then the first layers of the trained shallow CNNs are used to initialize the first layer of the cloud CNN. This method will boost the generalization performance of the cloud CNN significantly, especially during the early stage of training. Experiments using CIFAR-10 and ImageNet datasets are performed to examine the proposed method. Results demonstrate the improvement of testing accuracy is 12% on average and as much as 20% for the CIFAR-10 case while 5% testing accuracy improvement for the ImageNet case during the early stage of learning. It is also shown that universal improvements of testing accuracy are obtained across different settings of dropout and number of shallow CNNs.",
            "authors": "Xishuang Dong;Hsiang-Huang Wu;Yuzhong Yan;Lijun Qian",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1804.00021v2",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1804.00021v2",
            "publicationDate": "2018-03-30T18:19:32Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.12415849896925515,
            "title": "Hierarchical Transfer Convolutional Neural Networks for Image   Classification",
            "topics": [
                {
                    "affinity": 0.44303008913993835,
                    "id": 0
                },
                {
                    "affinity": 0.20122340321540833,
                    "id": 1
                },
                {
                    "affinity": 0.3557465374469757,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Machine learning methods are used today for most recognition problems. Convolutional Neural Networks (CNN) have time and again proved successful for many image processing tasks primarily for their architecture. In this paper we propose to apply CNN to small data sets like for example, personal albums or other similar environs where the size of training dataset is a limitation, within the framework of a proposed hybrid CNN-AIS model. We use Artificial Immune System Principles to enhance small size of training data set. A layer of Clonal Selection is added to the local filtering and max pooling of CNN Architecture. The proposed Architecture is evaluated using the standard MNIST dataset by limiting the data size and also with a small personal data sample belonging to two different classes. Experimental results show that the proposed hybrid CNN-AIS based recognition engine works well when the size of training data is limited in size",
            "authors": "Vandna Bhalla;Santanu Chaudhury;Arihant Jain",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1503.03270v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1503.03270v1",
            "publicationDate": "2015-03-11T10:58:25Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.12378842673387641,
            "title": "A Novel Hybrid CNN-AIS Visual Pattern Recognition Engine",
            "topics": [
                {
                    "affinity": 0.21932393312454224,
                    "id": 0
                },
                {
                    "affinity": 0.7373617887496948,
                    "id": 2
                }
            ]
        }
    ],
    "max_tfidf": 0.23256704435701123,
    "topics": [
        {
            "id": 0,
            "name": "cnn cnns network model image"
        },
        {
            "id": 1,
            "name": "cnn network cnns train model"
        },
        {
            "id": 2,
            "name": "cnn image network model propose"
        }
    ],
    "sources": ["ieee", "arxiv"],
    "topicsVisualization": {
        "ldaPlot": "
<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css">


<div id="ldavis_el11406171307159523194713451"></div>
<script type="text/javascript">

var ldavis_el11406171307159523194713451_data = {"mdsDat": {"x": [0.016098515717118154, 0.003223014552117276, -0.019321530269235426], "y": [0.010637533289510896, -0.01671277558635737, 0.0060752422968464856], "topics": [1, 2, 3], "cluster": [1, 1, 1], "Freq": [41.30827123998561, 38.33295197184272, 20.358776788171678]}, "tinfo": {"Term": ["train", "cnns", "network", "attack", "cnn", "prune", "design", "paper", "recognition", "model", "performance", "temporal", "layer", "base", "different", "feature", "face", "study", "dataset", "target", "time", "adversarial", "graph", "image", "weight", "filter", "number", "compare", "task", "energy", "bilateral", "cervical", "earthquake", "wsi", "dtw", "hw", "ssl", "vgq", "lb", "relief", "stain", "prostate", "observers", "metamodel", "sod", "tfds", "magnifications", "graph", "grasp", "precnn", "sfq", "songs", "shadow", "gw", "cadx", "haar", "hdr", "pluralistic", "abiu", "manga", "quantum", "mine", "weave", "fish", "knee", "detectors", "defect", "localization", "noise", "fmri", "object", "dct", "wave", "cnn", "image", "method", "mask", "data", "signal", "stream", "generate", "patch", "transformer", "segmentation", "test", "detection", "base", "edge", "process", "structure", "domain", "propose", "result", "use", "model", "time", "network", "feature", "neural", "information", "classification", "learn", "convolutional", "methods", "deep", "improve", "approach", "train", "cnns", "performance", "paper", "layer", "different", "accuracy", "achieve", "dfa", "promoters", "uaps", "electrodes", "vcs", "bo", "fcsns", "fsnet", "bn", "winograd", "collage", "pin", "multigrid", "mtt", "minimizers", "asc", "promoter", "pavia", "hobflops", "spectralnet", "ubi", "pos", "hsi", "separator", "epu", "phmm", "migration", "demosaicing", "chinese", "dd", "mil", "salinas", "rnns", "lipschitz", "elm", "filter", "optimization", "os", "ecg", "task", "text", "insurance", "cnns", "document", "cifar", "resolution", "sentiment", "convolutional", "learn", "neural", "pca", "model", "achieve", "network", "function", "propose", "popular", "high", "performance", "cnn", "train", "state", "deep", "reduce", "approach", "feature", "convolution", "art", "accuracy", "classification", "recognition", "methods", "image", "layer", "result", "base", "input", "demonstrate", "paper", "data", "method", "forensic", "cycnn", "substation", "boson", "rigidity", "atomistic", "higgs", "ht", "urine", "hi", "psn", "cbam", "cnld", "cfo", "a_z", "gluon", "deepisign", "constituents", "lepton", "porous", "ascad", "tortuosity", "cylindrical", "cyconv", "untrusted", "azimuth", "kinetics", "dscs", "manycore", "transferrable", "attacker", "fool", "secret", "hmdb", "security", "pdi", "attack", "lofar", "spoof", "specaugment", "equipment", "temporal", "prune", "ann", "train", "face", "target", "sparsity", "network", "cnns", "design", "energy", "paper", "recognition", "study", "examples", "dataset", "cnn", "layer", "model", "different", "performance", "base", "feature", "tune", "time", "propose", "video", "image", "weight", "neural", "compare", "convolutional", "classification", "result", "accuracy", "learn", "deep", "data", "improve", "map", "use", "methods", "approach", "method"], "Freq": [2351.0, 3243.0, 3596.0, 227.0, 7030.0, 215.0, 699.0, 1106.0, 715.0, 3037.0, 1531.0, 214.0, 1221.0, 1950.0, 916.0, 1838.0, 252.0, 575.0, 658.0, 276.0, 779.0, 276.0, 236.0, 3317.0, 376.0, 523.0, 454.0, 690.0, 1115.0, 218.0, 7.430612286327104, 8.244312618009964, 7.319612000371302, 6.3580699257281275, 10.28541944262239, 6.337819216140475, 7.092213495425681, 6.300771176798062, 7.872823989967404, 4.711383073724408, 10.197625474310485, 15.686046507743365, 4.696600098028971, 4.689986290449869, 4.677771492794716, 5.455627321144706, 3.889991956574584, 195.08833916020754, 21.013686552259777, 3.1058833109424375, 6.22144849445843, 3.8799917497840934, 8.523508295153096, 13.180010972913305, 6.18438987922841, 4.62934767621388, 11.526140272988528, 3.829843005934745, 4.600355642990417, 7.6642576155519055, 22.052742127402812, 22.60929468736401, 9.120433953519983, 19.464911267710878, 10.56177937176881, 38.06257436961303, 27.495020563211433, 50.50425724663274, 159.9467346997519, 14.126117187066177, 449.764218542586, 11.909181295592528, 26.79163378984917, 3520.837999434681, 1763.0486335354424, 677.8498008327634, 73.91073836971269, 819.1762386210183, 179.81242557381844, 60.533836802300726, 207.97131575382335, 125.9517595128108, 90.09884059364356, 300.6460533755844, 242.67931446291314, 359.3372095666517, 895.217942196916, 113.47899198694002, 366.36469792487054, 266.7975274397355, 154.23355560797899, 1047.1967524101713, 676.5137757437669, 363.4094282970299, 1196.8871548835127, 373.81984697550234, 1343.0342570971823, 749.8738628988025, 903.1661363074952, 320.866789487644, 575.1441355821943, 748.4470036263369, 809.5353610080602, 406.4116303163214, 499.2327043797069, 333.3079816434788, 407.99051162758826, 759.0014123095036, 942.4160698089704, 549.0249533894012, 438.46774918026, 465.0966947961278, 384.7548748030113, 422.98487534425317, 366.6414646731602, 7.741151661955932, 8.30160229199789, 4.9133212376546975, 4.876847001230494, 4.056291016370238, 4.044033160512892, 4.044174811209081, 7.26417156710014, 17.74604109230352, 7.232994019872261, 6.428632667954684, 8.834103571278481, 12.843338422657192, 3.9857941146933, 3.1881305834542206, 13.54622684259571, 6.369357249374055, 7.950992283588178, 7.149218592194529, 3.9557697284447952, 8.686729925252333, 4.734126564217286, 36.32122750128983, 3.147938098039501, 5.496869436760118, 3.9192169789326337, 5.480961049450619, 7.8286001935986045, 15.655617039923401, 3.9041748526935978, 6.247063300744428, 8.465320400109567, 32.361862200554455, 9.918375204451609, 11.360094027444068, 335.71277284029475, 126.53642427422761, 11.20868022876678, 24.02660919905803, 624.8890588182519, 81.58004471226758, 9.751789405802286, 1554.101546342846, 25.990383789254086, 99.95937760624126, 153.99798892273017, 25.910130327846975, 977.2319295922707, 885.7772885733966, 1045.8231406614843, 23.919205911835256, 1279.4780066753776, 460.08778414511414, 1420.5564366696615, 159.6510045345442, 994.8305021685669, 79.86956904643965, 328.38009009836725, 655.3793966898572, 2412.104463656906, 927.5667694537133, 385.50684100086653, 529.1736904789296, 218.42877052425834, 429.841820876879, 717.6836565382655, 217.07309026870396, 349.48509905648723, 459.53919749693824, 549.2520622989188, 319.5907906750074, 398.2254168925536, 1060.631504649539, 474.2676246930406, 545.2271254842243, 665.430411905104, 291.6459517990978, 281.1799059324965, 388.35441648537545, 455.7939347699597, 356.4856400311198, 21.47386582190607, 4.667025810139184, 6.819373761260366, 5.274691516308737, 3.7359429821429786, 5.153884417199212, 2.9427258030448424, 2.861497625141143, 5.6443092970277675, 10.576146584077287, 4.929984479254354, 2.8019728735822493, 2.7791039713662897, 4.1713038181520545, 3.466244081819474, 2.07177444123813, 4.146590447860937, 2.0597200709428702, 2.7434607030276914, 7.535642428772371, 2.7309339223427975, 4.76515582059294, 2.04088672449007, 2.0379791158580267, 2.7068879033965065, 4.727491907594009, 19.47800680943805, 4.026789327477406, 3.351746328384524, 2.004552128111621, 8.626406223254897, 20.265183251343544, 7.159496732216882, 10.225906837774096, 27.630101186897, 5.818682070261957, 127.22925617665294, 7.05366638028884, 10.505181821597075, 9.210316584469279, 9.152060860304974, 100.15863622613615, 98.58125046414882, 8.027961120019466, 664.840299610961, 99.57405521243777, 105.22607249568316, 34.892165355588695, 832.6052652709196, 747.3152378568277, 209.11209654118716, 78.04521256628396, 279.9476989977247, 197.06431462473384, 162.68079435226173, 46.91065817537303, 178.14577098228264, 1097.978892128038, 282.6077583308098, 560.7574359340089, 225.35980593230252, 327.27005023121166, 389.46354555162543, 370.5396311294889, 69.51197769496126, 187.45573401629386, 412.9719930959512, 84.28977807574229, 494.2756788246057, 107.25549332137105, 373.56301314464025, 162.12754844654103, 340.64533765895175, 249.85554289008505, 257.61259446042646, 208.40673414251594, 277.2599517942121, 214.35820297219095, 227.1902521582202, 160.50551738344285, 131.66853361555178, 144.37021629841206, 149.80593460352344, 147.28162367291924, 147.44956339463207], "Total": [2351.0, 3243.0, 3596.0, 227.0, 7030.0, 215.0, 699.0, 1106.0, 715.0, 3037.0, 1531.0, 214.0, 1221.0, 1950.0, 916.0, 1838.0, 252.0, 575.0, 658.0, 276.0, 779.0, 276.0, 236.0, 3317.0, 376.0, 523.0, 454.0, 690.0, 1115.0, 218.0, 8.481484176474437, 9.421339976722793, 8.481018548720451, 7.536556942192314, 12.244868039655394, 7.546275331136887, 8.479569962085012, 7.548407581997936, 9.438965696961768, 5.654216452063898, 12.2532017597604, 18.875746810414388, 5.660053249132446, 5.654156488697553, 5.6612285852828785, 6.6054713043887885, 4.711117716063298, 236.55716237796142, 25.485575073892225, 3.769178320585467, 7.555750649803968, 4.71576530335021, 10.3654761651477, 16.078236631741046, 7.5486117363051335, 5.655846894123588, 14.17170059309658, 4.713353618008681, 5.663347580362417, 9.440623704966333, 27.43841573436995, 28.34605232008422, 11.313029393534729, 24.540128270655387, 13.219404204410482, 50.151197507768686, 35.89938032627655, 68.14094433793196, 230.44147815168992, 17.984983195124286, 698.4017173049425, 15.127803149560572, 35.89892961441914, 7030.921355219625, 3317.955817009587, 1181.7850042585153, 109.68596522758696, 1502.1604255491982, 292.77942858338304, 89.0725994289474, 348.5573905284805, 202.26818617314828, 140.49276778460955, 547.3977445207476, 431.1075229781272, 686.8315292038009, 1950.1118996536457, 186.60789053086125, 710.0429946869217, 501.33280525977835, 270.14371022856847, 2454.9992476746893, 1479.3534956884178, 731.4999878797937, 3037.1225974928993, 779.0418685482817, 3596.1959590377633, 1838.097150566557, 2322.5522901136196, 664.6391962058856, 1374.251740771198, 1911.4842439939457, 2127.4126282592824, 954.4429818123983, 1242.7645978308274, 747.0904397304191, 985.1139561773866, 2351.408481374178, 3243.832854008644, 1531.67440031047, 1106.76986466336, 1221.972077819978, 916.7764385692615, 1090.9308069837073, 967.9721924294856, 8.834726651974595, 9.802017474572734, 5.875235891500032, 5.872880789058538, 4.893196121244523, 4.891510170746034, 4.892648878108495, 8.809726234566785, 21.540457197159814, 8.806372112309518, 7.827862945109086, 10.76615140829974, 15.66256511684916, 4.889061963524349, 3.910778889790674, 16.636587111461782, 7.8250922684238144, 9.782434236604324, 8.802587748855995, 4.888089715372981, 10.758768158349513, 5.865356521875793, 45.022297450055625, 3.9082228610156866, 6.842320474888631, 4.885274429076231, 6.840794158723223, 9.774825599523375, 19.558748234729386, 4.884784503537215, 7.816866393732764, 10.746503951404351, 42.92947267314925, 12.69563605413361, 14.645833279609008, 523.9163588811988, 189.48522936495593, 14.639092260413824, 33.12716943305815, 1115.2382862134523, 125.26256915125968, 12.687251513671866, 3243.832854008644, 36.98406579886729, 162.76960935307494, 267.9208982863498, 36.981505417980564, 2127.4126282592824, 1911.4842439939457, 2322.5522901136196, 34.0646605511276, 3037.1225974928993, 967.9721924294856, 3596.1959590377633, 299.1080582374658, 2454.9992476746893, 135.45625138311323, 688.8616492746753, 1531.67440031047, 7030.921355219625, 2351.408481374178, 853.3458911250991, 1242.7645978308274, 441.92806284846574, 985.1139561773866, 1838.097150566557, 442.86831555620023, 783.789098113386, 1090.9308069837073, 1374.251740771198, 715.4774378439822, 954.4429818123983, 3317.955817009587, 1221.972077819978, 1479.3534956884178, 1950.1118996536457, 688.9174394583134, 652.632660819006, 1106.76986466336, 1502.1604255491982, 1181.7850042585153, 25.41395843548742, 5.643538860626222, 8.467750186064844, 6.589208327309599, 4.701332542769489, 6.593689967413549, 3.767741666088412, 3.769739464141768, 7.529510291473139, 14.166038801297127, 6.605446929829505, 3.7728283675082883, 3.7709109073136355, 5.661710484873587, 4.718482431605869, 2.8288544559404976, 5.667996874696191, 2.8295104212625883, 3.7728924226688223, 10.394144487075407, 3.772923443854932, 6.594683992431516, 2.8284532120869734, 2.8278937674020383, 3.7702859287414006, 6.611101676076224, 27.345923464900167, 5.655154658209533, 4.71662274785016, 2.8320169245045985, 12.289511387579035, 29.323888799294455, 10.37559017908588, 15.152216188237427, 43.59321677478791, 8.471715325579838, 227.6499403509024, 10.417469040254085, 16.03293997301716, 14.160630115778543, 14.170577959180603, 214.081444292752, 215.18940661947957, 12.362350901252965, 2351.408481374178, 252.2790055507777, 276.0175966269016, 74.27815301682969, 3596.1959590377633, 3243.832854008644, 699.5266024655986, 218.15547303573223, 1106.76986466336, 715.4774378439822, 575.5731651790563, 116.15615077871256, 658.6572080084641, 7030.921355219625, 1221.972077819978, 3037.1225974928993, 916.7764385692615, 1531.67440031047, 1950.1118996536457, 1838.097150566557, 201.27153362463963, 779.0418685482817, 2454.9992476746893, 263.7361261385953, 3317.955817009587, 376.2882517092553, 2322.5522901136196, 690.808677996292, 2127.4126282592824, 1374.251740771198, 1479.3534956884178, 1090.9308069837073, 1911.4842439939457, 1242.7645978308274, 1502.1604255491982, 747.0904397304191, 530.9069208423416, 731.4999878797937, 954.4429818123983, 985.1139561773866, 1181.7850042585153], "Category": ["Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3"], "logprob": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -9.4759, -9.3719, -9.4909, -9.6317, -9.1507, -9.6349, -9.5225, -9.6408, -9.418, -9.9315, -9.1593, -8.7287, -9.9346, -9.936, -9.9386, -9.7848, -10.1231, -6.208, -8.4363, -10.3482, -9.6535, -10.1256, -9.3386, -8.9028, -9.6594, -9.949, -9.0368, -10.1386, -9.9553, -9.4449, -8.388, -8.3631, -9.2709, -8.5128, -9.1242, -7.8422, -8.1675, -7.5594, -6.4066, -8.8334, -5.3727, -9.0042, -8.1934, -3.315, -4.0067, -4.9625, -7.1786, -4.7732, -6.2895, -7.3783, -6.1441, -6.6456, -6.9806, -5.7755, -5.9897, -5.5972, -4.6844, -6.7498, -5.5778, -5.895, -6.443, -4.5276, -4.9645, -5.5859, -4.394, -5.5577, -4.2788, -4.8616, -4.6756, -5.7104, -5.1268, -4.8635, -4.785, -5.4741, -5.2684, -5.6724, -5.4702, -4.8495, -4.633, -5.1733, -5.3982, -5.3392, -5.5289, -5.4341, -5.5771, -9.3602, -9.2903, -9.8148, -9.8222, -10.0064, -10.0095, -10.0094, -9.4238, -8.5305, -9.4281, -9.5459, -9.2281, -8.8539, -10.024, -10.2473, -8.8006, -9.5552, -9.3334, -9.4397, -10.0315, -9.2449, -9.8519, -7.8143, -10.26, -9.7025, -10.0408, -9.7054, -9.3489, -8.6559, -10.0447, -9.5746, -9.2707, -7.9297, -9.1123, -8.9766, -5.5905, -6.5662, -8.99, -8.2275, -4.9691, -7.0051, -9.1293, -4.0581, -8.149, -6.8019, -6.3698, -8.1521, -4.522, -4.6202, -4.4541, -8.232, -4.2525, -5.2753, -4.1479, -6.3337, -4.5041, -7.0263, -5.6125, -4.9215, -3.6185, -4.5741, -5.4521, -5.1354, -6.0202, -5.3433, -4.8307, -6.0265, -5.5502, -5.2765, -5.0982, -5.6397, -5.4197, -4.4401, -5.2449, -5.1055, -4.9063, -5.7312, -5.7677, -5.4448, -5.2847, -5.5304, -7.7071, -9.2334, -8.8541, -9.111, -9.4559, -9.1342, -9.6946, -9.7226, -9.0433, -8.4153, -9.1786, -9.7436, -9.7518, -9.3457, -9.5308, -10.0455, -9.3516, -10.0513, -9.7647, -8.7543, -9.7693, -9.2126, -10.0605, -10.062, -9.7781, -9.2205, -7.8046, -9.3809, -9.5644, -10.0785, -8.6191, -7.765, -8.8055, -8.449, -7.455, -9.0128, -5.9279, -8.8204, -8.422, -8.5536, -8.5599, -6.1672, -6.183, -8.691, -4.2744, -6.173, -6.1178, -7.2216, -4.0494, -4.1574, -5.431, -6.4166, -5.1393, -5.4904, -5.6821, -6.9257, -5.5913, -3.7727, -5.1299, -4.4446, -5.3562, -4.9831, -4.8091, -4.859, -6.5324, -5.5404, -4.7505, -6.3397, -4.5708, -6.0987, -4.8508, -5.6855, -4.9431, -5.253, -5.2225, -5.4344, -5.149, -5.4063, -5.3481, -5.6956, -5.8936, -5.8015, -5.7646, -5.7816, -5.7804], "loglift": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.7518, 0.7507, 0.7368, 0.7141, 0.7097, 0.7096, 0.7054, 0.7034, 0.7027, 0.7017, 0.7005, 0.699, 0.6975, 0.6971, 0.6933, 0.6929, 0.6926, 0.6914, 0.6912, 0.6905, 0.6898, 0.689, 0.6885, 0.6853, 0.6848, 0.6838, 0.6775, 0.6765, 0.6762, 0.6757, 0.6656, 0.658, 0.6687, 0.6524, 0.6597, 0.6083, 0.6174, 0.5846, 0.519, 0.6426, 0.444, 0.6449, 0.5915, 0.1925, 0.2518, 0.3283, 0.4893, 0.2777, 0.3966, 0.4979, 0.3677, 0.4104, 0.4399, 0.2849, 0.3095, 0.2363, 0.1055, 0.3867, 0.2224, 0.2533, 0.3236, 0.0321, 0.1017, 0.1845, -0.0471, 0.1498, -0.1008, -0.0125, -0.0604, 0.1559, 0.0131, -0.0535, -0.0821, 0.0303, -0.0279, 0.077, 0.0026, -0.2467, -0.352, -0.1419, -0.0418, -0.0819, 0.0159, -0.0633, -0.0867, 0.8267, 0.7927, 0.7801, 0.773, 0.7713, 0.7686, 0.7684, 0.766, 0.7651, 0.762, 0.7619, 0.7611, 0.7604, 0.7546, 0.7546, 0.7534, 0.753, 0.7516, 0.7508, 0.7472, 0.7449, 0.7446, 0.7441, 0.7425, 0.7399, 0.7385, 0.7372, 0.7368, 0.7363, 0.7348, 0.7347, 0.7203, 0.6763, 0.712, 0.7048, 0.5138, 0.5551, 0.6919, 0.6377, 0.3796, 0.53, 0.6957, 0.223, 0.6061, 0.4713, 0.4051, 0.6031, 0.1809, 0.1897, 0.161, 0.6053, 0.0944, 0.2151, 0.03, 0.331, 0.0556, 0.4306, 0.218, 0.11, -0.111, 0.0287, 0.1643, 0.1051, 0.2542, 0.1295, 0.0184, 0.2458, 0.1512, 0.0943, 0.0418, 0.153, 0.0848, -0.1816, 0.0124, -0.0393, -0.1163, 0.0993, 0.1168, -0.0884, -0.2338, -0.2396, 1.4232, 1.4017, 1.3752, 1.3691, 1.3618, 1.3453, 1.3445, 1.316, 1.3035, 1.2994, 1.2991, 1.2942, 1.2865, 1.2862, 1.2832, 1.2802, 1.2791, 1.2741, 1.273, 1.2701, 1.2685, 1.2667, 1.2653, 1.2641, 1.2603, 1.2563, 1.2524, 1.2521, 1.25, 1.2461, 1.2377, 1.2222, 1.2206, 1.1984, 1.1357, 1.216, 1.0098, 1.2017, 1.1689, 1.1615, 1.1545, 0.8321, 0.811, 1.1599, 0.3284, 0.662, 0.6273, 0.8361, 0.1286, 0.1236, 0.3841, 0.5637, 0.2171, 0.3022, 0.3281, 0.685, 0.2841, -0.2652, 0.1275, -0.0977, 0.1885, 0.0483, -0.0192, -0.0099, 0.5285, 0.1671, -0.1908, 0.451, -0.3124, 0.3365, -0.2357, 0.1422, -0.2402, -0.1131, -0.1562, -0.0636, -0.339, -0.1658, -0.2972, 0.0538, 0.1974, -0.0311, -0.2601, -0.3088, -0.4896]}, "token.table": {"Topic": [1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 2, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 3, 1, 2, 3, 1, 2, 3, 1, 3, 3, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 1, 2, 3, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 1, 2, 3, 1, 2, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 1, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 2, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3], "Freq": [0.21193254706252326, 0.21193254706252326, 0.6357976411875698, 0.8828700568083503, 0.17657401136167006, 0.3877422814463772, 0.4216582729676915, 0.19066287125495615, 0.37914312298463576, 0.47522026314150534, 0.14566534152815708, 0.17725121275696773, 0.510049408137397, 0.31109396524692295, 0.08089076325269547, 0.32356305301078186, 0.6471261060215637, 0.41416528254578155, 0.43649772425168154, 0.14922131503487718, 0.37765261179631704, 0.4452728429625495, 0.1773436251340813, 0.12021696436897801, 0.8415187505828461, 0.060108482184489005, 0.26504645929901616, 0.26504645929901616, 0.7951393778970484, 0.15166014855749452, 0.15166014855749452, 0.7583007427874726, 0.14056669617692327, 0.3030969386314908, 0.5578740754521642, 0.16274040007981055, 0.16274040007981055, 0.7323318003591476, 0.1512607200731351, 0.1512607200731351, 0.7563036003656755, 0.4589480225001235, 0.34100607258389065, 0.19947573268441124, 0.8253272486690818, 0.11790389266701169, 0.09284853992159958, 0.8356368592943962, 0.09284853992159958, 0.8177433676663369, 0.15176329997875543, 0.15176329997875543, 0.7588164998937772, 0.7948481402405335, 0.1324746900400889, 0.1324746900400889, 0.26505313854508467, 0.795159415635254, 0.849136112247888, 0.106142014030986, 0.106142014030986, 0.17662506810825168, 0.17662506810825168, 0.7065002724330067, 0.10225603274798081, 0.8180482619838465, 0.10225603274798081, 0.23345881427761828, 0.6143653007305744, 0.1535913251826436, 0.4184095118390197, 0.3994901252167336, 0.18191717906044336, 0.26518791469204755, 0.7955637440761426, 0.5007878515645855, 0.3430560346418007, 0.1561672993518645, 0.29039720675986774, 0.47906290796691553, 0.23028313529683778, 0.12774878750589372, 0.7664927250353624, 0.12774878750589372, 0.44874942929084627, 0.3170197581119204, 0.23450776627457126, 0.7068360607442319, 0.3387011324384639, 0.4899876382609778, 0.17160857376882171, 0.380744190967207, 0.4592433019443966, 0.16028860385162666, 0.177193782960686, 0.88596891480343, 0.707240145671166, 0.7071002594115038, 0.5452147361028826, 0.3035627834712021, 0.15111568387711155, 0.4008154724340426, 0.3279399319914894, 0.2702467958078015, 0.7932414165733359, 0.13220690276222263, 0.06610345138111132, 0.20471732156779296, 0.8188692862711718, 0.40152415097032473, 0.4256638794855747, 0.1721967300754499, 0.1764291727937131, 0.1764291727937131, 0.7057166911748524, 0.752102118605021, 0.11142253608963273, 0.11142253608963273, 0.37080583692564173, 0.43056380238060055, 0.199193218183196, 0.10230361552934106, 0.8184289242347285, 0.10230361552934106, 0.35023685895069107, 0.3516663971504898, 0.2987734837579365, 0.5226900407675887, 0.30866375666498275, 0.16889148949593397, 0.7577087265785348, 0.1395779233170985, 0.09969851665507037, 0.11318969328569958, 0.9055175462855967, 0.11318969328569958, 0.41994971053230623, 0.33486898995692993, 0.24542515550589328, 0.24334804207155727, 0.7030054548733876, 0.05407734268256828, 0.5700669464771202, 0.3183490740067035, 0.11105200256047795, 0.17682982348649118, 0.17682982348649118, 0.7073192939459647, 0.816668662137859, 0.08166686621378591, 0.08166686621378591, 0.8253725610653339, 0.11791036586647627, 0.11791036586647627, 0.18112021348894666, 0.7244808539557867, 0.09056010674447333, 0.6055478130026449, 0.17148256651402333, 0.2197120383460924, 0.8513709335485309, 0.17027418670970618, 0.13655761074274586, 0.7510668590851023, 0.06827880537137293, 0.3483753991702595, 0.293368757196008, 0.35754317283263476, 0.14614924917212044, 0.7307462458606022, 0.14614924917212044, 0.2117062556405054, 0.14113750376033693, 0.6351187669215161, 0.1463547120495277, 0.44767323685737886, 0.40462773331340013, 0.4003503968929001, 0.20612099642010698, 0.39638653157712883, 0.8175530473681581, 0.4080306635418195, 0.3906213552307019, 0.20183916823202006, 0.2366790001839163, 0.6413237424338377, 0.12215690332073098, 0.7742420818036161, 0.0814991665056438, 0.1222487497584657, 0.7784271938488877, 0.16680582725333307, 0.05560194241777769, 0.13640755587970452, 0.17050944484963065, 0.6820377793985226, 0.07869691001017967, 0.07869691001017967, 0.8263175551068865, 0.11351090526244652, 0.7945763368371256, 0.11351090526244652, 0.3209542416399372, 0.534923736066562, 0.14710402741830453, 0.5967453442448365, 0.27255196011182437, 0.13197252805414653, 0.7069999645263009, 0.8243250723832953, 0.13527385803213052, 0.03804577257153671, 0.8239955323398879, 0.07847576498475123, 0.07847576498475123, 0.808546378421617, 0.12439175052640261, 0.062195875263201306, 0.884040903793734, 0.1768081807587468, 0.1768081807587468, 0.8467579399642077, 0.14112632332736794, 0.07056316166368397, 0.07059136389690208, 0.14118272779380417, 0.7765050028659228, 0.7962329336433873, 0.37888594941352205, 0.4761478597993687, 0.1451670304266368, 0.13199389285064372, 0.19799083927596556, 0.6599694642532186, 0.11360295728150649, 0.7952207009705454, 0.11360295728150649, 0.15547851612337815, 0.7996037972059448, 0.02221121658905402, 0.7958109647991258, 0.7950942334747899, 0.13251570557913164, 0.531351258796737, 0.31977520452826885, 0.14888685300373686, 0.44572916783697575, 0.33864708547373834, 0.2155026907560153, 0.48296880748598475, 0.3550798709242754, 0.16249417821958365, 0.413692532190928, 0.423853401402635, 0.16257390738731203, 0.15763855535178653, 0.7881927767589327, 0.07881927767589327, 0.18284260929852103, 0.10970556557911262, 0.69480191533438, 0.8321101185732708, 0.07564637441575188, 0.07564637441575188, 0.38053242659158715, 0.38789757033206945, 0.23159285317294442, 0.8475504898354547, 0.10594381122943183, 0.10594381122943183, 0.3913189461803218, 0.463514152828563, 0.14491356696784646, 0.26504863854364347, 0.26504863854364347, 0.7951459156309304, 0.07876722329909631, 0.7876722329909631, 0.07876722329909631, 0.7484486822940885, 0.14675464358707616, 0.11740371486966093, 0.0959926058945705, 0.191985211789141, 0.6719482412619935, 0.8490554134025073, 0.8474016389183611, 0.10592520486479513, 0.10592520486479513, 0.21201610844450106, 0.21201610844450106, 0.6360483253335032, 0.4181523941103891, 0.33527534302544715, 0.248631153254826, 0.6746533145462841, 0.1367540502458684, 0.19145567034421573, 0.8843052027291449, 0.5737084135920273, 0.3012392260158727, 0.12438810737172272, 0.4253789987842373, 0.41699714659144443, 0.157159728614866, 0.1461818579535569, 0.7309092897677846, 0.1461818579535569, 0.12792850096577807, 0.7675710057946684, 0.8114004638206236, 0.10583484310703786, 0.07055656207135858, 0.7671106151850421, 0.3941230429710365, 0.4211222823391443, 0.18471430835985922, 0.818152854237205, 0.06384650231552685, 0.830004530101849, 0.06384650231552685, 0.3734501721533961, 0.395139757728947, 0.23163365108248618, 0.3887964132578583, 0.45036660937731976, 0.16102974369705314, 0.6943194484053723, 0.18659835175894382, 0.12150590347094016, 0.2972775303170024, 0.44261321180531465, 0.2598425820548613, 0.6443283125598571, 0.25057212155105557, 0.10595621139873207, 0.8833839152955643, 0.17667678305911286, 0.23220807314354983, 0.6702369383916097, 0.10027166794835106, 0.1366204928845406, 0.7514127108649732, 0.0683102464422703, 0.3957462287186722, 0.3505697185909699, 0.2529884567151329, 0.6229353334495215, 0.21753297358554716, 0.15820579897130702, 0.10222404524409251, 0.8177923619527401, 0.10222404524409251, 0.2348474891740903, 0.7045424675222709, 0.05871187229352257, 0.23607969851880167, 0.11803984925940084, 0.7082390955564051, 0.35843126965412353, 0.4276365785490909, 0.21349184913824842, 0.8187871649937933, 0.09288370208402322, 0.8359533187562089, 0.09288370208402322, 0.8486526418719964, 0.2362386355244237, 0.5905965888110593, 0.17717897664331778, 0.09620801416061221, 0.19241602832122442, 0.7696641132848977, 0.17049261988940295, 0.8524630994470147, 0.17049261988940295, 0.7959294426627206, 0.5154617434981946, 0.3408244314933418, 0.14365327277818538, 0.12779402027439954, 0.7667641216463972, 0.10201981404278099, 0.8161585123422479, 0.426476709103553, 0.4052954398835103, 0.16822815745918568, 0.8476485810443409, 0.10595607263054262, 0.05297803631527131, 0.07900017137024398, 0.4600598215090679, 0.4600598215090679, 0.15139021032537633, 0.15139021032537633, 0.7569510516268816, 0.801795563307335, 0.14578101151042455, 0.03644525287760614, 0.27813595436309785, 0.4472537959607603, 0.2753406181383431, 0.2760630298371276, 0.49329295495486736, 0.22854398371762202, 0.8842958246097747, 0.3545822688996266, 0.5747965201109736, 0.07091645377992532, 0.45763233870276404, 0.3684041722200981, 0.17440050721611983, 0.21270565119627, 0.85082260478508, 0.11647010057794886, 0.7454086436988727, 0.13976412069353863, 0.18610703620861185, 0.7444281448344474, 0.09305351810430593, 0.1927601192297869, 0.09638005961489345, 0.6746604173042541, 0.16057544081143474, 0.20645413818613037, 0.642301763245739, 0.5498743884367456, 0.332482188357102, 0.11874363869896501, 0.2433648900518842, 0.7030541268165543, 0.05408108667819649, 0.7676123155424014, 0.794097142440198, 0.132349523740033, 0.8682669138019051, 0.09647410153354502, 0.09647410153354502, 0.614797292524725, 0.30056756523431005, 0.08538851285065627, 0.8832005146370824, 0.17664010292741647, 0.8482186331788585, 0.24233235842471232, 0.296183993630204, 0.47120180804805173, 0.2118549792962417, 0.14123665286416112, 0.6355649378887251, 0.2045788965073641, 0.8183155860294564, 0.24948637035577073, 0.12474318517788537, 0.6860875184783695, 0.8255135615720298, 0.11793050879600425, 0.11793050879600425, 0.8161132246136736, 0.08161132246136736, 0.08161132246136736, 0.3878849168226392, 0.4523370933339539, 0.1593726546461599, 0.6848346224436762, 0.1684019563386089, 0.14594836216012771, 0.5325803482212722, 0.2892290280602415, 0.17952146569256366, 0.4256626521561032, 0.29188296147847076, 0.2831959685773258, 0.11809512302874428, 0.11809512302874428, 0.8266658612012099, 0.3405579975651395, 0.2789677214097419, 0.3804105291951026, 0.31293760653156305, 0.5604183498058077, 0.12643037971619023, 0.25691157018162036, 0.2755960480130109, 0.4671119457847643, 0.5636644851876754, 0.259795976712015, 0.1762901270545816, 0.30336277035890463, 0.6546249255113205, 0.04789938479351125, 0.7569482584350815, 0.15138965168701632, 0.4800768932957819, 0.27983091641304936, 0.24003844664789095, 0.15163728863242945, 0.15163728863242945, 0.7581864431621472, 0.3227852608392548, 0.39465707781136816, 0.2828092206299136, 0.706210468833924, 0.6406023699239781, 0.21353412330799268, 0.14947388631559488, 0.33785204879899344, 0.3179783988696409, 0.34778887376366974, 0.8510296594616269, 0.17020593189232539, 0.09294744391568045, 0.8365269952411241, 0.09294744391568045, 0.26523187336452775, 0.7956956200935832, 0.13281076209331422, 0.13281076209331422, 0.7968645725598853, 0.4962406097259584, 0.30622010076753353, 0.196855779064843, 0.8174616142266233, 0.7948696377113094, 0.13247827295188488, 0.40191687635654516, 0.2767918110757339, 0.3185001661693377, 0.7521115612637987, 0.11142393500204427, 0.13927991875255533, 0.7955428813031636, 0.08839365347812929, 0.08839365347812929, 0.2763838613817716, 0.4384936262306953, 0.28435647276778425, 0.11355413866763628, 0.7948789706734539, 0.11355413866763628, 0.7961195073588412, 0.13268658455980686, 0.13268658455980686], "Term": ["a_z", "a_z", "a_z", "abiu", "abiu", "accuracy", "accuracy", "accuracy", "achieve", "achieve", "achieve", "adversarial", "adversarial", "adversarial", "ann", "ann", "ann", "approach", "approach", "approach", "art", "art", "art", "asc", "asc", "asc", "ascad", "ascad", "ascad", "atomistic", "atomistic", "atomistic", "attack", "attack", "attack", "attacker", "attacker", "attacker", "azimuth", "azimuth", "azimuth", "base", "base", "base", "bilateral", "bilateral", "bn", "bn", "bn", "bo", "boson", "boson", "boson", "cadx", "cadx", "cadx", "cbam", "cbam", "cervical", "cervical", "cervical", "cfo", "cfo", "cfo", "chinese", "chinese", "chinese", "cifar", "cifar", "cifar", "classification", "classification", "classification", "cnld", "cnld", "cnn", "cnn", "cnn", "cnns", "cnns", "cnns", "collage", "collage", "collage", "compare", "compare", "compare", "constituents", "convolution", "convolution", "convolution", "convolutional", "convolutional", "convolutional", "cycnn", "cycnn", "cyconv", "cylindrical", "data", "data", "data", "dataset", "dataset", "dataset", "dct", "dct", "dct", "dd", "dd", "deep", "deep", "deep", "deepisign", "deepisign", "deepisign", "defect", "defect", "defect", "demonstrate", "demonstrate", "demonstrate", "demosaicing", "demosaicing", "demosaicing", "design", "design", "design", "detection", "detection", "detection", "detectors", "detectors", "detectors", "dfa", "dfa", "dfa", "different", "different", "different", "document", "document", "document", "domain", "domain", "domain", "dscs", "dscs", "dscs", "dtw", "dtw", "dtw", "earthquake", "earthquake", "earthquake", "ecg", "ecg", "ecg", "edge", "edge", "edge", "electrodes", "electrodes", "elm", "elm", "elm", "energy", "energy", "energy", "epu", "epu", "epu", "equipment", "equipment", "equipment", "examples", "examples", "examples", "face", "face", "face", "fcsns", "feature", "feature", "feature", "filter", "filter", "filter", "fish", "fish", "fish", "fmri", "fmri", "fmri", "fool", "fool", "fool", "forensic", "forensic", "forensic", "fsnet", "fsnet", "fsnet", "function", "function", "function", "generate", "generate", "generate", "gluon", "graph", "graph", "graph", "grasp", "grasp", "grasp", "gw", "gw", "gw", "haar", "haar", "haar", "hdr", "hdr", "hdr", "hi", "hi", "hi", "higgs", "high", "high", "high", "hmdb", "hmdb", "hmdb", "hobflops", "hobflops", "hobflops", "hsi", "hsi", "hsi", "ht", "hw", "hw", "image", "image", "image", "improve", "improve", "improve", "information", "information", "information", "input", "input", "input", "insurance", "insurance", "insurance", "kinetics", "kinetics", "kinetics", "knee", "knee", "knee", "layer", "layer", "layer", "lb", "lb", "lb", "learn", "learn", "learn", "lepton", "lepton", "lepton", "lipschitz", "lipschitz", "lipschitz", "localization", "localization", "localization", "lofar", "lofar", "lofar", "magnifications", "manga", "manga", "manga", "manycore", "manycore", "manycore", "map", "map", "map", "mask", "mask", "mask", "metamodel", "method", "method", "method", "methods", "methods", "methods", "migration", "migration", "migration", "mil", "mil", "mine", "mine", "mine", "minimizers", "model", "model", "model", "mtt", "multigrid", "multigrid", "multigrid", "network", "network", "network", "neural", "neural", "neural", "noise", "noise", "noise", "number", "number", "number", "object", "object", "object", "observers", "observers", "optimization", "optimization", "optimization", "os", "os", "os", "paper", "paper", "paper", "patch", "patch", "patch", "pavia", "pavia", "pavia", "pca", "pca", "pca", "pdi", "pdi", "pdi", "performance", "performance", "performance", "phmm", "pin", "pin", "pin", "pluralistic", "popular", "popular", "popular", "porous", "porous", "porous", "pos", "pos", "pos", "precnn", "process", "process", "process", "promoter", "promoter", "promoters", "promoters", "propose", "propose", "propose", "prostate", "prostate", "prostate", "prune", "prune", "prune", "psn", "psn", "psn", "quantum", "quantum", "quantum", "recognition", "recognition", "recognition", "reduce", "reduce", "reduce", "relief", "resolution", "resolution", "resolution", "result", "result", "result", "rigidity", "rigidity", "rnns", "rnns", "rnns", "salinas", "salinas", "salinas", "secret", "secret", "secret", "security", "security", "security", "segmentation", "segmentation", "segmentation", "sentiment", "sentiment", "sentiment", "separator", "sfq", "sfq", "shadow", "shadow", "shadow", "signal", "signal", "signal", "sod", "sod", "songs", "sparsity", "sparsity", "sparsity", "specaugment", "specaugment", "specaugment", "spectralnet", "spectralnet", "spoof", "spoof", "spoof", "ssl", "ssl", "ssl", "stain", "stain", "stain", "state", "state", "state", "stream", "stream", "stream", "structure", "structure", "structure", "study", "study", "study", "substation", "substation", "substation", "target", "target", "target", "task", "task", "task", "temporal", "temporal", "temporal", "test", "test", "test", "text", "text", "text", "tfds", "tfds", "time", "time", "time", "tortuosity", "tortuosity", "tortuosity", "train", "train", "train", "transferrable", "transformer", "transformer", "transformer", "tune", "tune", "tune", "uaps", "uaps", "ubi", "ubi", "ubi", "untrusted", "untrusted", "urine", "urine", "urine", "use", "use", "use", "vcs", "vgq", "vgq", "video", "video", "video", "wave", "wave", "wave", "weave", "weave", "weave", "weight", "weight", "weight", "winograd", "winograd", "winograd", "wsi", "wsi", "wsi"]}, "R": 30, "lambda.step": 0.01, "plot.opts": {"xlab": "PC1", "ylab": "PC2"}, "topic.order": [3, 1, 2]};

function LDAvis_load_lib(url, callback){
  var s = document.createElement('script');
  s.src = url;
  s.async = true;
  s.onreadystatechange = s.onload = callback;
  s.onerror = function(){console.warn("failed to load library " + url);};
  document.getElementsByTagName("head")[0].appendChild(s);
}

if(typeof(LDAvis) !== "undefined"){
   // already loaded: just create the visualization
   !function(LDAvis){
       new LDAvis("#" + "ldavis_el11406171307159523194713451", ldavis_el11406171307159523194713451_data);
   }(LDAvis);
}else if(typeof define === "function" && define.amd){
   // require.js is available: use it to load d3/LDAvis
   require.config({paths: {d3: "https://d3js.org/d3.v5"}});
   require(["d3"], function(d3){
      window.d3 = d3;
      LDAvis_load_lib("https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js", function(){
        new LDAvis("#" + "ldavis_el11406171307159523194713451", ldavis_el11406171307159523194713451_data);
      });
    });
}else{
    // require.js not available: dynamically load d3 & LDAvis
    LDAvis_load_lib("https://d3js.org/d3.v5.js", function(){
         LDAvis_load_lib("https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js", function(){
                 new LDAvis("#" + "ldavis_el11406171307159523194713451", ldavis_el11406171307159523194713451_data);
            })
         });
}
</script>"
    }
}