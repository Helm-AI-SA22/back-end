{
    "criteria": {
        "topic": {"mode": "union", "topics": [0,1]},
        "availability": 0,
        "citationCount": {"min": 2, "max": 100},
        "date": {"min": 2018, "max": 2022},
        "author": ["a"]
    },
    
    "documents": [
        {
            "abstract": "In recent years, deep learning poses a deep technical revolution in almost every field and attracts great attentions from industry and academia. Especially, the convolutional neural network (CNN), one representative model of deep learning, achieves great successes in computer vision and natural language processing. However, simply or blindly applying CNN to the other fields results in lower training effects or makes it quite difficult to adjust the model parameters. In this poster, we propose a general methodology named V-CNN by introducing data visualizing for CNN. V-CNN introduces a data visualization model prior to CNN modeling to make sure the data after processing is fit for the features of images as well as CNN modeling. We apply V-CNN to the network intrusion detection problem based on a famous practical dataset: AWID. Simulation results confirm V-CNN significantly outperforms other studies and the recall rate of each invasion category is more than 99.8%.",
            "authors": "Mao Yang;Bo Li;Guanxiong Feng;Zhongjiang Yan",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1807.02164v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1807.02164v1",
            "publicationDate": "2018-06-12T10:57:57Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.23256704435701123,
            "title": "V-CNN: When Convolutional Neural Network encounters Data Visualization",
            "topics": [
                {
                    "affinity": 0.17752324044704437,
                    "id": 0
                },
                {
                    "affinity": 0.8182066082954407,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Hyperspectral remote sensing plays role in the field of earth observation research because of rich spatial, radiation and spectral information. With the rapid development of deep learning, deep neural networks are widely used in hyperspectral remote sensing image classification tasks, but at the same time, a series of difficulties have arisen, such as high demand for training samples, time-consuming model training. Convolutional neural network (CNN) is well known for its capability of feature learning and has demonstrated excellent performance in hyperspectral image classification. In this paper, the one-dimension CNN (1D-CNN) modules was added after the two-dimension CNN (2D-CNN), so the complete network structure contains two different dimension CNN, called multi-dimension CNN (MD-CNN). The proposed 1D-CNN blocks can continue to learn contextual features and is expected to have more discriminative power. Experimental results with hyperspectral image benchmark datasets demonstrate that the proposed method can outperform the state-of-the-art CNN-based classification methods.",
            "authors": "Haojie Cai;Tao Chen",
            "citationCount": 1,
            "id": "10.1109/IGARSS39084.2020.9323561",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9323561",
            "publicationDate": "26 Sept.-2 Oct. 2020",
            "source": [
                "ieee"
            ],
            "tfidf": 0.2249825317521736,
            "title": "Multi-Dimension CNN for Hyperspectral Image Classificaton",
            "topics": [
                {
                    "affinity": 0.4710131883621216,
                    "id": 0
                },
                {
                    "affinity": 0.5251523852348328,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Deep Convolutional Neural Networks (CNN) have exhibited superior performance in many visual recognition tasks including image classification, object detection, and scene label- ing, due to their large learning capacity and resistance to overfit. For the image classification task, most of the current deep CNN- based approaches take the whole size-normalized image as input and have achieved quite promising results. Compared with the previously dominating approaches based on feature extraction, pooling, and classification, the deep CNN-based approaches mainly rely on the learning capability of deep CNN to achieve superior results: the burden of minimizing intra-class variation while maximizing inter-class difference is entirely dependent on the implicit feature learning component of deep CNN; we rely upon the implicitly learned filters and pooling component to select the discriminative regions, which correspond to the activated neurons. However, if the irrelevant regions constitute a large portion of the image of interest, the classification performance of the deep CNN, which takes the whole image as input, can be heavily affected. To solve this issue, we propose a novel latent CNN framework, which treats the most discriminate region as a latent variable. We can jointly learn the global CNN with the latent CNN to avoid the aforementioned big irrelevant region issue, and our experimental results show the evident advantage of the proposed latent CNN over traditional deep CNN: latent CNN outperforms the state-of-the-art performance of deep CNN on standard benchmark datasets including the CIFAR-10, CIFAR- 100, MNIST and PASCAL VOC 2007 Classification dataset.",
            "authors": "Miao Sun;Tony X. Han;Xun Xu;Ming-Chang Liu;Ahmad Khodayari-Rostamabad",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/1604.04333v2",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1604.04333v2",
            "publicationDate": "2016-04-15T02:07:42Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.20687995550141433,
            "title": "Latent Model Ensemble with Auto-localization",
            "topics": [
                {
                    "affinity": 0.5589706301689148,
                    "id": 0
                },
                {
                    "affinity": 0.43851685523986816,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "In this paper, we present the result of sparsity increase by CNN compression on 6 representative CNN networks including a famous localization CNN network VGG16-SSD-300. We will also show activation analysis result by applying the compressed CNN networks to a CNN HW accelerator model. Finally, this paper will be ended up with processing time estimation result of the CNN HW accelerator model which reflects the reduced transmission time of sparse weights.",
            "authors": "Mi-Young Lee;Joo-Hyun Lee;Jin-Kyu Kim;Byung-Jo Kim;Ju-Yeob Kim",
            "citationCount": 5,
            "id": "10.1109/ISOCC47750.2019.9027643",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9027643",
            "publicationDate": "6-9 Oct. 2019",
            "source": [
                "ieee"
            ],
            "tfidf": 0.20624370631916503,
            "title": "The Sparsity and Activation Analysis of Compressed CNN Networks in a HW CNN Accelerator Model",
            "topics": [
                {
                    "affinity": 0.9803771376609802,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "In recent years, convolutional neural network (CNN) has made a breakthrough development and been widely used in various fields, such as image recognition, target classification and natural language processing. However, with the continuous development of CNN, the complexity of CNN is gradually increasing. The ordinary hardware processors cannot meet the speed requirements of CNN. The hardware platform about CNN based on FPGA has gradually become the focus of research because of its parallel computing advantages. However, it is difficult and not friendly to implement CNN on FPGA for software developers. In this paper, we propose a FPGA-Cloud architecture for CNN. We implement CNN platform based on FPGA, and deploy the platform in the cloud. CNN resources based on FPGA can be utilized by local users through the network, which is language-friendly for software developers in hardware development and satisfies the user's requirement for CNN processing task.",
            "authors": "Guangju Wei;Yanzhao Hou;Zhao Zhao;Qimei Cui;Gang Deng;Xiaofeng Tao",
            "citationCount": 3,
            "id": "10.1109/APCC.2018.8633447",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8633447",
            "publicationDate": "12-14 Nov. 2018",
            "source": [
                "ieee"
            ],
            "tfidf": 0.19211028951644238,
            "title": "Demo: FPGA-Cloud Architecture For CNN",
            "topics": [
                {
                    "affinity": 0.2717796862125397,
                    "id": 0
                },
                {
                    "affinity": 0.7233000993728638,
                    "id": 1
                }
            ]
        },
        {
            "abstract": "With the rapid development of deep learning technology, CNN and LSTM have become two of the most popular neural networks. This paper combines CNN and LSTM or its variant and makes a slight change. It proposes a text classification model named NA-CNN-LSTM or NA-CNN-COIF-LSTM, which has no activation function in CNN. The experimental results on the subjective and objective text categorization dataset [1] show that the proposed model has better performance than the standard CNN or LSTM.",
            "authors": "Yuandong Luan;Shaofu Lin",
            "citationCount": 27,
            "id": "10.1109/ICAICA.2019.8873454",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8873454",
            "publicationDate": "29-31 March 2019",
            "source": [
                "ieee"
            ],
            "tfidf": 0.18899070687828898,
            "title": "Research on Text Classification Based on CNN and LSTM",
            "topics": [
                {
                    "affinity": 0.9838643670082092,
                    "id": 0
                }
            ]
        },
        {
            "abstract": "We present O-CNN, an Octree-based Convolutional Neural Network (CNN) for 3D shape analysis. Built upon the octree representation of 3D shapes, our method takes the average normal vectors of a 3D model sampled in the finest leaf octants as input and performs 3D CNN operations on the octants occupied by the 3D shape surface. We design a novel octree data structure to efficiently store the octant information and CNN features into the graphics memory and execute the entire O-CNN training and evaluation on the GPU. O-CNN supports various CNN structures and works for 3D shapes in different representations. By restraining the computations on the octants occupied by 3D surfaces, the memory and computational costs of the O-CNN grow quadratically as the depth of the octree increases, which makes the 3D CNN feasible for high-resolution 3D models. We compare the performance of the O-CNN with other existing 3D CNN solutions and demonstrate the efficiency and efficacy of O-CNN in three shape analysis tasks, including object classification, shape retrieval, and shape segmentation.",
            "authors": "Peng-Shuai Wang;Yang Liu;Yu-Xiao Guo;Chun-Yu Sun;Xin Tong",
            "citationCount": -1,
            "id": "10.1145/3072959.3073608",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/1712.01537v1",
            "publicationDate": "2017-12-05T09:25:19Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.18183423190987502,
            "title": "O-CNN: Octree-based Convolutional Neural Networks for 3D Shape Analysis",
            "topics": [
                {
                    "affinity": 0.9781863689422607,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Convolutional Neural Network (CNN) give an unmatched performance in image classification, object detection and object tracking. As many of the modern embedded systems for portable devices deals with similar tasks, they often deploy CNN based algorithms. The intensive computational workload associated with CNN inference demands powerful computing platforms like Graphics Processing Units. However, deploying CNN on mobile devices demands low power, application specific computing platforms like Field-Programmable Gate Array (FPGA) and Application-Specific Integrated Circuit (ASIC) which can work as computation accelerator units. Moreover, using certain algorithmic optimizations like using Depthwise Separable Convolution instead of standard convolution, significantly reduces the computational burden of CNN inference. This paper discusses a pipelined architecture of Depthwise Separable Convolution followed by activation and pooling operations for a single layer of CNN. The architecture is implemented on Xilinx 7 series FPGA and works at a clock period of 40ns. It can be used as a building block for an integrated system of CNN accelerator for implementation on FPGAs of different sizes. This work focuses on speeding up the convolution process, instead of implementing large design of an integrated system of CNN accelerator which makes it difficult to focus on performance of the subsystems. To the best of the knowledge of the authors, earlier works have implemented an integrated system of CNN accelerator but the blueprint for architecture of a single layer of CNN is not discussed individually, which can be a great support for the beginners in understanding FPGA based computing accelerators for CNN.",
            "authors": "Harsh Srivastava;Kishor Sarawadekar",
            "citationCount": 3,
            "id": "10.1109/ASPCON49795.2020.9276672",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9276672",
            "publicationDate": "7-9 Oct. 2020",
            "source": [
                "ieee"
            ],
            "tfidf": 0.18145568481012792,
            "title": "A Depthwise Separable Convolution Architecture for CNN Accelerator",
            "topics": [
                {
                    "affinity": 0.10285790264606476,
                    "id": 0
                },
                {
                    "affinity": 0.22056636214256287,
                    "id": 1
                },
                {
                    "affinity": 0.6765757203102112,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Convolutional neural network (CNN) models have been widely used for fault diagnosis of complex systems. However, traditional CNN models rely on small kernel filters to obtain local features from images. Thus, an excessively deep CNN is required to capture global features, which are critical for fault diagnosis of dynamical systems. In this work, we present an improved CNN that embeds global features (GF-CNN). Our method uses a multi-layer perceptron (MLP) for dimension reduction to directly extract global features and integrate them into the CNN. The advantage of this method is that both local and global patterns in images can be captured by a simple model architecture instead of establishing deep CNN models. The proposed method is applied to the fault diagnosis of the Tennessee Eastman process. Simulation results show that the GF-CNN can significantly improve the fault diagnosis performance compared to traditional CNN. The proposed method can also be applied to other areas such as computer vision and image processing.",
            "authors": "Qiugang Lu;Saif S. S. Al-Wahaibi",
            "citationCount": -1,
            "id": "http://arxiv.org/abs/2210.01727v1",
            "openaccess": 1,
            "pdfLink": "http://arxiv.org/pdf/2210.01727v1",
            "publicationDate": "2022-10-04T16:31:21Z",
            "source": [
                "arxiv"
            ],
            "tfidf": 0.18083222574200727,
            "title": "Enhanced CNN with Global Features for Fault Diagnosis of Complex   Chemical Processes",
            "topics": [
                {
                    "affinity": 0.6079731583595276,
                    "id": 0
                },
                {
                    "affinity": 0.38794636726379395,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "For millimeter wave (mmWave) massive multiple-input multiple-output (MIMO) systems, hybrid processing architecture is usually used to reduce the complexity and cost, which poses a very challenging issue in channel estimation. In this paper, deep convolutional neural network (CNN) is employed to address this problem. We first propose a spatial-frequency CNN (SF-CNN) based channel estimation exploiting both the spatial and frequency correlation, where the corrupted channel matrices at adjacent subcarriers are input into the CNN simultaneously. Then, exploiting the temporal correlation in time-varying channels, a spatial-frequency-temporal CNN (SFT-CNN) based approach is developed to further improve the accuracy. Moreover, we design a spatial pilot-reduced CNN (SPR-CNN) to save spatial pilot overhead for channel estimation, where channels in several successive coherence intervals are grouped and estimated by a channel estimation unit with memory. Numerical results show that the proposed SF-CNN and SFT-CNN based approaches outperform the non-ideal minimum mean-squared error (MMSE) estimator but with reduced complexity, and achieve the performance close to the ideal MMSE estimator that is very difficult to be implemented in practical situations. They are also robust to different propagation scenarios. The SPR-CNN based approach achieves comparable performance to SF-CNN and SFT-CNN based approaches while only requires about one-third of spatial pilot overhead at the cost of complexity. The results in this paper clearly demonstrate that deep CNN can efficiently exploit channel correlation to improve the estimation performance for mmWave massive MIMO systems.",
            "authors": "Peihao Dong;Hua Zhang;Geoffrey Ye Li;Ivan Sim\u00f5es Gaspar;Navid NaderiAlizadeh",
            "citationCount": 112,
            "id": "10.1109/JSTSP.2019.2925975",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8752012",
            "publicationDate": "Sept. 2019",
            "source": [
                "ieee",
                "arxiv"
            ],
            "tfidf": 0.18053579629091496,
            "title": "Deep CNN-Based Channel Estimation for mmWave Massive MIMO Systems",
            "topics": [
                {
                    "affinity": 0.9949624538421631,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Faster R-CNN (R corresponds to \u201cRegion\u201d) which combined the RPN network and the Fast R-CNN network is one of the best ways to object detection of R-CNN series based on deep learning. The proposal obtained by RPN is directly connected to the ROI Pooling layer, which is a framework for CNN to achieve end-to-end object detection. The feasibility of Faster R-CNN implementation of ResNet101 network and PVANET network is discussed based on the implementation of Faster R-CNN in VGG16 network. Different Faster R-CNN models can be obtained by training with deep learning framework of Caffe. A better model can be obtained by comparing the experimental results using mean average precision (mAP) as an evaluation index. Numerical results show that Faster R-CNN trained by PVANET network obtained the highest mAP.",
            "authors": "Bin Liu;Wencang Zhao;Qiaoqiao Sun",
            "citationCount": 19,
            "id": "10.1109/CAC.2017.8243900",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8243900",
            "publicationDate": "20-22 Oct. 2017",
            "source": [
                "ieee"
            ],
            "tfidf": 0.1792899782807232,
            "title": "Study of object detection based on Faster R-CNN",
            "topics": [
                {
                    "affinity": 0.9716699123382568,
                    "id": 2
                }
            ]
        },
        {
            "abstract": "Deep learning is a powerful means to recognize remote sensing image scene categories. In this study, a deep convolutional neural network (CNN) based ensemble method is proposed. Firstly, a CNN architecture composed of the feature layer and the classifier layer is designed. Then the classifier layer of CNN is treated as base-learner and integrated with the AdaBoost technique to construct a CNN-AdaBoost ensemble framework. The proposed method is compared with the CNN-SVM and fine-tuned VGG16. The experiment results on UC Merced land-use dataset show that the CNN-AdaBoost achieves an improved overall accuracy by 4.46% against the sole CNN. Also, our method outperforms another two paradigms. Therefore, the proposed CNN based ensemble method is promising for image representations regarding remote sensing image scene classification.",
            "authors": "Xudong Hu;Penglin Zhang;Qi Zhang",
            "citationCount": 4,
            "id": "10.1109/IGARSS39084.2020.9324261",
            "openaccess": 0,
            "pdfLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9324261",
            "publicationDate": "26 Sept.-2 Oct. 2020",
            "source": [
                "ieee"
            ],
            "tfidf": 0.17885280098180822,
            "title": "A Novel Framework of CNN Integrated with Adaboost for Remote Sensing Scene Classification",
            "topics": [
                {
                    "affinity": 0.9903144836425781,
                    "id": 2
                }
            ]
        }
    ],
    "lda_plot": "
<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css">


<div id="ldavis_el11402569396790726393978260"></div>
<script type="text/javascript">

var ldavis_el11402569396790726393978260_data = {"mdsDat": {"x": [0.031635381331758215, -0.031635381331758215], "y": [0.0, 0.0], "topics": [1, 2], "cluster": [1, 1], "Freq": [78.32269227849653, 21.67730772150347]}, "tinfo": {"Term": ["result", "low", "multipliers", "circuit", "hard", "integrator", "number", "cell", "important", "implementation", "limit", "type", "output", "process", "connect", "block", "components", "programmable", "show", "describe", "voltage", "stand", "mu", "power", "range", "technology", "mean", "incorporate", "detector", "simulation", "lstm", "mri", "sequence", "tumor", "td", "model", "brain", "performance", "datasets", "timedistributed", "consider", "study", "approach", "evaluate", "patient", "tumors", "layer", "contain", "highest", "diagnosis", "research", "learn", "cnn", "challenge", "accuracy", "test", "image", "convolutional", "compare", "hybrid", "develop", "respectively", "dataset", "age", "gender", "propose", "feature", "network", "face", "better", "recognition", "single", "result", "low", "multipliers", "circuit", "integrator", "number", "cell", "important", "implementation", "limit", "output", "process", "connect", "block", "components", "programmable", "show", "describe", "voltage", "stand", "mu", "power", "range", "technology", "mean", "incorporate", "detector", "simulation", "component", "application", "reduction", "result", "hard", "type", "cnn", "network", "gender", "age", "feature", "single", "recognition", "better", "dataset", "face"], "Freq": [3.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.996470926946909, 3.993167495309844, 3.9798014178360632, 3.243592249772697, 2.587657291398319, 3.871445624795803, 2.5663503746385112, 2.531383543516872, 3.1592743423013983, 1.8406523339398422, 1.8330713663725, 1.8327543508967312, 1.8322644617875656, 2.436918967828379, 1.8260725193779512, 1.8255389289731923, 1.8250442109915852, 1.8222750940901458, 1.8188697732445704, 1.7917073657624927, 2.9498835682854474, 1.7210793116621614, 6.3083476915706385, 1.7065384902452334, 1.704122726084683, 2.2709197172387094, 2.270143475993777, 1.6955867282702708, 1.1286134832721386, 1.1254634890568949, 1.683832769861004, 2.228008425176712, 2.7523486418872745, 3.2629521651641893, 3.251333415183278, 2.190463941945929, 2.5629250139723045, 2.477245605916193, 2.035666951089112, 1.9858550795867456, 1.9290530530602248, 1.924741232135614, 1.8735035153231552, 1.0309583785611707, 1.0274378816275096, 1.0175226472250882, 0.7546310773301035, 0.7531345503891673, 0.7477874820152942, 0.7457265582651083, 0.7369528808947036, 0.7361024098798656, 0.4638048724642133, 0.46082922627459877, 0.45932300983125424, 0.4591084708831017, 0.45853070924294387, 0.4583430001928394, 0.4581752381528653, 0.4578962138955811, 0.4578202348318685, 0.457509402277023, 0.457285040178153, 0.4566591317346487, 0.4566287601564101, 0.45621531911226754, 0.45611264379881594, 0.45575386491308756, 0.4552598673506464, 0.455183921699011, 0.4551172311927795, 0.4549779362426397, 0.4549725903102764, 1.218396790839756, 0.8444065891650296, 0.5827723825618805, 1.0319953557913313, 0.6322582751412033, 0.6421814616180149, 0.6373236797036879, 0.5964260942900743, 0.5296636518562177, 0.527859867452704, 0.5041053506449296, 0.5172086652242499, 0.48327525931231424], "Total": [3.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.3270370221610746, 4.325115580864063, 4.3173404799282284, 3.555362882099621, 2.8400909980499742, 4.254298981383864, 2.8276953328887315, 2.807350044431536, 3.5063078805069203, 2.0718339881350354, 2.0674227408081323, 2.0672380037462768, 2.066952930670778, 2.752391357608156, 2.0633501736590314, 2.063039759224399, 2.0627524968305635, 2.0611407909918054, 2.059158974992292, 2.0433547692937073, 3.384484380727025, 2.0022648625766375, 7.340343047361969, 1.9938048053383608, 1.9923997899273138, 2.655811979018035, 2.6553603695158605, 1.987433271151841, 1.3239204397478768, 1.3220876331488207, 1.9805945634298063, 2.6308463804820637, 3.2695573071115245, 3.900275844867877, 3.8935148768012926, 2.6090023325968206, 3.1593511082623786, 3.1095038810573965, 2.518942210401426, 2.489960430231675, 2.456912920512929, 2.454404883991832, 3.091900306162911, 1.7570229910390365, 1.761917528665134, 1.7757119372274797, 1.3436234374122236, 1.3457056481984093, 1.3531416916728463, 1.3560088736925864, 1.3682130857860233, 1.3693960529561993, 0.9503944432712037, 0.9545334162929171, 0.9566268828109086, 0.9569262566123169, 0.9577297637535283, 0.957990355148689, 0.9582237215063197, 0.9586124397663033, 0.9587173623364214, 0.959149995054813, 0.9594625733909583, 0.9603327401711139, 0.9603752242058329, 0.9609502533504158, 0.9610927640351692, 0.9615921059627052, 0.9622790996971319, 0.9623837538747996, 0.9624775026554289, 0.9626710980992084, 0.9626784279570033, 3.091900306162911, 2.0165446312992055, 1.5827028135304715, 7.340343047361969, 3.1095038810573965, 3.8935148768012926, 3.900275844867877, 3.1593511082623786, 2.454404883991832, 2.456912920512929, 2.489960430231675, 3.2695573071115245, 2.518942210401426], "Category": ["Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2"], "logprob": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.1724, -4.1732, -4.1766, -4.3811, -4.607, -4.2042, -4.6153, -4.629, -4.4074, -4.9477, -4.9518, -4.952, -4.9522, -4.6671, -4.9556, -4.9559, -4.9562, -4.9577, -4.9596, -4.9746, -4.476, -5.0148, -3.7159, -5.0233, -5.0247, -4.7376, -4.7379, -5.0298, -5.4368, -5.4396, -5.0367, -4.7567, -4.5453, -4.3752, -4.3787, -4.7737, -4.6166, -4.6506, -4.847, -4.8717, -4.9008, -4.903, -4.93, -4.2427, -4.2461, -4.2558, -4.5547, -4.5567, -4.5639, -4.5666, -4.5784, -4.5796, -5.0415, -5.0479, -5.0512, -5.0517, -5.0529, -5.0534, -5.0537, -5.0543, -5.0545, -5.0552, -5.0557, -5.057, -5.0571, -5.058, -5.0582, -5.059, -5.0601, -5.0603, -5.0604, -5.0607, -5.0607, -4.0757, -4.4423, -4.8132, -4.2417, -4.7317, -4.7161, -4.7237, -4.79, -4.9087, -4.9121, -4.9582, -4.9325, -5.0004], "loglift": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.1649, 0.1645, 0.1629, 0.1526, 0.1512, 0.15, 0.1474, 0.1409, 0.1401, 0.126, 0.124, 0.1239, 0.1238, 0.1226, 0.1222, 0.122, 0.1219, 0.1212, 0.1203, 0.1129, 0.1069, 0.093, 0.0928, 0.0888, 0.088, 0.0878, 0.0876, 0.0855, 0.0847, 0.0833, 0.082, 0.0781, 0.0721, 0.0659, 0.0641, 0.0695, 0.0351, 0.017, 0.0313, 0.0181, 0.0025, 0.0012, -0.2566, 0.9958, 0.9896, 0.9721, 0.952, 0.9485, 0.9358, 0.931, 0.9102, 0.9081, 0.8115, 0.8007, 0.7952, 0.7945, 0.7924, 0.7917, 0.7911, 0.7901, 0.7898, 0.7887, 0.7878, 0.7856, 0.7855, 0.7839, 0.7836, 0.7823, 0.7805, 0.7802, 0.7799, 0.7794, 0.7794, 0.5977, 0.6584, 0.5298, -0.433, -0.064, -0.2733, -0.2826, -0.1383, -0.0045, -0.0089, -0.0683, -0.3151, -0.1221]}, "token.table": {"Topic": [1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1], "Freq": [1.0038146009205127, 0.7691763658069231, 0.256392121935641, 1.038776381647374, 0.9676079074287144, 0.8032256158439885, 0.4016128079219943, 1.060934664745245, 0.7390209067933836, 0.7390209067933836, 1.0031072222541804, 0.5631544053036874, 0.5631544053036874, 0.8174004894984204, 0.1362334149164034, 0.7553323976102649, 1.03898532406321, 0.967388023998528, 0.9703364315242217, 1.0063230947325723, 0.9175554113930905, 0.30585180379769683, 0.8556008491662399, 1.0431744451843192, 1.039199542331056, 1.0097977834174143, 0.9787825540893748, 0.7266408515895108, 0.7939840746411067, 0.9495620768943213, 0.3165206922981071, 0.7705120167576301, 0.25683733891921007, 0.49589777705823795, 0.49589777705823795, 0.9712703216649344, 0.756379512921013, 0.7531934357989423, 0.7308803068679255, 0.7308803068679255, 0.7374583009009901, 0.7374583009009901, 1.0399419814276059, 0.7442561451041435, 0.7442561451041435, 0.9695782713015821, 0.9988688496617161, 0.730248928234632, 0.730248928234632, 0.5691445160934622, 0.5691445160934622, 0.9244201007557498, 1.0404822899732153, 0.9402254090517295, 0.9248307762450334, 1.0422501384976104, 0.5675634550032663, 0.5675634550032663, 0.6431894207251782, 0.3215947103625891, 0.7431045573292868, 0.7431045573292868, 0.9692974200560975, 1.0686234179989742, 1.0413057455708716, 0.7665765472924427, 1.041259681419764, 0.8140296643409163, 0.40701483217045814, 1.0387684723778432, 0.8863979450115136, 0.7602116242277627, 0.6468513865125316, 0.3234256932562658, 0.9264963045181223, 1.0435976250180994, 1.03908653483992, 0.8148614815120518, 0.4074307407560259, 1.0425897984213122, 0.967474473851377, 1.056304182527891, 1.040636595404845, 0.7530653584669362, 0.9653283088575563, 0.8437957247920496, 0.9694432649964541, 0.6318305568493559, 0.6318305568493559, 1.0430602795833088], "Term": ["accuracy", "age", "age", "application", "approach", "better", "better", "brain", "cell", "cell", "challenge", "circuit", "circuit", "cnn", "cnn", "compare", "component", "consider", "contain", "convolutional", "dataset", "dataset", "datasets", "describe", "detector", "develop", "diagnosis", "evaluate", "face", "feature", "feature", "gender", "gender", "hard", "hard", "highest", "hybrid", "image", "implementation", "implementation", "important", "important", "incorporate", "integrator", "integrator", "layer", "learn", "limit", "limit", "low", "low", "lstm", "mean", "model", "mri", "mu", "multipliers", "multipliers", "network", "network", "number", "number", "patient", "performance", "power", "propose", "range", "recognition", "recognition", "reduction", "research", "respectively", "result", "result", "sequence", "show", "simulation", "single", "single", "stand", "study", "td", "technology", "test", "timedistributed", "tumor", "tumors", "type", "type", "voltage"]}, "R": 30, "lambda.step": 0.01, "plot.opts": {"xlab": "PC1", "ylab": "PC2"}, "topic.order": [1, 2]};

function LDAvis_load_lib(url, callback){
  var s = document.createElement('script');
  s.src = url;
  s.async = true;
  s.onreadystatechange = s.onload = callback;
  s.onerror = function(){console.warn("failed to load library " + url);};
  document.getElementsByTagName("head")[0].appendChild(s);
}

if(typeof(LDAvis) !== "undefined"){
   // already loaded: just create the visualization
   !function(LDAvis){
       new LDAvis("#" + "ldavis_el11402569396790726393978260", ldavis_el11402569396790726393978260_data);
   }(LDAvis);
}else if(typeof define === "function" && define.amd){
   // require.js is available: use it to load d3/LDAvis
   require.config({paths: {d3: "https://d3js.org/d3.v5"}});
   require(["d3"], function(d3){
      window.d3 = d3;
      LDAvis_load_lib("https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js", function(){
        new LDAvis("#" + "ldavis_el11402569396790726393978260", ldavis_el11402569396790726393978260_data);
      });
    });
}else{
    // require.js not available: dynamically load d3 & LDAvis
    LDAvis_load_lib("https://d3js.org/d3.v5.js", function(){
         LDAvis_load_lib("https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js", function(){
                 new LDAvis("#" + "ldavis_el11402569396790726393978260", ldavis_el11402569396790726393978260_data);
            })
         });
}
</script>",
    "topics": [
        {
            "id": 0,
            "name": "cnn cnns network model image"
        },
        {
            "id": 1,
            "name": "cnn network cnns train model"
        },
        {
            "id": 2,
            "name": "cnn image network model propose"
        }
    ]
}